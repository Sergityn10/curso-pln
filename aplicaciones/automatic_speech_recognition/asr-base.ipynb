{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "print(\"Hugging Face logging\")\n",
    "login(token)"
   ],
   "id": "e80a3016d6e15e39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ],
   "id": "96efaa14351d2553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Automatic Text to Speech (ATS)",
   "id": "fbc91ce919b79c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generación estática",
   "id": "cf1f7a5ba64bfa50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Función para transcribir un archivo de audio\n",
    "def transcribe_audio_file(model_asr, processor_asr, audio_path):\n",
    "    # Cargar el archivo de audio\n",
    "    waveform, sampling_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Resamplear si es necesario\n",
    "    if sampling_rate != SAMPLE_RATE:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=SAMPLE_RATE)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Normalizar el audio para evitar problemas de rendimiento\n",
    "    waveform = waveform / torch.max(torch.abs(waveform))\n",
    "\n",
    "    # Procesar el audio\n",
    "    inputs = processor_asr(waveform.squeeze().numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "    inputs = {key: value.clone().detach().to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Generar transcripción con parámetros optimizados\n",
    "    with torch.no_grad():\n",
    "        forced_decoder_ids = processor_asr.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n",
    "        predicted_ids = model_asr.generate(\n",
    "            inputs[\"input_features\"],\n",
    "            max_length=1000, # generación máxima de output\n",
    "            num_beams=3,\n",
    "            early_stopping=False, # evita finalización prematura\n",
    "            forced_decoder_ids = forced_decoder_ids # Indica el idioma\n",
    "        )\n",
    "    return processor_asr.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n"
   ],
   "id": "f26777b9da2635b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuración inicial\n",
    "MODEL_NAME = \"openai/whisper-small\"  # Modelo preentrenado en Hugging Face\n",
    "SAMPLE_RATE = 16000\n",
    "cache_dir = \"./models/whisper-small\"\n",
    "\n",
    "# Cargar el modelo y el procesador con cache optimizado\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, cache_dir=cache_dir)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir=cache_dir).to(device)\n",
    "print(\"Modelo Whisper cargado correctamente\")"
   ],
   "id": "6a9d2c3484e37a83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy\n",
    "\n",
    "audio_path = \"./provided/no_es_pais.wav\"\n",
    "transcription = transcribe_audio_file(model, processor, audio_path)\n",
    "print(f\"Transcripción: {transcription}\")"
   ],
   "id": "10e952085f865b24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generación ventana",
   "id": "5c53ff3eb2619c00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chunk_audio(waveform, chunk_size_s, sampling_rate):\n",
    "    # Tamaño del fragmento en muestras\n",
    "    chunk_size = int(chunk_size_s * sampling_rate)\n",
    "    return waveform.split(chunk_size, dim=1)\n",
    "\n",
    "def transcribe_audio_file_chunked(model_asr, processor_asr, audio_path, chunk_size_s=30):\n",
    "    global device\n",
    "    # Cargar el archivo de audio\n",
    "    waveform, sampling_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Resamplear si es necesario\n",
    "    if sampling_rate != SAMPLE_RATE:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=SAMPLE_RATE)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Normalizar el audio\n",
    "    waveform = waveform / torch.max(torch.abs(waveform)).detach()\n",
    "\n",
    "    # Dividir el audio en fragmentos\n",
    "    chunks = chunk_audio(waveform, chunk_size_s, SAMPLE_RATE)\n",
    "\n",
    "    # Procesar cada fragmento\n",
    "    transcriptions = []\n",
    "    for chunk in chunks:\n",
    "        inputs = processor_asr(chunk.squeeze().numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "        inputs = {key: value.clone().detach().to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Generar transcripción\n",
    "        with torch.no_grad():\n",
    "            forced_decoder_ids = processor_asr.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n",
    "            predicted_ids = model_asr.generate(inputs[\"input_features\"],forced_decoder_ids=forced_decoder_ids, max_length=500, num_beams=5)\n",
    "            transcription_tmp = processor_asr.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "            transcriptions.append(transcription_tmp)\n",
    "\n",
    "    # Combinar las transcripciones\n",
    "    return \" \".join(transcriptions)\n",
    "\n"
   ],
   "id": "3c5cb904b58a1431",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "audio_path = \"./provided/no_es_pais.wav\"\n",
    "transcription = transcribe_audio_file_chunked(model, processor, audio_path, chunk_size_s=30)\n",
    "print(f\"Transcripción: {transcription}\")"
   ],
   "id": "532fefd875fe2c9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Limitaciones",
   "id": "349a50f561def233"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Tarea ASRB1\n",
    "\n",
    "Convierta a texto el audio de los ficheros `padrino.wav` y `patria_invento.wav`. Cuando tenga la transcripción, escuche los audios y valores cuantitativamente como de bien funciona el modelo. Para transcribir el audio utilice la función `transcribe_audio_file_chunked`"
   ],
   "id": "5d2c47cd5dc1ce6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: transcribir el audio de \"./provided/padrino.wav\"\n",
   "id": "91b5821fa4443738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: transcribir el audio de \"./provided/patria_invento.wav\"\n",
   "id": "fc4ca6ec5c2f447d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ASR en vivo",
   "id": "d355e62a0587c40d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from transformers import pipeline\n",
    "\n",
    "# Función para grabar audio\n",
    "def record_audio(duration, sample_rate):\n",
    "    print(\"Grabando...\")\n",
    "    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    print(\"Grabación completada.\")\n",
    "    return audio.flatten()\n",
    "\n",
    "# Convertir audio a formato compatible con el modelo (int16)\n",
    "def preprocess_audio(audio):\n",
    "    # Convertir de float32 a int16\n",
    "    audio_int16 = np.int16(audio * 32767)\n",
    "    return audio_int16\n",
    "\n"
   ],
   "id": "b7b6f27406cb332e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuración de grabación\n",
    "SAMPLE_RATE = 16000  # Frecuencia de muestreo\n",
    "DURATION = 5  # Duración en segundos\n",
    "\n",
    "# Grabar el audio\n",
    "audio = record_audio(DURATION, SAMPLE_RATE)\n",
    "# Preprocesar el audio\n",
    "audio_int16 = preprocess_audio(audio)\n",
    "\n",
    "# Guardar temporalmente el audio como WAV\n",
    "output_filename = \"output.wav\"\n",
    "wav.write(output_filename, SAMPLE_RATE, audio_int16)\n",
    "\n",
    "# Usar el modelo ASR\n",
    "transcription = transcribe_audio_file_chunked(model, processor, output_filename, chunk_size_s=30)\n",
    "print(f\"Transcripción: {transcription}\")"
   ],
   "id": "b40e23e8024a746a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generación ventana en vivo (live stream)\n",
    "\n",
    "Ventanas de audio:\n",
    "\n",
    "* El audio se graba en bloques de duración fija (CHUNK_DURATION), definidos por CHUNK_SIZE.\n",
    "\n",
    "Callback de grabación:\n",
    "\n",
    "* La función audio_callback se ejecuta cada vez que se captura un fragmento de audio.\n",
    "* El fragmento se procesa y se convierte al formato esperado por el modelo (int16).\n",
    "\n",
    "Transcripción en vivo:\n",
    "\n",
    "* El fragmento de audio capturado se pasa directamente al modelo ASR de Hugging Face.\n",
    "* La transcripción se imprime en la consola en tiempo real.\n",
    "\n",
    "Control del flujo:\n",
    "\n",
    "* sd.InputStream mantiene el flujo de grabación abierto.\n",
    "* El programa se ejecuta indefinidamente hasta que el usuario presiona Ctrl+C.\n",
    "\n",
    "Notas importantes:\n",
    "* Latencia: La duración de CHUNK_DURATION afecta la latencia de la transcripción. Puedes ajustarla según la capacidad del modelo y la potencia de tu máquina.\n",
    " * Rendimiento: Procesar en tiempo real puede ser intensivo. Asegúrate de que tu equipo tenga suficiente capacidad para manejar las solicitudes."
   ],
   "id": "f0af661fda17b442"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "# Configuración inicial\n",
    "MODEL_NAME = \"openai/whisper-small\"\n",
    "SAMPLE_RATE = 16000\n",
    "cache_dir = \"./models/whisper-small\"\n",
    "CHUNK_DURATION = 5  # en segundos\n",
    "CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION)\n",
    "\n",
    "\n",
    "# Cargar el modelo y el procesador con cache optimizado\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, cache_dir=cache_dir)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir=cache_dir).to(device)\n",
    "print(\"Modelo Whisper cargado correctamente\")\n",
    "\n",
    "\n",
    "def transcribe_audio_live(model_asr, processor_asr, audio_chunk):\n",
    "    # Preprocesar audio para Whisper\n",
    "    inputs = processor_asr(audio_chunk, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "    inputs = {key: value.clone().detach().to(model_asr.device) for key, value in inputs.items()}\n",
    "\n",
    "    # Generar transcripción\n",
    "    with torch.no_grad():\n",
    "        forced_decoder_ids = processor_asr.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n",
    "        predicted_ids = model_asr.generate(inputs[\"input_features\"], max_length=448, num_beams=5, forced_decoder_ids=forced_decoder_ids)\n",
    "        transcription = processor_asr.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    global model, processor\n",
    "    if status:\n",
    "        print(f\"Estado de grabación: {status}\")\n",
    "    audio_chunk = indata.squeeze().astype(np.float32)\n",
    "    try:\n",
    "        transcription = transcribe_audio_live(model, processor, audio_chunk)\n",
    "        print(\"Transcripción:\", transcription)\n",
    "    except Exception as e:\n",
    "        print(\"Error durante la transcripción:\", e)\n",
    "\n",
    "def start_live_transcription(chunk_size, sampling_rate):\n",
    "    print(\"Iniciando transcripción en vivo con Whisper. Presiona Ctrl+C para detener.\")\n",
    "    try:\n",
    "        with sd.InputStream(\n",
    "            samplerate=sampling_rate,\n",
    "            channels=1,\n",
    "            dtype=\"float32\",\n",
    "            blocksize=chunk_size,\n",
    "            callback=audio_callback\n",
    "        ):\n",
    "            while True:\n",
    "                sd.sleep(6000)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Transcripción detenida.\")\n",
    "\n",
    "# Iniciar transcripción\n",
    "# TODO: Uncomment -> start_live_transcription(CHUNK_SIZE, SAMPLE_RATE)\n",
    "# TODO: probar cambiando el tiempo de duración de la grabación y observar los distintos resultados"
   ],
   "id": "8a7fa0c71cd72cd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluar diversos modelos\n",
   "id": "d62808b8ec27938b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# Cargar el dataset\n",
    "data = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", cache_dir=\"./data/common_voice_11_0_test\")\n",
    "data = data.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(\"Dataset cargado correctamente\")\n",
    "\n",
    "# Preprocesamiento: Normalización del texto\n",
    "def normalize_text(batch):\n",
    "    text = batch[\"sentence\"].lower().strip()\n",
    "    batch[\"sentence\"] = text\n",
    "    return batch\n",
    "\n",
    "data = data.map(normalize_text)\n",
    "print(\"Texto normalizado\")\n",
    "# Filtrar el dataset por idioma y clientId\n",
    "def filter_dataset(batch):\n",
    "    return batch[\"client_id\"] == \"0d461bf9e0450a750b67f5ec88f07d9e2bd8b5a0c46f00bbf6f5d7a4a50a0f1fa46222902c3e70fd317747176028de656c385e09c6270e08aab36c419e8f7d7c\" #Notese que se ha buscado una voz con acento argentino\n",
    "\n",
    "# TODO: visite la web del dataset y escuche el audio que estamos filtrando\n",
    "data = data.filter(filter_dataset)\n",
    "print(\"Dataset filtrado por clientId\")\n",
    "print(data)"
   ],
   "id": "60c2650434cdd009",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para evaluar los modelos, vamos a utilizar la metrica WER. El **Word Error Rate (WER)** es una métrica utilizada para evaluar la precisión de los sistemas de reconocimiento automático del habla (ASR). Mide la proporción de errores cometidos en las transcripciones generadas por el modelo, comparándolas con una referencia. Estos errores se calculan como la suma de sustituciones, inserciones y eliminaciones necesarias para alinear la transcripción con la referencia, y se normalizan dividiendo por el número total de palabras en la referencia. Un WER más bajo indica un mejor rendimiento del sistema.\n",
    "\n"
   ],
   "id": "c2658eccfed555cc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from evaluate import load\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Configuración de los modelos\n",
    "whisper_model_name = \"openai/whisper-small\"\n",
    "wav2vec_model_name = \"facebook/wav2vec2-large-xlsr-53-spanish\"\n",
    "\n",
    "# Cargar Whisper\n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name).to(device)\n",
    "\n",
    "# Cargar Wav2Vec2\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(wav2vec_model_name)\n",
    "wav2vec_model = Wav2Vec2ForCTC.from_pretrained(wav2vec_model_name).to(device)\n",
    "\n",
    "# Métrica WER (Word Error Rate)\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "# Evaluación de ambos modelos\n",
    "def evaluate_model(processor, model, dataset, is_whisper=False):\n",
    "    global device\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    for batch in dataset:\n",
    "        audio = batch[\"audio\"][\"array\"]\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {key: value.clone().detach().to(device) for key, value in inputs.items()}\n",
    "\n",
    "        if is_whisper:\n",
    "            # Asegurar que las características Mel estén rellenadas a la longitud requerida\n",
    "            input_features = inputs[\"input_features\"]\n",
    "            padding_length = 3000 - input_features.shape[-1]\n",
    "            if padding_length > 0:\n",
    "                input_features = torch.nn.functional.pad(input_features, (0, padding_length), \"constant\", 0)\n",
    "\n",
    "            inputs[\"input_features\"] = input_features\n",
    "\n",
    "        with torch.no_grad():\n",
    "            transcription=\"\"\n",
    "            if is_whisper:\n",
    "                forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n",
    "                predicted_ids = model.generate(inputs[\"input_features\"], forced_decoder_ids=forced_decoder_ids)\n",
    "                transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "            else:\n",
    "                logits = model(inputs[\"input_values\"]).logits\n",
    "                predicted_ids = torch.argmax(logits, dim=-1)\n",
    "                transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "            predictions.append(transcription.lower().strip())\n",
    "            references.append(batch[\"sentence\"])\n",
    "\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    return wer\n",
    "\n",
    "# Evaluar Whisper\n",
    "print(\"Evaluando Whisper...\")\n",
    "whisper_wer = evaluate_model(whisper_processor, whisper_model, data, is_whisper=True)\n",
    "print(f\"WER de Whisper: {whisper_wer:.4f}\")\n",
    "\n",
    "# Evaluar Wav2Vec2\n",
    "print(\"Evaluando Wav2Vec2...\")\n",
    "wav2vec_wer = evaluate_model(wav2vec_processor, wav2vec_model, data)\n",
    "print(f\"WER de Wav2Vec2: {wav2vec_wer:.4f}\")\n",
    "\n",
    "# Crear gráfica de comparación\n",
    "models = [\"Whisper\", \"Wav2Vec2\"]\n",
    "wer_scores = [whisper_wer, wav2vec_wer]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, wer_scores)\n",
    "plt.title(\"Comparación de WER entre modelos ASR\")\n",
    "plt.ylabel(\"WER (Word Error Rate)\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.ylim(0, max(wer_scores) + 0.1)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tarea ASR2",
   "id": "86b9e1a32b504641"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cambiar el código anterior para evaluar los modelos con las primeras 30 muestras del dataset de test. ¿Qué ocurre con los resultados?",
   "id": "14a8de5fbcd91a23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tarea ASR3",
   "id": "86414d2f37d333da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cambiar el modelo `openai/whisper-small` por `openai/whisper-tiny`. ¿Qué ocurre con los resultados?",
   "id": "6d156a314771402c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
