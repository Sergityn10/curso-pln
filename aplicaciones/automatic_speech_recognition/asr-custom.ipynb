{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "print(\"Hugging Face logging\")\n",
    "login(token)"
   ],
   "id": "e80a3016d6e15e39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ],
   "id": "96efaa14351d2553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Configuración inicial\n",
    "MODEL_NAME = \"openai/whisper-tiny\"  # Modelo preentrenado en Hugging Face\n",
    "SAMPLE_RATE = 16000\n",
    "cache_dir = \"./models/whisper-tiny\""
   ],
   "id": "99280f8ac5e6449",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Automatic Text to Speech (ATS)",
   "id": "fbc91ce919b79c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Uso de distintos modelos",
   "id": "4b8aa58564fee9dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea ASRC1\n",
    "\n",
    "Escribir el código necesario para realizar una generación en ventana y transcribir el audio `patria_invento.wav`"
   ],
   "id": "5c53ff3eb2619c00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DONE: cargar el modelo y el processor whisper\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, cache_dir=cache_dir)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir=cache_dir).to(device)\n",
    "print(\"Modelo Whisper cargado correctamente\")\n"
   ],
   "id": "649699172427b86b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DONE: escribir las funciones necesarias para realizar un ASR en ventana\n",
    "def chunk_audio(waveform, chunk_size_s, sampling_rate):\n",
    "    chunk_size = int(chunk_size_s * sampling_rate)  # Tamaño del fragmento en muestras\n",
    "    return waveform.split(chunk_size, dim=1)\n",
    "\n",
    "def transcribe_audio_file_chunked(model_asr, processor_asr, audio_path, chunk_size_s=30):\n",
    "    global device\n",
    "    # Cargar el archivo de audio\n",
    "    waveform, sampling_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Resamplear si es necesario\n",
    "    if sampling_rate != SAMPLE_RATE:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=SAMPLE_RATE)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Normalizar el audio\n",
    "    waveform = waveform / torch.max(torch.abs(waveform)).detach()\n",
    "\n",
    "    # Dividir el audio en fragmentos\n",
    "    chunks = chunk_audio(waveform, chunk_size_s, SAMPLE_RATE)\n",
    "\n",
    "    # Procesar cada fragmento\n",
    "    transcriptions = []\n",
    "    for chunk in chunks:\n",
    "        inputs = processor_asr(chunk.squeeze().numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "        inputs = {key: value.clone().detach().to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Generar transcripción\n",
    "        with torch.no_grad():\n",
    "            forced_decoder_ids = processor_asr.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n",
    "            predicted_ids = model_asr.generate(inputs[\"input_features\"],forced_decoder_ids=forced_decoder_ids, max_length=500, num_beams=5)\n",
    "            transcription_tmp = processor_asr.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "            transcriptions.append(transcription_tmp)\n",
    "\n",
    "    # Combinar las transcripciones\n",
    "    return \" \".join(transcriptions)\n"
   ],
   "id": "3c5cb904b58a1431",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DONE: transcribir el audio de patria_invento.wav\n",
    "audio_path = \"./provided/patria_invento.wav\"  # Reemplazar con la ruta al archivo de audio\n",
    "transcription = transcribe_audio_file_chunked(model, processor, audio_path, chunk_size_s=30)\n",
    "print(f\"Transcripción: {transcription}\")"
   ],
   "id": "fc4ca6ec5c2f447d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea ASRC2\n",
    "\n",
    "Comprobar la transcripción usando el modelo small `openai/whisper-small`, ¿hay diferencias significativas en los resultados?"
   ],
   "id": "d91abc36399d30f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ajuste fino ASR",
   "id": "d355e62a0587c40d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vamos entrenar el modelo con voces que posean acento argentino",
   "id": "b2542f9507773b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset personalizado\n",
    "dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", cache_dir=\"./data/common_voice_11_0_test\")\n",
    "dataset = dataset.filter(lambda example: \"Argentina\" in example[\"accent\"])\n",
    "print(dataset)"
   ],
   "id": "b7b6f27406cb332e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import Audio\n",
    "\n",
    "# Preprocesar el dataset\n",
    "def preprocess_data(batch):\n",
    "    # Extraer el audio y procesarlo\n",
    "    audio = batch[\"audio\"]\n",
    "    inputs = processor.feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\", padding=\"longest\"\n",
    "    )\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "\n",
    "    # Tokenizar el texto para las etiquetas\n",
    "    labels = processor.tokenizer(batch[\"sentence\"], return_tensors=\"pt\", padding=\"longest\").input_ids[0]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset = dataset.map(preprocess_data, remove_columns=dataset.column_names)\n"
   ],
   "id": "ecd8116a9efe6a15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Crear un DataCollator personalizado con padding a 3000\n",
    "class WhisperDataCollator:\n",
    "    def __init__(self, processor, max_length=3000):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Obtener input_features y labels\n",
    "        input_features = [torch.tensor(feature[\"input_features\"]) for feature in features]\n",
    "        labels = [torch.tensor(feature[\"labels\"]) for feature in features]\n",
    "\n",
    "        # Aplicar padding a input_features a la longitud especificada\n",
    "        padded_input_features = [\n",
    "            torch.nn.functional.pad(\n",
    "                feat, (0, self.max_length - feat.shape[1]), mode=\"constant\", value=0\n",
    "            )\n",
    "            for feat in input_features\n",
    "        ]\n",
    "\n",
    "        # Crear el batch con input_features y labels\n",
    "        batch = {\n",
    "            \"input_features\": torch.stack(padded_input_features),\n",
    "            \"labels\": torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100),\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "# Instanciar el DataCollator personalizado\n",
    "data_collator = WhisperDataCollator(processor, max_length=3000)\n"
   ],
   "id": "b40e23e8024a746a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./models/whisper-fine-tuned-argentine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Crear el Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=data_collator,  # Usar el DataCollator personalizado\n",
    ")\n",
    "\n",
    "\n",
    "# Iniciar el entrenamiento\n",
    "trainer.train()\n",
    "# Guardar el modelo ajustado\n",
    "trainer.save_model(\"./models/whisper-fine-tuned-argentine\")\n"
   ],
   "id": "f458dc01d0ac5078",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "# Ruta al modelo fine-tuned y modelo base\n",
    "fine_tuned_model_path = \"./models/whisper-fine-tuned-argentine\"\n",
    "base_model_name = \"openai/whisper-tiny\"\n",
    "\n",
    "# Cargar modelos\n",
    "fine_tuned_model = WhisperForConditionalGeneration.from_pretrained(fine_tuned_model_path, cache_dir=\"./models/whisper-fine-tuned-argentine\")\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(base_model_name, cache_dir=\"./models/whisper-tiny\")\n",
    "\n",
    "# Cargar procesador\n",
    "processor = WhisperProcessor.from_pretrained(base_model_name, cache_dir=\"./models/whisper-tiny\")\n"
   ],
   "id": "585fbbd45d1cf1d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar el dataset personalizado\n",
    "eval_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"validation\", cache_dir=\"./data/common_voice_11_0_validation\")\n",
    "eval_dataset = eval_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "eval_dataset = eval_dataset.filter(lambda sample: \"Argentina\" in sample[\"accent\"])\n",
    "\n",
    "# Preprocesamiento: Normalización del texto\n",
    "def normalize_text(batch):\n",
    "    text = batch[\"sentence\"].lower().strip()\n",
    "    batch[\"sentence\"] = text\n",
    "    return batch\n",
    "\n",
    "eval_dataset = eval_dataset.map(normalize_text)\n",
    "eval_dataset = eval_dataset.select(range(50))\n",
    "print(eval_dataset)"
   ],
   "id": "a26c3dc9abc7117c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from evaluate import load\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Función de preprocesamiento ajustada\n",
    "def preprocess_data(batch):\n",
    "    # Convertir audio y texto a tensores\n",
    "    processed = processor(batch[\"audio\"][\"array\"], sampling_rate=batch[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "    batch[\"input_features\"] = processed.input_features.squeeze(0)  # Asegurarse de que sea tensor\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"], return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "    return batch\n",
    "\n",
    "# Aplicar preprocesamiento al dataset de evaluación\n",
    "eval_dataset = eval_dataset.map(preprocess_data)\n",
    "\n",
    "# Métrica WER\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "# Función para evaluar un modelo\n",
    "def evaluate_model(model, dataset, processor_asr, batch_size=8):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=lambda x: x)\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Verificar y convertir input_features en tensores\n",
    "        input_features = []\n",
    "        for sample in batch:\n",
    "            if isinstance(sample[\"input_features\"], torch.Tensor):\n",
    "                input_features.append(sample[\"input_features\"])\n",
    "            else:\n",
    "                input_features.append(torch.tensor(sample[\"input_features\"]))\n",
    "        input_features = torch.stack(input_features)\n",
    "        labels = [processor_asr.decode(sample[\"labels\"], skip_special_tokens=True) for sample in batch]\n",
    "\n",
    "        # Generar predicciones\n",
    "        with torch.no_grad():\n",
    "            forced_decoder_ids = processor_asr.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n",
    "            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "        predicted_texts = processor_asr.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(predicted_texts)\n",
    "        references.extend(labels)\n",
    "\n",
    "    # Calcular la métrica WER\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    return wer\n",
    "\n",
    "# Evaluar ambos modelos\n",
    "print(\"Evaluando modelo base...\")\n",
    "base_wer = evaluate_model(base_model, eval_dataset, processor)\n",
    "print(f\"WER modelo base: {base_wer}\")\n",
    "\n",
    "print(\"Evaluando modelo fine-tuned...\")\n",
    "fine_tuned_wer = evaluate_model(fine_tuned_model, eval_dataset, processor)\n",
    "print(f\"WER modelo fine-tuned: {fine_tuned_wer}\")\n"
   ],
   "id": "e7e9190905df1859",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "¿Qué ocurre con los resultados? ¿Se podría mejorar subiendo los epoch? ¿Puede ser que se haya realizado un overfitting de los datos?¿Puede ser que la calidad de los datos de entrenamiento fueran nefastos?",
   "id": "2664de9b7049da92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
