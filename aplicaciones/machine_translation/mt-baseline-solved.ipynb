{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Uso de modelos para traducción (Machine Translation)\n",
    "\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "\n",
    "token = \"\"\n",
    "print(\"Hugging Face logging\")\n",
    "login(token)"
   ],
   "id": "763042b074e946dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelos entrenados inglés-español",
   "id": "4b6a2f5be8ae37d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En [Hugging Face](https://huggingface.co/models) existen multiples modelos, entre los cuales, modelos orientados a machine translation. Principalmente, los más utilizados son:\n",
    "\n",
    "* MarianMT: es un modelo especializado para traducción automática, para nuestro problema vamos a usar la versión preentrenada para inglés a español `Helsinki-NLP/opus-mt-en-es`.\n",
    "* T5: es un modelo versátil preentrenado para varias tareas, incluyendo traducción. Para nuestro problema vamos a utilizar la versión más ligera `t5-small`."
   ],
   "id": "c50f76c8250f09f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Modelo MarianMT\n",
   "id": "826b3dcd4edc8eaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para cargar un modelo MarianMt utilizaremos el método estático de la clase con el mismo nombre `MarianMTModel.from_pretrained` indicando el nombre del modelo, que en este caso es [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es) y los parámetro `cache_dir` y `local_files_only` que sirven para que la primera vez se descargue el modelo y las posteriores ejecuciones se hagan usando el modelo descargado en lugar de volver a descargarlo  en cada ejecución.",
   "id": "2e2d64a0158fd879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "def load_pretrained_marian_mt():\n",
    "    marian_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\", cache_dir=\"./models/pretrained_marian_en-es\", local_files_only=False)\n",
    "    marian_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\", cache_dir=\"./models/pretrained_marian_en-es\", local_files_only=False)\n",
    "    return marian_model, marian_tokenizer\n",
    "\n",
    "marian_model, marian_tokenizer = load_pretrained_marian_mt()\n",
    "\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "inputs = marian_tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = marian_model.generate(**inputs, do_sample=False, temperature=1.0)\n",
    "translated_text = marian_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ],
   "id": "5da641c9d42b7170",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notese que para utilizar el modelo, tenemos que pasar las entradas por el tokenizador y por el modelo. Para pasarla por el modelo utilizamos el método `.generate()` que posee varios parámetros interesantes:\n",
    "\n",
    "* `do_sample` indica si el modelo debe muestrear probabilidades de las palabras posibles en cada paso, en lugar de elegir únicamente la palabra con la probabilidad más alta (modo greedy). Si valor es False (predeterminado)no se hace ningún muestreo y el modelo selecciona la palabra con la probabilidad más alta en cada paso (modo greedy o beam search). En cambio, si su valor es True se hacen muestreos según las probabilidades de las palabras posibles, lo que permite generar salidas más variadas. Es decir, utilizando `do_sample=False` se obtienen traducciones más deterministas y consistentes, en cambio, usando `do_sample=True` se obtienen resultados más creativos o variados, pero menos deterministas.\n",
    "* `temperature` escala las probabilidades de las palabras generadas por el modelo, controlando la \"aleatoriedad\" del muestreo. Puede tomar los siguientes valores: un valor mayor a 1.0 aumenta la diversidad favoreciendo palabras menos probables; un valor menor a 1.0 reduce la diversidad, concentrándose en palabras más probables; un valor de 1.0 (predeterminado) utiliza las probabilidades originales del modelo. Por lo tanto, cuando `do_sample=False`, este parámetro no tiene efecto (porque no hay muestreo). Por el contrario, si `do_sample=True` regula la aleatoriedad en las generaciones.\n",
    "* `num_beams` activa el beam search, una técnica que explora múltiples caminos en paralelo para encontrar la secuencia más probable. Puede tomar distintos valores: `num_beams=1` (predeterminado) indica un modo de operación greedy; `num_beams>1` indica el número de caminos a explorar. Este parámetro puede mejorar la calidad de las generaciones a costa de mayor tiempo de cómputo si se aumenta su valor."
   ],
   "id": "a96fde84b259c121"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MTB1\n",
    "\n",
    "Cambiar los parámetros `do_sample` y `temperature` para que el modelo haga traducciones o generaciones lo menos deterministas posible, es decir, produzca alucinaciones."
   ],
   "id": "d415f2fecaab454"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"On the table, there were a good number of topics\"\n",
    "inputs = marian_tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = marian_model.generate(**inputs, do_sample=False, temperature=1.0)\n",
    "translated_text = marian_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ],
   "id": "26ccabd2fb263e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Modelo T5",
   "id": "8d7f876922cfc2e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para cargar un modelo T5 utilizaremos el método estático de la clase con el mismo nombre `T5ForConditionalgeneration.from_pretrained` indicando el nombre del modelo, que en este caso es [t5-small](https://huggingface.co/google-t5/t5-small) y los parámetro `cache_dir` y `local_files_only` que sirven para que la primera vez se descargue el modelo y las posteriores ejecuciones se hagan usando el modelo descargado en lugar de volver a descargarlo en cada ejecución. Además del modelo small, existen variantes más complejas y pesadas, como el [t5-base](https://huggingface.co/google-t5/t5-base) o el [t5-large](https://huggingface.co/google-t5/t5-large).",
   "id": "36f71e9888a3e470"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "def load_pretrained_t5():\n",
    "    # También existen t5-base o t5-large\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(\"vgaraujov/t5-base-translation-en-es\", cache_dir=\"./models/pretrained_t5_en-es\", local_files_only=False)\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(\"vgaraujov/t5-base-translation-en-es\", cache_dir=\"./models/pretrained_t5_en-es\", local_files_only=False)\n",
    "    return t5_model, t5_tokenizer\n",
    "\n",
    "t5_model, t5_tokenizer = load_pretrained_t5()\n",
    "\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "inputs = t5_tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = t5_model.generate(**inputs, do_sample=False, temperature=1.0)\n",
    "translated_text = t5_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MTB2\n",
    "\n",
    "Cambiar los parámetros `do_sample` y `temperature` para que el modelo haga traducciones o generaciones lo menos deterministas posible, es decir, produzca alucinaciones.\n"
   ],
   "id": "be4e6c773946b808"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"On the table, there were a good number of topics\"\n",
    "inputs = t5_tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = t5_model.generate(**inputs, do_sample=False, temperature=1.0)\n",
    "translated_text = t5_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ],
   "id": "870ecfec2f7a8b7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea conjunta de la clase MTB3: ¿Qué modelo es mejor?\n",
    "\n",
    "Justificar que modelo de los anteriores traduce mejor, ¿Qué prueba numérica sustenta la respuesta? ¿Es una justificación reproducible y objetiva?"
   ],
   "id": "1e4068a0e6ed6a4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MTB4\n",
    "\n",
    "Escribir una función de python que recibe una frase de entrada, un modelo y un tokenizador y devuelve la traducción de salida. Usando esta función se tiene que poder pasar por parámetro los modelos T5 y MarianMT y obtener las traducciones."
   ],
   "id": "2ee82275f1204f66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## TODO: REMOVE\n",
    "def translate(text, model, tokenizer, do_sample=False, temperature=1.0):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated_tokens = model.generate(**inputs, do_sample=do_sample, temperature=temperature)\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "translated_text = translate(text, marian_model, marian_tokenizer)\n",
    "print(translated_text)\n",
    "translated_text = translate(text, t5_model, t5_tokenizer)\n",
    "print(translated_text)"
   ],
   "id": "61af7368e05ffaa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para traducir un array de frases o textos, se puede usar la siguiente función que optimizar los recursos del ordenador.",
   "id": "291d8a0797f84206"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "def translate_batch(texts, model, tokenizer, batch_size=32, do_sample=False, temperature=1.0):\n",
    "    translations = []\n",
    "    model.eval()  # Asegurarse de estar en modo inferencia\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Usar GPU si está disponible\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():  # Deshabilitar gradientes para inferencia\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=90, padding=True).to(device)\n",
    "            translated_tokens = model.generate(**inputs, do_sample=do_sample, temperature=temperature)\n",
    "            translations.extend(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True))\n",
    "\n",
    "    return translations"
   ],
   "id": "c670ec0d22c5db2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Comparación de modelos",
   "id": "3d4a60285622e9c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para comparar modelos, es necesario calcular métricas objetivas y reproducibles. Estas métricas normalmente dependen de un `gold-standard`, la verdad absoluta, lo que nos permite para una entrada ver si el modelo es capaz de generar una salida tal y como el `gold-standard` indica. Sin embargo, para problemas más difusos como la traducción las métricas normalmente no son simples. En particular, para la traducción se suelen utilizar tres métricas: BLEU (Bilingual Evaluation Understudy), METEOR (Metric for Evaluation of Translation with Explicit ORdering), y ROUGE (Recall-Oriented Understudy for Gisting Evaluation).\n",
    "\n",
    "* **BLEU** mide la similitud entre una traducción generada y la esperada. Para ello, calcula la similitud usando ngrams (unigramas, bigramas, etc.) entre la salida obtenida y la esperada (presente en el `gold-standard`); además, penaliza las salidas muy cortas. El resultado de esta métrica es un valor porcentual donde a mayor valor se considera que más **precisa** es la traducción. Una limitación conocida es que tiende a favorecer las traducciones literales por el uso de ngrams.\n",
    "\n",
    "* **METEOR** es una métrica similar a **BLEU** pero que se centra en evaluar la similitud con sinonimias, stemming y coincidencias exactas. Esto hace que sea menos sensible a las traducciones literales y más flexible con traducciones menos literales. Internamente, esta métrica calcula precisión, recall y un F1 ajustado. Su resultado es un valor de 0 a 1, donde a mayor valor mejor es la traducción. Una limitación conocida es que es computacionalmente más costoso de **BLEU**, lo que lo hace menos usado que **BLEU** o **ROUGE**.\n",
    "\n",
    "* **ROUGE** evalúa la calidad de la traducción comparando la salida generada con la del `gold-standard` usado en resúmenes automáticos (n-grams y la longitud de las subsecuencias comunes). Tiene tres variantes principales: `ROUGE-1` compara unigramas, `ROUGE-2` compara bigramas, `ROUGE-L` compara la subsecuencia común más larga. Su resultado es un valor entre 0 y 1, donde a mayor valor mejor es la traducción; normalmente esta métrica se asocia al **recall**. Como limitaciones conocidas, esta metrica es insensible a la sinonimia o variaciones semánticas, además, penaliza traducciones \"creativas\".\n",
    "\n",
    "Para calcular estas métricas vamos a hacer uso del paquete `evaluate`"
   ],
   "id": "27ff7d499250b1c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MTB5\n",
    "\n",
    "Cargue en una variable el dataset de `okezieowen/english_to_spanish` y almacénelo en la carpeta `./data/okezieowen`. Despúes, prepare un conjunto de testing que contenga 50 frases del dataset. No \"baraje\" las frases del dataset ni utilice ningún `seed`."
   ],
   "id": "7e5800309287e57b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: REMOVE THIS\n",
    "dataset = load_dataset(\"okezieowen/english_to_spanish\", split=\"train\", cache_dir=\"./data/okezieowen\")\n",
    "split_dataset = dataset.train_test_split(test_size=50)\n",
    "eval_dataset = split_dataset['test']\n",
    "print(\"Evaluation size: \", len(eval_dataset))\n",
    "print(eval_dataset)"
   ],
   "id": "bfc7ac6d2ee28c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Antes de poder aplicar las métricas, necesitamos construir el código que usando el dataset, genere una variable con los resultados esperados y, por otro lado, genere las salidas de cada modelo.",
   "id": "357545a20e8542a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "expected_results = [ [row['Spanish']] for row in eval_dataset]\n",
    "inputs = [row['English'] for row in eval_dataset]\n",
    "\n",
    "t5_results = translate_batch(inputs, t5_model, t5_tokenizer)\n",
    "marianmt_results = translate_batch(inputs, marian_model, marian_tokenizer)\n"
   ],
   "id": "2b6ec2cacd1d56c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**NOTA:** Un ejercicio interesante sería ejecutar el código anterior con la función `translate` para ver la diferencia de tiempo",
   "id": "d5aee9a308ac5ff8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluación de modelos",
   "id": "daf2ca10be2a0c40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "bleu_t5 = bleu_metric.compute(predictions=t5_results, references=expected_results)\n",
    "meteor_t5 = meteor_metric.compute(predictions=t5_results, references=expected_results)\n",
    "rouge_t5 = rouge_metric.compute(predictions=t5_results, references=expected_results)\n",
    "\n",
    "bleu_marian = bleu_metric.compute(predictions=marianmt_results, references=expected_results)\n",
    "meteor_marian = meteor_metric.compute(predictions=marianmt_results, references=expected_results)\n",
    "rouge_marian = rouge_metric.compute(predictions=marianmt_results, references=expected_results)\n",
    "\n",
    "print(\"Metric -- T5 -- MarianMT \\n\")\n",
    "print(\"BLEU --- \",bleu_t5['bleu'], \" --- \", bleu_marian['bleu'])\n",
    "print(\"METEOR --- \",meteor_t5['meteor'], \" --- \", meteor_marian['meteor'])\n",
    "print(\"ROUGE1 --- \",rouge_t5['rouge1'], \" --- \", rouge_marian['rouge1'])\n",
    "print(\"ROUGE2 --- \",rouge_t5['rouge2'], \" --- \", rouge_marian['rouge2'])\n",
    "print(\"ROUGEL --- \",rouge_t5['rougeL'], \" --- \", rouge_marian['rougeL'])\n"
   ],
   "id": "afe2264654e373b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sustituye estos valores por los resultados reales\n",
    "scores_t5 = {\n",
    "    \"BLEU\": bleu_t5['bleu'],\n",
    "    \"METEOR\": meteor_t5[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_t5[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_t5[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_t5[\"rougeL\"]\n",
    "}\n",
    "\n",
    "scores_marian = {\n",
    "    \"BLEU\": bleu_marian[\"bleu\"],\n",
    "    \"METEOR\": meteor_marian[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_marian[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_marian[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_marian[\"rougeL\"],\n",
    "}\n",
    "\n",
    "# Crear gráfico\n",
    "labels = list(scores_t5.keys())\n",
    "t5_values = list(scores_t5.values())\n",
    "marian_values = list(scores_marian.values())\n",
    "\n",
    "x = range(len(labels))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, t5_values, width=0.4, label=\"T5\", align=\"center\")\n",
    "plt.bar([i + 0.4 for i in x], marian_values, width=0.4, label=\"MarianMT\", align=\"center\")\n",
    "\n",
    "# Configurar etiquetas y leyenda\n",
    "plt.xticks([i + 0.2 for i in x], labels)\n",
    "plt.xlabel(\"Métricas\")\n",
    "plt.ylabel(\"Puntaje\")\n",
    "plt.title(\"Comparación de Métricas entre T5 y MarianMT\")\n",
    "plt.legend()"
   ],
   "id": "d261e0332c7bd645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Como se puede observar los resultados no son muy buenos, esto se debe a la naturaleza juridica del dataset que estamos usando. Veamos un ejemplo en particular:",
   "id": "6395a4156a0d66bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "expected_output = \"Su Señoría, si así lo desea, podrá plantear esta cuestión en ese momento, es decir, el jueves antes de que se presente el informe.\"\n",
    "text = \"That is precisely the time when you may, if you wish, raise this question, i.e. on Thursday prior to the start of the presentation of the report.\"\n",
    "translated_text = translate(text, marian_model, marian_tokenizer)\n",
    "print(\"Marian translation: \", translated_text)\n",
    "translated_text = translate(text, t5_model, t5_tokenizer)\n",
    "print(\"T5 translation: \", translated_text)\n",
    "print(\"Expected: \", expected_output)"
   ],
   "id": "f3862434bf0b5004",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelos refinados inglés-español",
   "id": "42d265218c93a2ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cuando se usan modelos, otra opción, es utilizar un modelo preentrenado y refinarlos con un nuevo conjunto de entrenamiento. Para ello podemos usar los modelos anteriores y volverlos a entrenar con las clases `TrainerArguments`y `Trainer`.",
   "id": "d96c1e073d8a535"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`TrainingArguments` es una clase que codifica los parámetros que se aplicarán durante el entrenamiento del modelo, o su refinamiento si el modelo ya está entrenado. Recibe ciertos argumentos que es importante conocer:\n",
    "\n",
    "* `output_dir` indica en que directorio se guardará los resultados del entrenamiento, los checkpoints, y los logs.\n",
    "* `eval_strategy` indica cuándo se realizará la evaluación durante el entrenamiento, su resultado nos da una idea de como el modelo está aprendiendo. Puede tomar como valor `no`, si no se desea realiza evaluación alguna, `epoch` si se desea realizar una evaluación después de cada época o `steps` si se desea realizar la evaluación después de un número específico de pasos (requiere `eval_steps`).\n",
    "* `eval_steps` recibe un número entero que indica el número de pasos entre evaluaciones\n",
    "* `num_train_epochs` indica el número de épocas de entrenamiento\n",
    "* `save_strategy` indica cada cuantas epocas se guarda el modelo. Puede tomar como valor `no` (no se guardan checkpoints), `epoch` (guarda después de cada época) y `steps` (guarda después de un número específico de pasos).\n",
    "* `learning_rate` indica la tasa de aprendizaje para el optimizador. Un valor típico es `5e-5`.\n",
    "* `per_device_train_batch_size` tamaño del lote (batch) durante el entrenamiento. Si se usan múltiples GPUs, este valor se multiplica por el número de dispositivos.\n"
   ],
   "id": "12aae9ebc5447cc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/training/refined-t5-en-es\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"none\"\n",
    ")\n"
   ],
   "id": "2bcd0452158c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`Trainer` es la clase encargada de, usando los argumentos de entrenamiento, refinar un modelo. Sin embargo necesita recibir un conjunto de parámetros complejo:\n",
    "\n",
    "* `model` el modelo a entrenar\n",
    "* `args` los argumentos de entrenamiento resultado de crear un objeto `TrainingArguments`\n",
    "* `train_dataset` el dataset de entrenamiento ya tokenizado\n",
    "* `test_dataset` el dataset de evaluación/validación ya tokenizado\n",
    "* `data_collator` una instancia de la clase `DataCollatorForSeq2Seq`\n",
    "\n",
    "A continuación podemos ver un fragmento de código para el modelo t5 y MarianMT"
   ],
   "id": "e7ec6c7e7173bba1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Refinando un modelo T5",
   "id": "ce07cf96dabc5bb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train\", cache_dir=\"./data/iker\")\n",
    "split_dataset = dataset.train_test_split(train_size=50, test_size=50)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "t5_model, t5_tokenizer = load_pretrained_t5()\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    global t5_tokenizer\n",
    "    inputs_lang = examples['en']\n",
    "    targets_lang =  examples['es']\n",
    "\n",
    "    model_inputs = t5_tokenizer(inputs_lang, text_target=targets_lang, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "encoded_training_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model)\n",
    "trainer = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_training_dataset,\n",
    "    eval_dataset=encoded_eval_dataset,\n",
    "    data_collator= collator\n",
    ")\n",
    "\n",
    "# Entrenar o refinar el modelo\n",
    "if not os.path.exists(\"./models/refined-t5-en-es\"):\n",
    "    trainer.train()\n",
    "\n",
    "# Guardar el modelo y el tokenizador después del entrenamiento\n",
    "t5_model.save_pretrained(\"./models/refined-t5-en-es\")\n",
    "t5_tokenizer.save_pretrained(\"./models/refined-t5-en-es\")"
   ],
   "id": "fefa4f613ea0f67d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_refined_t5():\n",
    "    # También existen t5-base o t5-large\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(\"./models/refined-t5-en-es\")\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(\"./models/refined-t5-en-es\")\n",
    "    return t5_model, t5_tokenizer\n",
    "\n",
    "t5_refined_model, t5_refined_tokenizer = load_refined_t5()\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "translated_text = translate(text, t5_refined_model, t5_refined_tokenizer)\n",
    "print(\">T5 refined: \", translated_text)"
   ],
   "id": "b3b2492891074810",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Refinando un modelo MarianMT",
   "id": "440c674334916f30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MB6\n",
    "\n",
    "Reutilizando el código del modelo T5, refinar el modelo MarianMT utilizando los mismos datos que para el T5. Es importante que el modelo se guarde en \"./models/refined-marian-en-es\""
   ],
   "id": "12665b8a42bd3990"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/training/refined-marian-en-es\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# TODO: preparar los conjuntos de train y eval igual que antes a partir del dataset \"Iker/Document-Translation-en-es\"\n",
    "dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train\", cache_dir=\"./data/iker\")\n",
    "split_dataset = dataset.train_test_split(train_size=50, test_size=50)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# TODO: Cargar el modelo y tokenizador pre-entrenado marian mt\n",
    "marian_model, marian_tokenizer = load_pretrained_marian_mt()\n",
    "\n",
    "# TODO: modificar la funcion `preprocess_function` para que use el tokenizador de marian\n",
    "def preprocess_function(examples):\n",
    "    global marian_tokenizer\n",
    "    inputs_lang = examples['en']\n",
    "    targets_lang =  examples['es']\n",
    "\n",
    "    model_inputs = marian_tokenizer(inputs_lang, text_target=targets_lang, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "# TODO: codificar el dataset de training y eval\n",
    "encoded_training_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "# TODO: Construir el collator y el trainer\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=marian_tokenizer, model=marian_model)\n",
    "trainer = Trainer(\n",
    "    model=marian_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_training_dataset,\n",
    "    eval_dataset=encoded_eval_dataset,\n",
    "    data_collator= collator\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Entrenar o refinar el modelo\n",
    "if not os.path.exists(\"./models/refined-marian-en-es\"):\n",
    "    trainer.train()\n",
    "\n",
    "# TODO: Guardar el modelo y el tokenizador después del entrenamiento en \"./models/refined-marian-en-es\")\n",
    "marian_model.save_pretrained(\"./models/refined-t5-en-es\")\n",
    "marian_tokenizer.save_pretrained(\"./models/refined-t5-en-es\")\n",
    "\n",
    "def load_refined_marian():\n",
    "    marian_model = MarianMTModel.from_pretrained(\"./models/refined-marian-en-es\")\n",
    "    marian_tokenizer = MarianTokenizer.from_pretrained(\"./models/refined-marian-en-es\")\n",
    "    return marian_model, marian_tokenizer\n",
    "\n",
    "# DONE: Cargar el modelo y probar a traducir la frase \"On the table, there were a good number of topics\"\n",
    "marian_refined_model, marian_refined_tokenizer = load_refined_marian()\n",
    "text = [\"On the table, there were a good number of topics\"]\n",
    "translated_text = translate(text, marian_refined_model, marian_refined_tokenizer)\n",
    "print(\">Marian refined: \", translated_text)"
   ],
   "id": "fe6e5dd430314193"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/training/refined-marian-en-es\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# TODO: preparar los conjuntos de train y eval igual que antes a partir del dataset \"Iker/Document-Translation-en-es\"\n",
    "dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train\", cache_dir=\"./data/iker\")\n",
    "split_dataset = dataset.train_test_split(train_size=50, test_size=50)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# TODO: Cargar el modelo y tokenizador pre-entrenado marian mt\n",
    "marian_model, marian_tokenizer = load_pretrained_marian_mt()\n",
    "\n",
    "# TODO: modificar la funcion `preprocess_function` para que use el tokenizador de marian\n",
    "def preprocess_function(examples):\n",
    "    global marian_tokenizer\n",
    "    inputs_lang = examples['en']\n",
    "    targets_lang =  examples['es']\n",
    "\n",
    "    model_inputs = marian_tokenizer(inputs_lang, text_target=targets_lang, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "# TODO: codificar el dataset de training y eval\n",
    "encoded_training_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# TODO: Construir el collator y el trainer\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=marian_tokenizer, model=marian_model)\n",
    "trainer = Trainer(\n",
    "    model=marian_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_training_dataset,\n",
    "    eval_dataset=encoded_eval_dataset,\n",
    "    data_collator= collator\n",
    ")\n",
    "\n",
    "# TODO: Entrenar o refinar el modelo\n",
    "if not os.path.exists(\"./models/refined-marian-en-es\"):\n",
    "    trainer.train()\n",
    "\n",
    "# TODO: Guardar el modelo y el tokenizador después del entrenamiento\n",
    "marian_model.save_pretrained(\"./models/refined-marian-en-es\")\n",
    "marian_tokenizer.save_pretrained(\"./models/refined-marian-en-es\")\n",
    "\n",
    "# TODO: Escribir la funcion para cargar el modelo refinado marian\n",
    "def load_refined_marian():\n",
    "    marian_model = MarianMTModel.from_pretrained(\"./models/refined-marian-en-es\")\n",
    "    marian_tokenizer = MarianTokenizer.from_pretrained(\"./models/refined-marian-en-es\")\n",
    "    return marian_model, marian_tokenizer\n",
    "\n",
    "# TODO: Cargar el modelo y probar a traducir la frase \"On the table, there were a good number of topics\"\n",
    "marian_refined_model, marian_refined_tokenizer = load_refined_marian()\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "translated_text = translate(text, marian_refined_model, marian_refined_tokenizer)\n",
    "print(\">Marian refined: \", translated_text)"
   ],
   "id": "bc1338312be8c37b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Comparación de modelos preentrenados vs refinados",
   "id": "26fef9121c474f15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t5_refined_results = translate_batch(inputs, t5_refined_model, t5_refined_tokenizer)\n",
    "marianmt_refined_results = translate_batch(inputs, marian_refined_model, marian_refined_tokenizer)\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "bleu_t5_refined = bleu_metric.compute(predictions=t5_refined_results, references=expected_results)\n",
    "meteor_t5_refined = meteor_metric.compute(predictions=t5_refined_results, references=expected_results)\n",
    "rouge_t5_refined = rouge_metric.compute(predictions=t5_refined_results, references=expected_results)\n",
    "\n",
    "bleu_marian_refined = bleu_metric.compute(predictions=marianmt_refined_results, references=expected_results)\n",
    "meteor_marian_refined = meteor_metric.compute(predictions=marianmt_refined_results, references=expected_results)\n",
    "rouge_marian_refined = rouge_metric.compute(predictions=marianmt_refined_results, references=expected_results)\n",
    "\n",
    "# Sustituye estos valores por los resultados reales\n",
    "scores_t5 = {\n",
    "    \"BLEU\": bleu_t5['bleu'],\n",
    "    \"METEOR\": meteor_t5[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_t5[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_t5[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_t5[\"rougeL\"]\n",
    "}\n",
    "\n",
    "scores_marian = {\n",
    "    \"BLEU\": bleu_marian[\"bleu\"],\n",
    "    \"METEOR\": meteor_marian[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_marian[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_marian[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_marian[\"rougeL\"],\n",
    "}\n",
    "\n",
    "scores_t5_refined = {\n",
    "    \"BLEU\": bleu_t5_refined['bleu'],\n",
    "    \"METEOR\": meteor_t5_refined[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_t5_refined[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_t5_refined[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_t5_refined[\"rougeL\"]\n",
    "}\n",
    "\n",
    "\n",
    "scores_marian_refined = {\n",
    "    \"BLEU\": bleu_marian_refined[\"bleu\"],\n",
    "    \"METEOR\": meteor_marian_refined[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_marian_refined[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_marian_refined[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_marian_refined[\"rougeL\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Crear gráfico\n",
    "labels = list(scores_t5.keys())\n",
    "t5_values = list(scores_t5.values())\n",
    "marian_values = list(scores_marian.values())\n",
    "t5_values_refined = list(scores_t5_refined.values())\n",
    "marian_values_refined = list(scores_marian_refined.values())\n",
    "\n",
    "x = range(len(labels))\n",
    "bar_width = 0.2  # Reducimos el ancho de las barras para que estén más juntas\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar([i - bar_width * 1.5 for i in x], t5_values, width=bar_width, label=\"T5\", align=\"center\")\n",
    "plt.bar([i - bar_width * 0.5 for i in x], marian_values, width=bar_width, label=\"MarianMT\", align=\"center\")\n",
    "plt.bar([i + bar_width * 0.5 for i in x], t5_values_refined, width=bar_width, label=\"T5Refined\", align=\"center\")\n",
    "plt.bar([i + bar_width * 1.5 for i in x], marian_values_refined, width=bar_width, label=\"MarianMTRefined\", align=\"center\")\n",
    "\n",
    "# Configurar etiquetas y leyenda\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel(\"Métricas\")\n",
    "plt.ylabel(\"Puntaje\")\n",
    "plt.title(\"Comparación de Métricas entre T5 y MarianMT (pretrained vs refined)\")\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "caf1dcbb9f4653ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MB7 comentar los resultados obtenidos\n",
    "¿Cómo es eso posible? ¿En que puede influenciar el dataset utilizado?"
   ],
   "id": "dc090f9dfd620e95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modelos sin entrenamiento previo (tabula rasa)\n",
    "\n"
   ],
   "id": "b3e781674b566c12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para utilizar modelos sin entrenamiento previo, o bien disponemos de uno existente que no haya sido entrenado, o en nuestro caso tenemos que resetear los pesos internos de los modelos generando una configuración nueva incial del encoder y decoder.",
   "id": "f1f3e71071f094a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Entrenando un modelo MarianMt desde 0",
   "id": "b19975e0d6deed09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, MarianConfig\n",
    "\n",
    "# Crear configuración para un modelo Marian en blanco\n",
    "config = MarianConfig(\n",
    "    vocab_size=32000,  # Tamaño del vocabulario (ajústalo según tu caso)\n",
    "    max_position_embeddings=512,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=6,\n",
    "    encoder_attention_heads=8,\n",
    "    decoder_attention_heads=8,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1,\n",
    "    pad_token_id=0,\n",
    "    eos_token_id=1,\n",
    "    bos_token_id=2,\n",
    ")\n",
    "\n",
    "# Crear el modelo MarianMT desde cero\n",
    "marian_model_trained = MarianMTModel(config)\n",
    "\n",
    "# Crear un tokenizador vacío (puedes cargar o definir tu propio vocabulario)\n",
    "marian_tokenizer_trained = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "marian_model_trained.resize_token_embeddings(len(marian_tokenizer_trained))\n",
    "\n",
    "\n",
    "marian_model_trained.save_pretrained(\"./models/trained-marian-en-es\")\n",
    "# Ajustar el tamaño del vocabulario del modelo Marian para que coincida con el tokenizador\n",
    "marian_tokenizer_trained.save_pretrained(\"./models/trained-marian-en-es\")\n",
    "\n",
    "try:\n",
    "    text = \"On the table, there were a good number of topics\"\n",
    "    translated_text = translate(text,marian_model_trained, marian_tokenizer_trained)\n",
    "    print(\"MarianMt trained: \",translated_text)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ],
   "id": "e1e19b99530160e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generando un nuevo modelo \"tabula rasa\"",
   "id": "f4f80688c590004"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_trained_marian():\n",
    "    marian_model = MarianMTModel.from_pretrained(\"./models/trained-marian-en-es\")\n",
    "    marian_tokenizer = MarianTokenizer.from_pretrained(\"./models/trained-marian-en-es\")\n",
    "    return marian_model, marian_tokenizer\n"
   ],
   "id": "9a33ab0b5bd93070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Entrenando el modelo",
   "id": "f5e2350e5fc72edf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/training/trained-marian-en-es\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train\", cache_dir=\"./data/iker\")\n",
    "split_dataset = dataset.train_test_split(train_size=1000, test_size=50)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "marian_model_trained, marian_tokenizer_trained = load_trained_marian()\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    global marian_tokenizer_trained\n",
    "    inputs_lang = examples['en']\n",
    "    targets_lang =  examples['es']\n",
    "\n",
    "    model_inputs = marian_tokenizer_trained(inputs_lang, text_target=targets_lang, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "encoded_training_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=marian_tokenizer_trained, model=marian_model_trained)\n",
    "trainer = Trainer(\n",
    "    model=marian_model_trained,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_training_dataset,\n",
    "    eval_dataset=encoded_eval_dataset,\n",
    "    data_collator= collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "marian_model_trained.save_pretrained(\"./models/trained-marian-en-es\")\n",
    "marian_tokenizer_trained.save_pretrained(\"./models/trained-marian-en-es\")\n",
    "\n",
    "marian_trained_model, marian_trained_tokenizer = load_trained_marian()\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "translated_text = translate(text, marian_trained_model, marian_trained_tokenizer)\n",
    "print(\">Marian trained: \", translated_text)"
   ],
   "id": "bd26e8292998a9c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Entrenando un modelo T5 desde 0",
   "id": "a2e3a3cfae777ab8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generando un nuevo modelo \"tabula rasa\"",
   "id": "53e038df8087e901"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "\n",
    "# Crear configuración para un modelo T5 en blanco\n",
    "config = T5Config(\n",
    "    vocab_size=32000,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    num_heads=8,\n",
    "    dropout_rate=0.1,\n",
    "    pad_token_id=0,  # Token de padding\n",
    "    eos_token_id=1,  # Token de fin de secuencia\n",
    "    decoder_start_token_id=0,  # Token de inicio del decodificador\n",
    ")\n",
    "\n",
    "t5_model_trained = T5ForConditionalGeneration(config)\n",
    "\n",
    "t5_tokenizer_trained = T5Tokenizer.from_pretrained(\"vgaraujov/t5-base-translation-en-es\")\n",
    "\n",
    "t5_model_trained.save_pretrained(\"./models/trained-t5-en-es\")\n",
    "t5_tokenizer_trained.save_pretrained(\"./models/trained-t5-en-es\")\n",
    "\n",
    "\n",
    "try:\n",
    "    text = \"On the table, there were a good number of topics\"\n",
    "    translated_text = translate(text,t5_model_trained, t5_tokenizer_trained)\n",
    "    print(\"T5 trained: \",translated_text)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "def load_trained_t5():\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(\"./models/trained-t5-en-es\")\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(\"./models/trained-t5-en-es\")\n",
    "    return t5_model, t5_tokenizer\n"
   ],
   "id": "e4c521071be8e582",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Entrenando el modelo",
   "id": "7d94db1efdeb4bd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MB8\n",
    "\n",
    "Completar el siguiente código para entrenar el modelo T5"
   ],
   "id": "252b5ea0ecd607f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/training/trained-t5-en-es\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# TODO: Remove this. Ejercicio escribir el codigo de debajo\n",
    "dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train\", cache_dir=\"./data/iker\")\n",
    "split_dataset = dataset.train_test_split(train_size=1000, test_size=50)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "t5_model_trained, t5_tokenizer_trained = load_trained_t5()\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    global t5_tokenizer_trained\n",
    "    inputs_lang = examples['en']\n",
    "    targets_lang =  examples['es']\n",
    "\n",
    "    model_inputs = t5_tokenizer_trained(inputs_lang, text_target=targets_lang, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "encoded_training_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer_trained, model=t5_model_trained)\n",
    "trainer = Trainer(\n",
    "    model=t5_model_trained,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_training_dataset,\n",
    "    eval_dataset=encoded_eval_dataset,\n",
    "    data_collator= collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "t5_model_trained.save_pretrained(\"./models/trained-t5-en-es\")\n",
    "t5_tokenizer_trained.save_pretrained(\"./models/trained-t5-en-es\")\n",
    "\n",
    "t5_trained_model, t5_trained_tokenizer = load_trained_t5()\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "translated_text = translate(text, t5_trained_model, t5_trained_tokenizer)\n",
    "print(\">Marian trained: \", translated_text)"
   ],
   "id": "ed5f5e20e9872de7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Comparando todos los modelos",
   "id": "1c953f8fc50cdd09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t5_trained_results = translate_batch(inputs, t5_trained_model, t5_trained_tokenizer)\n",
    "marianmt_trained_results = translate_batch(inputs, marian_trained_model, marian_trained_tokenizer)\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "bleu_t5_trained = bleu_metric.compute(predictions=t5_trained_results, references=expected_results)\n",
    "meteor_t5_trained = meteor_metric.compute(predictions=t5_trained_results, references=expected_results)\n",
    "rouge_t5_trained = rouge_metric.compute(predictions=t5_trained_results, references=expected_results)\n",
    "\n",
    "bleu_marian_trained = bleu_metric.compute(predictions=marianmt_trained_results, references=expected_results)\n",
    "meteor_marian_trained = meteor_metric.compute(predictions=marianmt_trained_results, references=expected_results)\n",
    "rouge_marian_trained = rouge_metric.compute(predictions=marianmt_trained_results, references=expected_results)\n",
    "\n",
    "# Sustituye estos valores por los resultados reales\n",
    "scores_t5 = {\n",
    "    \"BLEU\": bleu_t5['bleu'],\n",
    "    \"METEOR\": meteor_t5[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_t5[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_t5[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_t5[\"rougeL\"]\n",
    "}\n",
    "\n",
    "scores_marian = {\n",
    "    \"BLEU\": bleu_marian[\"bleu\"],\n",
    "    \"METEOR\": meteor_marian[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_marian[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_marian[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_marian[\"rougeL\"],\n",
    "}\n",
    "\n",
    "scores_t5_refined = {\n",
    "    \"BLEU\": bleu_t5_refined['bleu'],\n",
    "    \"METEOR\": meteor_t5_refined[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_t5_refined[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_t5_refined[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_t5_refined[\"rougeL\"]\n",
    "}\n",
    "\n",
    "\n",
    "scores_marian_refined = {\n",
    "    \"BLEU\": bleu_marian_refined[\"bleu\"],\n",
    "    \"METEOR\": meteor_marian_refined[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_marian_refined[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_marian_refined[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_marian_refined[\"rougeL\"],\n",
    "}\n",
    "\n",
    "scores_t5_trained = {\n",
    "    \"BLEU\": bleu_t5_trained['bleu'],\n",
    "    \"METEOR\": meteor_t5_trained[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_t5_trained[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_t5_trained[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_t5_trained[\"rougeL\"]\n",
    "}\n",
    "\n",
    "\n",
    "scores_marian_trained = {\n",
    "    \"BLEU\": bleu_marian_trained[\"bleu\"],\n",
    "    \"METEOR\": meteor_marian_trained[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_marian_trained[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_marian_trained[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_marian_trained[\"rougeL\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Crear gráfico\n",
    "labels = list(scores_t5.keys())\n",
    "t5_values = list(scores_t5.values())\n",
    "marian_values = list(scores_marian.values())\n",
    "t5_values_refined = list(scores_t5_refined.values())\n",
    "marian_values_refined = list(scores_marian_refined.values())\n",
    "t5_values_trained = list(scores_t5_trained.values())\n",
    "marian_values_trained= list(scores_marian_trained.values())\n",
    "\n",
    "x = range(len(labels))\n",
    "bar_width = 0.15  # Ajustar el ancho de las barras\n",
    "\n",
    "# Crear gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar([i - 2 * bar_width for i in x], t5_values, width=bar_width, label=\"T5\")\n",
    "plt.bar([i - bar_width for i in x], marian_values, width=bar_width, label=\"MarianMT\")\n",
    "plt.bar(x, t5_values_refined, width=bar_width, label=\"T5Refined\")\n",
    "plt.bar([i + bar_width for i in x], marian_values_refined, width=bar_width, label=\"MarianMTRefined\")\n",
    "plt.bar([i + 2 * bar_width for i in x], t5_values_trained, width=bar_width, label=\"T5Trained\")\n",
    "plt.bar([i + 3 * bar_width for i in x], marian_values_trained, width=bar_width, label=\"MarianMTTrained\")\n",
    "\n",
    "# Configurar etiquetas y leyenda\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel(\"Métricas\")\n",
    "plt.ylabel(\"Puntaje\")\n",
    "plt.title(\"Comparación de Métricas entre T5 y MarianMT (pretrained vs refined vs trained)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "fb2253e20fd92d44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MB9\n",
    "Comentar los resultados obtenidos, ¿Por qué los modelos entrenados no tienen un buen *performance*? ¿Puede un modelo mal entrenado obtener resultados evaluables?"
   ],
   "id": "56ef7f7b4c3b0cca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
