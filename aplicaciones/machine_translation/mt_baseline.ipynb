{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Uso de modelos para traducción\n",
    "## Machine Translation\n",
    "\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "En este notebook vamos a usar diversos modelos para la traducción automática entre idiomas",
   "id": "233f4109de972cc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "token = \"\"\n",
    "print(\"Hugging Face logging\")\n",
    "login(token)"
   ],
   "id": "763042b074e946dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device_setup = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using: \", device_setup)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "id": "9db7f0f108ac5411",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para simplificar el uso de los distintos modelos, vamos a definir una clase abstracta que implementaremos con los distintos modelos que veamos:",
   "id": "b4e5819f30d7b926"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class AbstractMT(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def translate(self, text, do_sample=True, temperature=0.1):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def translate_batch(self, texts, do_sample=True, temperature=0.1):\n",
    "        return [self.translate(text, do_sample=do_sample, temperature=temperature) for text in texts]\n"
   ],
   "id": "f270d4a5ca221b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelos entrenados inglés-español",
   "id": "4b6a2f5be8ae37d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En [Hugging Face](https://huggingface.co/models) existen multiples modelos, entre los cuales, modelos orientados a machine translation. Entre los más utilizados encontramos:\n",
    "\n",
    "* MarianMT: es un modelo especializado para traducción automática, para nuestro problema vamos a usar la versión preentrenada para inglés a español `Helsinki-NLP/opus-mt-en-es`.\n",
    "* T5: es un modelo versátil preentrenado para varias tareas, incluyendo traducción. Para nuestro problema vamos a utilizar la versión más ligera `t5-small`."
   ],
   "id": "c50f76c8250f09f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Modelo MarianMT\n",
   "id": "826b3dcd4edc8eaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para cargar un modelo MarianMt utilizaremos el método estático de la clase con el mismo nombre `MarianMTModel.from_pretrained` indicando el nombre del modelo, que en este caso es [Helsinki-NLP/opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es) y los parámetro `cache_dir` y `local_files_only` que sirven para que la primera vez se descargue el modelo y las posteriores ejecuciones se hagan usando el modelo descargado en lugar de volver a descargarlo  en cada ejecución.",
   "id": "2e2d64a0158fd879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "def load_pretrained_marian_mt():\n",
    "    global device_setup\n",
    "    marian_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\", cache_dir=\"./models/pretrained_marian_en-es\", local_files_only=False).to(device_setup)\n",
    "    marian_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\", cache_dir=\"./models/pretrained_marian_en-es\", local_files_only=False)\n",
    "    return marian_model, marian_tokenizer\n",
    "\n",
    "marian_model, marian_tokenizer = load_pretrained_marian_mt()\n",
    "\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "inputs = marian_tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = marian_model.generate(**inputs, do_sample=False, temperature=1.0)\n",
    "translated_text = marian_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ],
   "id": "5da641c9d42b7170",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notese que para utilizar el modelo, tenemos que pasar las entradas por el tokenizador y por el modelo. Para pasarla por el modelo utilizamos el método `.generate()` que posee varios parámetros interesantes:\n",
    "\n",
    "* `do_sample` indica si el modelo debe muestrear probabilidades de las palabras posibles en cada paso, en lugar de elegir únicamente la palabra con la probabilidad más alta (modo greedy). Si valor es False (predeterminado)no se hace ningún muestreo y el modelo selecciona la palabra con la probabilidad más alta en cada paso (modo greedy o beam search). En cambio, si su valor es True se hacen muestreos según las probabilidades de las palabras posibles, lo que permite generar salidas más variadas. Es decir, utilizando `do_sample=False` se obtienen traducciones más deterministas y consistentes, en cambio, usando `do_sample=True` se obtienen resultados más creativos o variados, pero menos deterministas.\n",
    "* `temperature` escala las probabilidades de las palabras generadas por el modelo, controlando la \"aleatoriedad\" del muestreo. Puede tomar los siguientes valores: un valor mayor a 1.0 aumenta la diversidad favoreciendo palabras menos probables; un valor menor a 1.0 reduce la diversidad, concentrándose en palabras más probables; un valor de 1.0 (predeterminado) utiliza las probabilidades originales del modelo. Por lo tanto, cuando `do_sample=False`, este parámetro no tiene efecto (porque no hay muestreo). Por el contrario, si `do_sample=True` regula la aleatoriedad en las generaciones.\n",
    "* `num_beams` activa el beam search, una técnica que explora múltiples caminos en paralelo para encontrar la secuencia más probable. Puede tomar distintos valores: `num_beams=1` (predeterminado) indica un modo de operación greedy; `num_beams>1` indica el número de caminos a explorar. Este parámetro puede mejorar la calidad de las generaciones a costa de mayor tiempo de cómputo si se aumenta su valor."
   ],
   "id": "a96fde84b259c121"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Usando el código anterior, vamos a empaquetarlo en una clase hija de AbstractMT",
   "id": "2e9c3f99cee7b2da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MarianMT(AbstractMT):\n",
    "\n",
    "    def __init__(self, model_name=\"Helsinki-NLP/opus-mt-en-es\", cache_dir=\"./models/opus-mt-en-es\"):\n",
    "        global device_setup\n",
    "        self.model = MarianMTModel.from_pretrained(model_name, cache_dir=cache_dir).to(device_setup)\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "    def translate(self, text, do_sample=True, temperature=0.1):\n",
    "        global device_setup\n",
    "        # Tokenizar la entrada\n",
    "        inputs_tokenized = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device_setup)\n",
    "        # Traducir\n",
    "        translated_tokens = self.model.generate(**inputs_tokenized, max_length=200, do_sample=do_sample,  temperature=temperature)\n",
    "        # Decodificar salida\n",
    "        return self.tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n"
   ],
   "id": "d72aea2b0332abdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MTB1\n",
    "Usar la clase `MarianMT` para traducir la frase  \"On the table, there were a good number of topics\""
   ],
   "id": "ce98b620112722cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mariam_mt = None\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "\n",
    "# TODO: traducir la frase e imprimirla por pantalla"
   ],
   "id": "787da9e92e7e9258",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MTB2\n",
    "Usar la clase `MarianMT` para traducir el array de frases"
   ],
   "id": "743f0d09c7f77609"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "texts = [\"On the table, there were a good number of topics\", \"manners maketh the man\"]\n",
    "\n",
    "# TODO: traducir las frases e imprimirlas por pantalla"
   ],
   "id": "c088f94396e1c5b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MTB3\n",
    "\n",
    "Cambiar los parámetros `do_sample=True` y probar `temperature` con valores como 0.1, 0.7, 0.5, 1, 1.5, 2.0, 4.0, 8.0 para que el modelo haga traducciones o generaciones lo menos deterministas posible, es decir, produzca alucinaciones."
   ],
   "id": "d415f2fecaab454"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"On the table, there were a good number of topics\"\n",
    "\n",
    "# TODO: Para cada una de las temperaturas generar una traducción de la frase"
   ],
   "id": "26ccabd2fb263e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Repita el mismo código para la frase `manners maketh the man`",
   "id": "1fef91e0227da244"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"manners maketh the man\"\n",
    "# TODO: Para cada una de las temperaturas generar una traducción de la frase"
   ],
   "id": "d871d27b11277ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Modelo T5",
   "id": "8d7f876922cfc2e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para cargar un modelo T5 utilizaremos el método estático de la clase con el mismo nombre `T5ForConditionalgeneration.from_pretrained` indicando el nombre del modelo, que en este caso es [t5-small](https://huggingface.co/google-t5/t5-small) y los parámetro `cache_dir` y `local_files_only` que sirven para que la primera vez se descargue el modelo y las posteriores ejecuciones se hagan usando el modelo descargado en lugar de volver a descargarlo en cada ejecución. Además del modelo small, existen variantes más complejas y pesadas, como el [t5-base](https://huggingface.co/google-t5/t5-base) o el [t5-large](https://huggingface.co/google-t5/t5-large).",
   "id": "36f71e9888a3e470"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "def load_pretrained_t5():\n",
    "    global device_setup\n",
    "    # También existen t5-base o t5-large\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(\"vgaraujov/t5-base-translation-en-es\", cache_dir=\"./models/pretrained_t5_en-es\", local_files_only=False).to(device_setup)\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(\"vgaraujov/t5-base-translation-en-es\", cache_dir=\"./models/pretrained_t5_en-es\", local_files_only=False)\n",
    "    return t5_model, t5_tokenizer\n",
    "\n",
    "t5_model, t5_tokenizer = load_pretrained_t5()\n",
    "\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "inputs = t5_tokenizer(text, return_tensors=\"pt\")\n",
    "translated_tokens = t5_model.generate(**inputs, do_sample=False, max_length=200, temperature=1.0)\n",
    "translated_text = t5_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MB4\n",
    "\n",
    "Igual que antes, para facilitar el uso del modelo vamos a empaquetar el código anterior en una clase llamada `T5MT`. Cree dicha clase en la siguiente celda\n"
   ],
   "id": "119e321b059df5ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "class T5MT(AbstractMT):\n",
    "\n",
    "    # TODO: defina los argumentos necesarios en el constructor para poder instanciar objetos con distintos modelos y tokenizadores T5\n",
    "    def __init__(self):\n",
    "        # TODO: inicializar el modelo y el tokenizador\n",
    "        pass\n",
    "\n",
    "    def translate(self, text, do_sample=True, temperature=0.1):\n",
    "        # TODO: defina el código necesario para traducir la frase\n",
    "        pass\n"
   ],
   "id": "d29f1b056740516e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t5_mt = T5MT()\n",
    "\n",
    "text = \"On the table, there were a good number of topics\"\n",
    "translated_text = t5_mt.translate(text)\n",
    "print(translated_text)"
   ],
   "id": "7bafd4abb0db7a6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "texts = [\"On the table, there were a good number of topics\", \"manners maketh the man\"]\n",
    "translated_text = t5_mt.translate_batch(texts)\n",
    "print(translated_text)"
   ],
   "id": "bf1cf0f97ca0daa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MTB5\n",
    "\n",
    "Cambiar los parámetros `do_sample=True` y probar `temperature` con valores como 0.1, 0.7, 0.5, 1, 1.5, 2.0, 4.0, 8.0 para que el modelo haga traducciones o generaciones lo menos deterministas posible, es decir, produzca alucinaciones.\n"
   ],
   "id": "be4e6c773946b808"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"On the table, there were a good number of topics\"\n",
    "# TODO: Para cada una de las temperaturas generar una traducción de la frase"
   ],
   "id": "870ecfec2f7a8b7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Repita el mismo código para la frase `manners maketh the man`",
   "id": "de7000129537927e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"manners maketh the man\"\n",
    "# TODO: Para cada una de las temperaturas generar una traducción de la frase"
   ],
   "id": "4a8ad8864ad5062b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea conjunta de la clase MTB3: ¿Qué modelo es mejor?\n",
    "\n",
    "Justificar que modelo de los anteriores traduce mejor, ¿Qué prueba numérica sustenta la respuesta? ¿Es una justificación reproducible y objetiva?"
   ],
   "id": "1e4068a0e6ed6a4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Comparación de modelos",
   "id": "3d4a60285622e9c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para comparar modelos, es necesario calcular métricas objetivas y reproducibles. Estas métricas normalmente dependen de un `gold-standard`, la verdad absoluta, lo que nos permite para una entrada ver si el modelo es capaz de generar una salida tal y como el `gold-standard` indica. Sin embargo, para problemas más difusos como la traducción las métricas normalmente no son simples. En particular, para la traducción se suelen utilizar tres métricas: BLEU (Bilingual Evaluation Understudy), METEOR (Metric for Evaluation of Translation with Explicit ORdering), y ROUGE (Recall-Oriented Understudy for Gisting Evaluation).\n",
    "\n",
    "* **BLEU** mide la similitud entre una traducción generada y la esperada. Para ello, calcula la similitud usando ngrams (unigramas, bigramas, etc.) entre la salida obtenida y la esperada (presente en el `gold-standard`); además, penaliza las salidas muy cortas. El resultado de esta métrica es un valor porcentual donde a mayor valor se considera que más **precisa** es la traducción. Una limitación conocida es que tiende a favorecer las traducciones literales por el uso de ngrams.\n",
    "\n",
    "* **METEOR** es una métrica similar a **BLEU** pero que se centra en evaluar la similitud con sinonimias, stemming y coincidencias exactas. Esto hace que sea menos sensible a las traducciones literales y más flexible con traducciones menos literales. Internamente, esta métrica calcula precisión, recall y un F1 ajustado. Su resultado es un valor de 0 a 1, donde a mayor valor mejor es la traducción. Una limitación conocida es que es computacionalmente más costoso de **BLEU**, lo que lo hace menos usado que **BLEU** o **ROUGE**.\n",
    "\n",
    "* **ROUGE** evalúa la calidad de la traducción comparando la salida generada con la del `gold-standard` usado en resúmenes automáticos (n-grams y la longitud de las subsecuencias comunes). Tiene tres variantes principales: `ROUGE-1` compara unigramas, `ROUGE-2` compara bigramas, `ROUGE-L` compara la subsecuencia común más larga. Su resultado es un valor entre 0 y 1, donde a mayor valor mejor es la traducción; normalmente esta métrica se asocia al **recall**. Como limitaciones conocidas, esta metrica es insensible a la sinonimia o variaciones semánticas, además, penaliza traducciones \"creativas\".\n",
    "\n",
    "Para calcular estas métricas vamos a hacer uso del paquete `evaluate`"
   ],
   "id": "27ff7d499250b1c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea MTB5\n",
    "\n",
    "Cargue en una variable el dataset de `okezieowen/english_to_spanish` y almacénelo en la carpeta `./data/okezieowen`. Despúes, prepare un conjunto de testing que contenga 50 frases del dataset. No \"baraje\" las frases del dataset ni utilice ningún `seed`."
   ],
   "id": "7e5800309287e57b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carga y preparación del dataset\n",
    "dataset = load_dataset(\"okezieowen/english_to_spanish\", split=\"train\", cache_dir=\"./data/okezieowen\")\n",
    "split_dataset = dataset.train_test_split(test_size=50)\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# Imprimir el tamaño del dataset\n",
    "print(\"Evaluation size: \", len(eval_dataset))\n",
    "print(eval_dataset)"
   ],
   "id": "bfc7ac6d2ee28c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Antes de poder aplicar las métricas, necesitamos construir el código que usando el dataset, genere una variable con los resultados esperados y, por otro lado, genere las salidas de cada modelo.",
   "id": "357545a20e8542a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "expected_results = [ [row['Spanish']] for row in eval_dataset]\n",
    "inputs = [row['English'] for row in eval_dataset]\n",
    "\n",
    "# Traducir todas las frases del dataset\n",
    "results = {}\n",
    "results['t5'] = t5_mt.translate_batch(inputs)\n",
    "results['marian'] = mariam_mt.translate_batch(inputs)"
   ],
   "id": "2b6ec2cacd1d56c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluación de modelos",
   "id": "daf2ca10be2a0c40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "bleu_t5 = bleu_metric.compute(predictions=results['t5'], references=expected_results)\n",
    "meteor_t5 = meteor_metric.compute(predictions=results['t5'], references=expected_results)\n",
    "rouge_t5 = rouge_metric.compute(predictions=results['t5'], references=expected_results)\n",
    "\n",
    "bleu_marian = bleu_metric.compute(predictions=results['marian'], references=expected_results)\n",
    "meteor_marian = meteor_metric.compute(predictions=results['marian'], references=expected_results)\n",
    "rouge_marian = rouge_metric.compute(predictions=results['marian'], references=expected_results)\n",
    "\n",
    "print(\"Metric -- T5 -- MarianMT \\n\")\n",
    "print(\"BLEU --- \",bleu_t5['bleu'], \" --- \", bleu_marian['bleu'])\n",
    "print(\"METEOR --- \",meteor_t5['meteor'], \" --- \", meteor_marian['meteor'])\n",
    "print(\"ROUGE1 --- \",rouge_t5['rouge1'], \" --- \", rouge_marian['rouge1'])\n",
    "print(\"ROUGE2 --- \",rouge_t5['rouge2'], \" --- \", rouge_marian['rouge2'])\n",
    "print(\"ROUGEL --- \",rouge_t5['rougeL'], \" --- \", rouge_marian['rougeL'])\n"
   ],
   "id": "afe2264654e373b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores_t5 = {\n",
    "    \"BLEU\": bleu_t5['bleu'],\n",
    "    \"METEOR\": meteor_t5[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_t5[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_t5[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_t5[\"rougeL\"]\n",
    "}\n",
    "\n",
    "scores_marian = {\n",
    "    \"BLEU\": bleu_marian[\"bleu\"],\n",
    "    \"METEOR\": meteor_marian[\"meteor\"],\n",
    "    \"ROUGE-1\": rouge_marian[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_marian[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_marian[\"rougeL\"],\n",
    "}\n",
    "\n",
    "# Crear gráfico\n",
    "labels = list(scores_t5.keys())\n",
    "t5_values = list(scores_t5.values())\n",
    "marian_values = list(scores_marian.values())\n",
    "\n",
    "x = range(len(labels))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, t5_values, width=0.4, label=\"T5\", align=\"center\")\n",
    "plt.bar([i + 0.4 for i in x], marian_values, width=0.4, label=\"MarianMT\", align=\"center\")\n",
    "\n",
    "# Configurar etiquetas y leyenda\n",
    "plt.xticks([i + 0.2 for i in x], labels)\n",
    "plt.xlabel(\"Métricas\")\n",
    "plt.ylabel(\"Puntaje\")\n",
    "plt.title(\"Comparación de Métricas entre T5 y MarianMT\")\n",
    "plt.legend()"
   ],
   "id": "d261e0332c7bd645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Como se puede observar los resultados no son muy buenos, esto se debe a la naturaleza juridica del dataset que estamos usando. Veamos un ejemplo en particular:",
   "id": "6395a4156a0d66bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "expected_output = \"Su Señoría, si así lo desea, podrá plantear esta cuestión en ese momento, es decir, el jueves antes de que se presente el informe.\"\n",
    "text = \"That is precisely the time when you may, if you wish, raise this question, i.e. on Thursday prior to the start of the presentation of the report.\"\n",
    "print(\"Oringinal text: \", text)\n",
    "print(\"Marian translation: \", mariam_mt.translate(text))\n",
    "print(\"T5 translation: \", t5_mt.translate(text))\n",
    "print(\"Expected: \", expected_output)"
   ],
   "id": "f3862434bf0b5004",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
