{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Translation datasets\n",
   "id": "2e34e1a83be710ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Datasets listos para su uso",
   "id": "b082ed08d9a82151"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En [Hugging Face](https://huggingface.co/datasets) hay multiples datasets listos para su uso. Para el caso particular de la traducción de idiomas (Machine Translation) podemos filtrar los datasets que se ajusten a nuestra necesidad. Por ejemplo, vamos [a utilizar un dataset](https://huggingface.co/datasets/Iker/Document-Translation-en-es) que se define como:\n",
    "*\"Este conjunto de datos contiene 10533 artículos de noticias de [ELiRF/dacsa](https://huggingface.co/datasets/ELiRF/dacsa) traducidos del español al inglés utilizando GPT-3.5-turbo. El conjunto de datos está pensado para entrenar un modelo de traducción de textos del inglés al español y viceversa. El conjunto de datos también es útil para evaluar modelos de traducción automática a nivel de documento.\"*\n",
    "A continuación vemos como se puede descargar dicho dataset desde hugging face y dividirlo en dos conjuntos, de entrenamiento y de pruebas/validación."
   ],
   "id": "58bf256538ce4d92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### (Des)Cargar un dataset\n",
    "El método `load_dataset` se utiliza para cargar datasets del catálogo de Hugging Face o desde fuentes locales. Este método soporta varios parámetros que permiten personalizar cómo se carga el dataset."
   ],
   "id": "db10212bcd1485ff"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train\")\n",
    "\n",
    "print(\"Tipo de dato: \", type(dataset))\n",
    "print(\"Tamaño del dataset: \", len(dataset))\n",
    "print(\"--- Conjunto de pruebas ---\")\n",
    "print(dataset)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Primera fila es ---\")\n",
    "print(dataset[\"es\"][0])"
   ],
   "id": "45bc25d563c11808",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Primera fila en ---\")\n",
    "print(dataset[\"en\"][0])"
   ],
   "id": "968050f78fb39e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Existen diferentes parámetros que se pueden especificar que es importante conocer:",
   "id": "e57ced1968de705e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Parámetro split\n",
    "\n",
    "El parámetro `split` especifica qué parte del dataset se quiere cargar. Normalmente, toma como valor `train` para indicar datos de entrenamiento, `test` para indicar datos de prueba y `validation` para indicar datos de validación (si el dataset tiene esta partición); además de otros valores menos frecuentes."
   ],
   "id": "f4a7eb3a12666c27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_ds(name, split_type):\n",
    "    try:\n",
    "        ds = load_dataset(name, split=split_type)\n",
    "        print(name, \" tamaño del dataset de \",split_type,\" \", len(ds))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Prueba con dataset Iker/Document-Translation-en-es\n",
    "load_ds(\"Iker/Document-Translation-en-es\", split_type=\"train\")\n",
    "load_ds(\"Iker/Document-Translation-en-es\", split_type=\"test\")\n",
    "load_ds(\"Iker/Document-Translation-en-es\", split_type=\"validation\")\n",
    "# Prueba con dataset de imdb\n",
    "load_ds(\"imdb\", split_type=\"train\")\n",
    "load_ds(\"imdb\", split_type=\"test\")\n",
    "load_ds(\"imdb\", split_type=\"validation\")"
   ],
   "id": "df41ff130ab0f322",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Como se puede observar, el valor de `split` depende de como se ha preparado el dataset. En el caso de `Iker/Document-Translation-en-es` solo hay un split de training mientras que en `imdb` existe de training, test, pero no de validación y en su lugar existe uno llamado `unsupervised`.\n",
    "\n",
    "Además, se pueden crear subconjuntos personalizados añadiendo los carácteres `[]` al conjunto de `split`"
   ],
   "id": "41c13aff62e4ce73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar solo el 10% de los datos de entrenamiento\n",
    "subset_dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train[:10%]\")\n",
    "print(\"Tamaño del subset (10% de entrenamiento): \", len(subset_dataset))"
   ],
   "id": "60ab6d58bacfb3d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar ejemplos desde el índice 100 hasta el 200\n",
    "range_dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train[100:200]\")\n",
    "print(\"Tamaño del subset (índices 100-200): \", len(range_dataset))"
   ],
   "id": "70516dc09fd6413c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Además se pueden combinar distintas particiones:",
   "id": "1251b1d6cc82baa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combinar datos de entrenamiento y prueba\n",
    "combined_dataset = load_dataset(\"imdb\", split=\"train+test\")\n",
    "print(\"Tamaño del dataset combinado: \",len(combined_dataset))"
   ],
   "id": "3b861ce2b24e4e1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Parámetro `cache_dir`\n",
    "\n",
    "Este parámetro es *especialmente importante para datasets grandes* ya que permite guardar una copia del dataset en local para poder utilizarlo a posteriori sin tener que volver a descargarlo."
   ],
   "id": "f55db27a57d82156"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(\"Existe dataset: \", os.path.exists(\"./data/iker\"))\n",
    "# Almacenar el dataset en un directorio específico\n",
    "cached_dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train\", cache_dir=\"./data/iker\")\n",
    "print(\"Existe dataset: \", os.path.exists(\"./data/iker\"))"
   ],
   "id": "85f8046f693ab4c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Si el fragmento anterior se ejecuta dos veces, la segunda se ve como el dataset ya no es descargado.",
   "id": "becf1417c48da598"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Parámetro `data_files`\n",
    "\n",
    "Este parámetro se utiliza para indicar como dataset un fichero local el cuál será cargado. Los datasets locales  puede estar escritos en diversos formatos: `csv`, `json`, o `parquet`."
   ],
   "id": "1189d358cf7780e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cargar un dataset desde un archivo CSV local\n",
    "local_dataset = load_dataset(\"csv\", data_files={\"train\": \"./provided/sample-ds.csv\", \"test\": \"./provided/sample-ds.csv\"}, split=\"train\")\n",
    "print(f\"Tamaño del dataset local: {len(local_dataset)}\")"
   ],
   "id": "46cb1f9cf2bda3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Cargar un dataset desde memoria\n",
    "\n",
    "Por último, es posible cargar un dataset desde una variable que tengamos en el código. Para ello se usa el método estático `Dataset.from_dict()` que carga un diccionario de python."
   ],
   "id": "68bdb45d442e0f9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "import io\n",
    "\n",
    "# Importante el formato del dic !\n",
    "raw_dataset_json = { \"translations\": [\n",
    "    {\"español\": \"Hola, ¿cómo estás?\", \"latin\": \"Salve, quid agis?\"},\n",
    "    {\"español\": \"Buenos días\", \"latin\": \"Bonum mane\"},\n",
    "    {\"español\": \"Buenas noches\", \"latin\": \"Bonum nocte\"},\n",
    "    {\"español\": \"Gracias por tu ayuda\", \"latin\": \"Gratias tibi ago pro auxilio tuo\"},\n",
    "    {\"español\": \"¿Dónde está la biblioteca?\", \"latin\": \"Ubi est bibliotheca?\"},\n",
    "    {\"español\": \"Tengo hambre\", \"latin\": \"Esurio\"},\n",
    "    {\"español\": \"Tengo sed\", \"latin\": \"Sitiens sum\"},\n",
    "    {\"español\": \"Me gusta leer libros\", \"latin\": \"Libros legere mihi placet\"},\n",
    "    {\"español\": \"¿Cuánto cuesta esto?\", \"latin\": \"Quanti constat hoc?\"},\n",
    "    {\"español\": \"Por favor, repítelo\", \"latin\": \"Quaeso, iterum dic\"},\n",
    "    {\"español\": \"Disculpa, no entiendo\", \"latin\": \"Ignosce, non intellego\"},\n",
    "    {\"español\": \"Mi nombre es Juan\", \"latin\": \"Nomen meum est Ioannes\"},\n",
    "    {\"español\": \"Feliz cumpleaños\", \"latin\": \"Felix natalis\"},\n",
    "    {\"español\": \"Hasta luego\", \"latin\": \"Vale\"},\n",
    "    {\"español\": \"Estoy perdido\", \"latin\": \"Erravi\"},\n",
    "    {\"español\": \"¿Puedes ayudarme?\", \"latin\": \"Potesne me adiuvare?\"},\n",
    "    {\"español\": \"¿Cuál es tu nombre?\", \"latin\": \"Quod est nomen tuum?\"},\n",
    "    {\"español\": \"Estoy aprendiendo latín\", \"latin\": \"Disco latinam linguam\"},\n",
    "    {\"español\": \"Es un placer conocerte\", \"latin\": \"Placuit mihi te convenire\"},\n",
    "    {\"español\": \"Lo siento mucho\", \"latin\": \"Multum doleo\"},\n",
    "    {\"español\": \"Estoy cansado\", \"latin\": \"Fessus sum\"},\n",
    "    {\"español\": \"Vengo de España\", \"latin\": \"Ex Hispania venio\"},\n",
    "    {\"español\": \"Me gusta mucho la música\", \"latin\": \"Musica mihi multum placet\"},\n",
    "    {\"español\": \"Quiero ir al mercado\", \"latin\": \"Volo ire ad forum\"},\n",
    "    {\"español\": \"Estoy buscando un restaurante\", \"latin\": \"Quaero tabernam cauponiam\"},\n",
    "    {\"español\": \"¿Hablas español?\", \"latin\": \"Loquerisne Hispanice?\"},\n",
    "    {\"español\": \"Hoy es un buen día\", \"latin\": \"Hodie est dies bonus\"},\n",
    "    {\"español\": \"Estoy de acuerdo contigo\", \"latin\": \"Assentior tibi\"},\n",
    "    {\"español\": \"La comida está deliciosa\", \"latin\": \"Cibus est deliciosus\"},\n",
    "    {\"español\": \"Quiero aprender más\", \"latin\": \"Volo plus discere\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Load the dataset from the JSON buffer\n",
    "dataset = Dataset.from_dict(raw_dataset_json)\n",
    "\n",
    "# Print the dataset\n",
    "print(dataset)\n",
    "split_dataset = dataset.train_test_split(train_size=0.5, shuffle=True, seed=42)\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento: \", len(train_dataset))\n",
    "print(\"Tamaño del conjunto de prueba: \", len(eval_dataset))\n",
    "print(\"Tamaño total: \", len(eval_dataset) + len(train_dataset))"
   ],
   "id": "31632a350dffb94b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Preparar un dataset para entrenar y evaluar/validar",
   "id": "b577bac42bed8dd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Haciendo uso del método `train_test_split` que possen los objetos de tipo `dataset` se pueden generar los datasets para el entrenamiento y la validación. Es importante conocer algunos parámetros:\n",
    "* `test_size` y `train_size` sirven para indicar el tamaño del conjunto de evaluación/validación y de entrenamiento respectivamente. Aceptan un valor `n` que indica el procentaje de ejemplos (`n<1.0`) o el número exacto a usar (`n>1`)\n",
    "* shuffle: Determina si los datos deben barajarse antes de dividirlos. Si shuffle=True (por defecto), los datos se distribuyen aleatoriamente entre los subconjuntos. Si es False, los datos se dividen en su orden original.\n",
    "* seed: Establece la semilla para la aleatoriedad del barajado. Esto permite reproducir divisiones consistentes en diferentes ejecuciones al usar el mismo valor de semilla.\n"
   ],
   "id": "bdcb538a1fc5eae6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Iker/Document-Translation-en-es\", split=\"train\", cache_dir=\"./data/iker\")\n",
    "split_dataset = dataset.train_test_split(train_size=0.5, shuffle=True, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento: \", len(train_dataset))\n",
    "print(\"Tamaño del conjunto de prueba: \", len(eval_dataset))\n",
    "print(\"Tamaño total: \", len(eval_dataset) + len(train_dataset))\n"
   ],
   "id": "13af4f2eed636f2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea DS1\n",
    "\n",
    "Buscar el dataset `okezieowen/english_to_spanish` en hugging face para poder ser usado para traducción (machine translation) entre inglés y español. Escribir el código python necesario para que usando dicho dataset se genere un conjunto de entrenamiento y otro de evaluación/validación. Además, hacer que dicho dataset se guarde en cache local bajo la carpeta `./data/`\n",
    "\n"
   ],
   "id": "4c5c44d4634e4b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cae782d2d87d833e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Construir un dataset\n",
    "\n"
   ],
   "id": "162ef62d29d39806"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para construir un dataset orientado a la traducción, sabemos que necesitamos construir un `dict` de python donde tengamos una estructura:\n",
    "````\n",
    "{ \"translations\" : [\n",
    "  { \"l1\" : \"...\", \"l2\" : \"....\", ... }\n",
    " ] }\n",
    "````\n",
    "Donde `li` es el idioma en el que está codificada la frase"
   ],
   "id": "bcb71026b5d01f57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea DS2\n",
    "\n",
    "Construir un dataset para los idiomas del señor de los anillos utilizando [los subtitulos de las películas del señor de los anillos](https://github.com/howa003/complete-elvish-lotr-subtitles). Preferiblemente, el dataset tiene que por lo menos contener el idioma [Sindarin](https://es.wikipedia.org/wiki/Sindarin#:~:text=El%20sindarin%20(llamado%20tambi%C3%A9n%20%C3%A9lfico,hablada%20en%20la%20Tierra%20Media) (Élfico Gris).\n",
    "\n"
   ],
   "id": "af900d4f4ac1165d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9c7c0745b55be73f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una posible solución *out of the box* se encuentra implementada en el fichero `./libs/sindarin_builder.py`",
   "id": "c795ec111494d329"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from libs.lotr_ds_builder import build_datasets\n",
    "\n",
    "train, test = build_datasets()\n",
    "print(train.keys())\n",
    "print(train['Sindarin'])"
   ],
   "id": "7ebaa4330d1b509e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
