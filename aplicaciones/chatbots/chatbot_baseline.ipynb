{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed980c1",
   "metadata": {},
   "source": "# Chatbots"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "token = \"\"\n",
    "print(\"Hugging Face logging\")\n",
    "login(token)"
   ],
   "id": "d58d996ff8716ae2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n"
   ],
   "id": "9b8a4927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creación de una clase para chatbot básico",
   "id": "40937cb6"
  },
  {
   "cell_type": "code",
   "id": "0102e2b3",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class Chatbot:\n",
    "\n",
    "    def __init__(self, chatbot_model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", cache_dir=\"./models/TinyLlama-1.1B-Chat-v1.0\"):\n",
    "        global device\n",
    "        self.model_device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(chatbot_model)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            chatbot_model,\n",
    "            torch_dtype = torch.float32 if device == \"mps\" else (torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "            cache_dir=cache_dir,\n",
    "            local_files_only=False\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "    def __run_prompt(self, prompt, do_sample, temperature, top_p, show_prompt):\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(conversation=prompt, tokenize=False, return_dict=False, add_generation_prompt=True)\n",
    "\n",
    "        # Tokenizar\n",
    "        inputs = self.tokenizer(formatted_prompt, truncation=False,return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.model_device) for k, v in inputs.items()}\n",
    "\n",
    "        # Muestra infomacion de log\n",
    "        if show_prompt:\n",
    "          print(formatted_prompt)\n",
    "          print(\"--- Token size: ---\")\n",
    "          [print(\"\\t\", k, \": \", len(v[0])) for k, v in inputs.items()]\n",
    "          print(\"-------------------\")\n",
    "\n",
    "        # Generar respuesta\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=250,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decodificar y limpiar respuesta\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def __format_prompt(self, user_prompt, system_prompt):\n",
    "        return [\n",
    "            { \"role\": \"system\", \"content\": system_prompt,},\n",
    "            { \"role\": \"user\", \"content\":  user_prompt},\n",
    "        ]\n",
    "\n",
    "\n",
    "    def answer(self, user_prompt, system_prompt, do_sample=True, temperature=0.1,\n",
    "               top_p=0.9, show_prompt=False):\n",
    "        prompt = self.__format_prompt(user_prompt, system_prompt)\n",
    "        return self.__run_prompt(prompt, do_sample, temperature, top_p, show_prompt)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chatbot = Chatbot()\n",
    "\n",
    "answer = chatbot.answer(user_prompt=\"How many helicopters can a human eat in one sitting?\", system_prompt=\"You are a friendly chatbot who always responds in the style of a pirate\", show_prompt=True)\n",
    "\n",
    "print(answer)\n"
   ],
   "id": "25f66f65a33bc712",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea CB1\n",
    "\n",
    "Vamos a consultar al modelo para que nos resuelva una duda de programación, en particular, `El siguiente código python da un error, podrías arreglarlo por mi: x=1; y='1'; z = x+y`. ¿Cúal sería un buen prompt para el sistema?\n",
    "Generar distintas respuestas variando la temperatura, usar 0.1, 0.7, 1, y 1.3"
   ],
   "id": "2ee6b4237f3e1174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: usar temperatura 0.1",
   "id": "3dd3419153678a27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: usar temperatura 0.7",
   "id": "568e7c8a6e0b885c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: usar temperatura 1",
   "id": "3c861495a612a819",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: usar temperatura 1.3",
   "id": "afcd37d5923ba21d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea CB2\n",
    "\n",
    "En los ejemplos anteriores la interacción con el chatbot finaliza al terminar la ejecución, vamos a construir una interacción humano-máquina perpetua. Para ello, escriba el código necesario para pedirle al usuario el prompt necesario, pasárselo al modelo, e imprimir su respuesta. Este comportamiento tiene que repetirse sin parar."
   ],
   "id": "8e20e97fd66fa50d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: código de prompting perpetuo\n",
   "id": "1471f69fea54de8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creación de una clase para chatbot avanzado",
   "id": "91db362208fbb9a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea CB3\n",
    "\n",
    "En el código anterior, las respuestas que nos proporciona el modelo no se tienen en cuenta para la siguiente interacción. Vamos a generar una estructura de datos para que se vayan guardando en la conversación. Para ello modifique la clase `Chatbot`."
   ],
   "id": "4a9d233c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class DialogueController:\n",
    "\n",
    "    def __init__(self, system_prompt, assistant_token=\"<|assistant|>\"):\n",
    "        # TODO: crear un atributo prompt e inicializarlo con la parte correspondiente al sistema\n",
    "        pass\n",
    "\n",
    "    def get_prompt(self):\n",
    "        # TODO: devolver el prompt\n",
    "        pass\n",
    "    def add_user_prompt(self, user_prompt, role=\"user\"):\n",
    "        # TODO: añadir el nuevo prompt del usuario\n",
    "        # TODO: devolver el prompt actualizado\n",
    "        pass\n",
    "\n",
    "    def add_assistant_prompt(self, assistant_prompt):\n",
    "        # TODO: añadir el nuevo prompt del asistente\n",
    "        # TODO: hay que tener en cuenta la estructura del assistant_prompt\n",
    "        pass\n",
    "\n",
    "\n",
    "class Chatbot2:\n",
    "\n",
    "    def __init__(self, controller, device_setup, chatbot_model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", cache_dir=\"./models/TinyLlama-1.1B-Chat-v1.0\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(chatbot_model)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            chatbot_model,\n",
    "            torch_dtype = torch.float32 if device_setup == \"mps\" else (torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "            cache_dir=cache_dir,\n",
    "            local_files_only=False\n",
    "        ).to(device_setup)\n",
    "        self.model_device = device_setup\n",
    "        self.dialogue_controller = controller\n",
    "\n",
    "    def __run_prompt(self, prompt, do_sample, temperature, top_p, show_prompt):\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(conversation=prompt, tokenize=False, return_dict=False, add_generation_prompt=True)\n",
    "\n",
    "        # Tokenizar\n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.model_device) for k, v in inputs.items()}\n",
    "\n",
    "        # Muestra infomacion de log\n",
    "        if show_prompt:\n",
    "          print(formatted_prompt)\n",
    "          print(\"--- Token size: ---\")\n",
    "          [print(\"\\t\", k, \": \", len(v[0])) for k, v in inputs.items()]\n",
    "          print(\"-------------------\")\n",
    "\n",
    "        # Generar respuesta\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decodificar y limpiar respuesta\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    def answer(self, user_prompt, do_sample=True, temperature=0.1, top_p=0.9, show_prompt=False):\n",
    "        # Actualiza el prompt\n",
    "        prompt = self.dialogue_controller.add_user_prompt(user_prompt)\n",
    "        # Resolver prompt\n",
    "        answer = self.__run_prompt(prompt, do_sample, temperature, top_p, show_prompt)\n",
    "        # Actualiza el prompt con la respuesta del asistente\n",
    "        self.dialogue_controller.add_assistant_prompt(answer)\n",
    "        return answer\n",
    "\n"
   ],
   "id": "ab1739a65ad70c1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "answer = \"Please ask something: \"\n",
    "dialog_controller = DialogueController(system_prompt=\"Responde mis dudas como un chatbot educado de manera coincisa\")\n",
    "chatbot_windowing = Chatbot2(controller=dialog_controller, device_setup=device)\n",
    "\n",
    "while True:\n",
    "    user_input = input(answer)\n",
    "    answer = chatbot_windowing.answer(user_prompt=user_input, show_prompt=True, temperature=0.1)\n",
    "    print(\"----------------------\")\n"
   ],
   "id": "ca43f98fd73354ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea CB4\n",
    "\n",
    "Ejecute el código de la celda anterior y utilice el prompt del fichero `./provided/prompt-overflow.txt`. ¿Qué ha ocurrido? ¿Qué habría que hacer para evitar esta situación?"
   ],
   "id": "7771f882c702be22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Modificar el tokenizador para que trunque el prompt y conozca el tamaño máximo de prompt, añadir esa información también al modelo cuando genera su output",
   "id": "fce517dae835c715",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
