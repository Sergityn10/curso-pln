{
  "cells": [
    {
      "metadata": {
        "id": "8a77807f92f26ee"
      },
      "cell_type": "markdown",
      "source": [
        "# Text To Speech (TTS)\n"
      ],
      "id": "8a77807f92f26ee"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:52:30.745704Z",
          "start_time": "2025-01-14T22:52:30.600951Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbc121e30a2defb3",
        "outputId": "992c02b9-da5e-4f11-ae84-5f7be6073f7d"
      },
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "token = \"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\"\n",
        "print(\"Hugging Face logging\")\n",
        "login(token)"
      ],
      "id": "fbc121e30a2defb3",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face logging\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:52:30.768341Z",
          "start_time": "2025-01-14T22:52:30.764466Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "118312ae4a6a20dc",
        "outputId": "a39f9c24-450f-4dfe-cc4a-6b9fad07f6c5"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device =  (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")"
      ],
      "id": "118312ae4a6a20dc",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cpu\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AZll5HyvntZ",
        "outputId": "3d37cb82-f883-4412-f189-251bf557628a"
      },
      "id": "9AZll5HyvntZ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "98ddd920230f4f4e"
      },
      "cell_type": "markdown",
      "source": [
        "## Generación de audio con speaker embeddings default"
      ],
      "id": "98ddd920230f4f4e"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:52:32.784899Z",
          "start_time": "2025-01-14T22:52:30.773538Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e527da901507380",
        "outputId": "5e4cfc0d-4311-438a-e829-a2f476b1cf7a"
      },
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Configuración\n",
        "model_name = \"microsoft/speecht5_tts\"  # Modelo SpeechT5 para TTS\n",
        "\n",
        "# Cargar el modelo, el procesador y el vocoder\n",
        "processor = SpeechT5Processor.from_pretrained(model_name)\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(model_name, cache_dir=\"./models/speecht5_tts\")\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\", cache_dir=\"./models/speecht5_hifigan\")\n",
        "\n",
        "def text_to_speech(text, processor, model, vocoder, speaker_embeddings=torch.zeros((1, 512)), output_file=\"./output/output.wav\"):\n",
        "    # Procesar el texto\n",
        "    inputs = processor(text=text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generar el embedding de audio\n",
        "    with torch.no_grad():\n",
        "        speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings=speaker_embeddings)\n",
        "\n",
        "    # Usar el vocoder para generar el audio final\n",
        "    with torch.no_grad():\n",
        "        audio = vocoder(speech)\n",
        "\n",
        "    # Ajustar el tensor para que sea compatible con torchaudio (formato 2D)\n",
        "    audio = audio.squeeze(0).unsqueeze(0)\n",
        "    if output_file is not None:\n",
        "        # Guardar el audio generado\n",
        "        torchaudio.save(output_file, audio, sample_rate=16000)  # 16000 es la tasa de muestreo\n",
        "        print(f\"Audio guardado en {output_file}\")\n",
        "    return audio"
      ],
      "id": "5e527da901507380",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:52:34.905066Z",
          "start_time": "2025-01-14T22:52:32.788704Z"
        },
        "id": "bdee8eeaeea2e6fb"
      },
      "cell_type": "code",
      "source": [
        "# Configuración\n",
        "model_name = \"microsoft/speecht5_tts\"  # Modelo SpeechT5 para TTS\n",
        "\n",
        "# Cargar el modelo, el procesador y el vocoder\n",
        "processor = SpeechT5Processor.from_pretrained(model_name)\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(model_name)\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")"
      ],
      "id": "bdee8eeaeea2e6fb",
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:52:36.120344Z",
          "start_time": "2025-01-14T22:52:34.915362Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8a85a61886e9ff0",
        "outputId": "2e9f5976-ee4d-494a-f22e-41f0c3ff70fc"
      },
      "cell_type": "code",
      "source": [
        "texto = \"this is a generated text\"\n",
        "text_to_speech(text=texto,processor=processor, model=model, vocoder=vocoder, output_file=\"./audio_generado-es100.wav\")"
      ],
      "id": "b8a85a61886e9ff0",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio guardado en ./audio_generado-es100.wav\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0003,  0.0004, -0.0001,  ...,  0.0022,  0.0004, -0.0032]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "60ded034cac8f05d"
      },
      "cell_type": "markdown",
      "source": [
        "## Generación de audio con speaker embeddings personalizados"
      ],
      "id": "60ded034cac8f05d"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:52:43.969532Z",
          "start_time": "2025-01-14T22:52:37.808114Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1c8329bdbf037f8",
        "outputId": "065efeeb-d9b0-450f-e1fa-75a299c2c4f1"
      },
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Cargar embeddings del dataset de voz\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "# Seleccionar un embedding específico (ajusta el índice según el dataset)\n",
        "speaker_embeddings = torch.tensor(embeddings_dataset[500][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "texto = \"this is a generated text\"\n",
        "text_to_speech(text=texto,processor=processor, model=model, vocoder=vocoder, speaker_embeddings=speaker_embeddings, output_file=\"./audio_generado.wav\")"
      ],
      "id": "b1c8329bdbf037f8",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio guardado en ./audio_generado.wav\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0008, -0.0011, -0.0008,  ..., -0.0052, -0.0048, -0.0030]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": 10
    },
    {
      "metadata": {
        "id": "c695ecce4ef791d4"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tarea TTSB1"
      ],
      "id": "c695ecce4ef791d4"
    },
    {
      "metadata": {
        "id": "7c8bfac8616d99d6"
      },
      "cell_type": "markdown",
      "source": [
        "Generar distintos audios usando diferentes embeddings del dataset"
      ],
      "id": "7c8bfac8616d99d6"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:52:56.600229Z",
          "start_time": "2025-01-14T22:52:56.598291Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c56f6aed95138684",
        "outputId": "2dd9d16a-7684-44e2-ba41-29c3c73bda85"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# TODO: cambiar el numero del embedding del dataset para ver distintas generaciones, para ello puede usar random_embedding = random.randint(1, 7500)\n",
        "for iteration in range(0,5):\n",
        "  random_embedding = random.randint(1, 7500)\n",
        "  # Seleccionar un embedding específico (ajusta el índice según el dataset)\n",
        "  speaker_embeddings = torch.tensor(embeddings_dataset[random_embedding][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "  texto = \"this is a generated text\"\n",
        "  text_to_speech(text=texto,processor=processor, model=model, vocoder=vocoder, speaker_embeddings=speaker_embeddings, output_file=\"./audio_generado-es-\"+str(iteration)+\"_\"+str(random_embedding)+\".wav\")"
      ],
      "id": "c56f6aed95138684",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio guardado en ./audio_generado-es-0_3722.wav\n",
            "Audio guardado en ./audio_generado-es-1_6646.wav\n",
            "Audio guardado en ./audio_generado-es-2_5363.wav\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "64b6b728d6f9c839"
      },
      "cell_type": "markdown",
      "source": [
        "### Reproducción del audio en lugar de alamacenarlo"
      ],
      "id": "64b6b728d6f9c839"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:52:58.421717Z",
          "start_time": "2025-01-14T22:52:56.650481Z"
        },
        "id": "2097e609fc7e7fd1"
      },
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "import torch\n",
        "\n",
        "speaker_embeddings = torch.tensor(embeddings_dataset[7930][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "texto = \"this is a generated text\"\n",
        "audio = text_to_speech(text=texto,proceassor=processor, model=model, vocoder=vocoder, speaker_embeddings=speaker_embeddings, output_file=None)\n",
        "audio_np = audio.cpu().numpy().squeeze()\n",
        "ipd.display(ipd.Audio(audio_np, rate=16000))  # 16000 es la tasa de muestreo"
      ],
      "id": "2097e609fc7e7fd1",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f4a9e4182c05ef3a"
      },
      "cell_type": "markdown",
      "source": [
        "## Generación de audio con speaker embeddings personalizados a partir de fichero wav"
      ],
      "id": "f4a9e4182c05ef3a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:53:00.764202Z",
          "start_time": "2025-01-14T22:52:58.428345Z"
        },
        "id": "a78530f8985a0f2b"
      },
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "# Cargar el dataset personalizado\n",
        "dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", cache_dir=\"./data/common_voice\", trust_remote_code=True, streaming=True)\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
      ],
      "id": "a78530f8985a0f2b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:53:00.774172Z",
          "start_time": "2025-01-14T22:53:00.768176Z"
        },
        "id": "b63e2e0ed4505d9a"
      },
      "cell_type": "code",
      "source": [
        "def filter_dataset(batch):\n",
        "    return batch[\"client_id\"] == \"0a041928afd5c85b769b7aaafe7202013aa48b0a995bbb55c8ff5e6c76c9886a5c739f97ee85325f5ffa678576c81f0f9f038dd36428c69726934739e466cad8\"\n",
        "\n",
        "dataset = dataset.filter(filter_dataset)"
      ],
      "id": "b63e2e0ed4505d9a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:53:00.780370Z",
          "start_time": "2025-01-14T22:53:00.777418Z"
        },
        "id": "9bd8be13c1daa838"
      },
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "id": "9bd8be13c1daa838",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:53:02.921927Z",
          "start_time": "2025-01-14T22:53:00.787680Z"
        },
        "id": "47a06749c093445d"
      },
      "cell_type": "code",
      "source": [
        "from speechbrain.pretrained import SpeakerRecognition\n",
        "\n",
        "embedding_model = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", run_opts={\"device\": device})\n",
        "\n",
        "audio_entry = dataset[0][\"audio\"]\n",
        "\n",
        "if \"array\" in audio_entry and \"sampling_rate\" in audio_entry:\n",
        "\n",
        "    waveform = torch.tensor(audio_entry[\"array\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    sampling_rate = audio_entry[\"sampling_rate\"]\n",
        "else:\n",
        "    import torchaudio\n",
        "    audio_path = audio_entry[\"path\"]\n",
        "    waveform, sampling_rate = torchaudio.load(audio_path)\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "# generar speaker embedding\n",
        "with torch.no_grad():\n",
        "    speaker_embeddings = embedding_model.encode_batch(waveform).to(device).squeeze(0)\n",
        "    texto = \"Esto es un texto generado\"\n",
        "    text_to_speech(text=texto,processor=processor, model=model, vocoder=vocoder, speaker_embeddings=speaker_embeddings, output_file=\"./output/audio_generado-mozilla.wav\")"
      ],
      "id": "47a06749c093445d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3ae7c1106769d2db"
      },
      "cell_type": "markdown",
      "source": [
        "#### Incrementando el numero de muestras"
      ],
      "id": "3ae7c1106769d2db"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:53:03.630559Z",
          "start_time": "2025-01-14T22:53:02.930354Z"
        },
        "id": "ea0401458c7430e9"
      },
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", cache_dir=\"./data/common_voice\")\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
      ],
      "id": "ea0401458c7430e9",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:53:03.639718Z",
          "start_time": "2025-01-14T22:53:03.634379Z"
        },
        "id": "1a7aedeff17ac002"
      },
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(lambda example: \"España: Centro-Sur peninsular\" in example[\"accent\"] and \"female\" in example[\"gender\"] )\n"
      ],
      "id": "1a7aedeff17ac002",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:53:03.646943Z",
          "start_time": "2025-01-14T22:53:03.643070Z"
        },
        "id": "39bf47a5cfa80210"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_average_embedding(dataset, embedding_model, device=\"cpu\"):\n",
        "    embeddings = []\n",
        "    for entry in dataset:\n",
        "        audio_entry = entry[\"audio\"]\n",
        "        if \"array\" in audio_entry and \"sampling_rate\" in audio_entry:\n",
        "            waveform = torch.tensor(audio_entry[\"array\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        else:\n",
        "            waveform, _ = torchaudio.load(audio_entry[\"path\"])\n",
        "            waveform = waveform.to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = embedding_model.encode_batch(waveform).squeeze(0).cpu().numpy()\n",
        "            embeddings.append(embedding)\n",
        "    embeddings = np.array(embeddings)  # Convertir a un numpy array para mejor manejo\n",
        "    return torch.tensor(embeddings.mean(axis=0)).unsqueeze(0).to(device)\n"
      ],
      "id": "39bf47a5cfa80210",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-14T22:53:08.785191Z",
          "start_time": "2025-01-14T22:53:03.650299Z"
        },
        "id": "4b4c5c1bf1b55286"
      },
      "cell_type": "code",
      "source": [
        "# Cargar el modelo de reconocimiento de hablantes\n",
        "embedding_model = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", run_opts={\"device\": device})\n",
        "\n",
        "# Generar embedding promedio\n",
        "speaker_embeddings = get_average_embedding(dataset, embedding_model, device=device)\n",
        "# Ajustar dimensiones de speaker_embeddings\n",
        "if speaker_embeddings.dim() > 2:\n",
        "    speaker_embeddings = speaker_embeddings.squeeze()  # Elimina dimensiones extra\n",
        "if speaker_embeddings.dim() == 1:\n",
        "    speaker_embeddings = speaker_embeddings.unsqueeze(0)  # Añade la dimensión de batch\n",
        "\n",
        "# Generar audio\n",
        "texto = \"Esto es un texto generado usando embeddings promedio.\"\n",
        "text_to_speech(text=texto, processor=processor, model=model, vocoder=vocoder, speaker_embeddings=speaker_embeddings, output_file=\"./output/audio_generado_mozilla-promedio.wav\")\n"
      ],
      "id": "4b4c5c1bf1b55286",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2b0517899d69ecb0"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tarea TTSB2"
      ],
      "id": "2b0517899d69ecb0"
    },
    {
      "metadata": {
        "id": "f9b899b314ba542"
      },
      "cell_type": "markdown",
      "source": [
        "Con lo visto en el notebook, ¿Cómo podríamos generar un sistema TTS que sonara como otra persona?"
      ],
      "id": "f9b899b314ba542"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}