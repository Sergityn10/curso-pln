{"cells":[{"cell_type":"markdown","id":"fbc91ce919b79c3","metadata":{"id":"fbc91ce919b79c3"},"source":["## Automatic Text to Speech (ATS)"]},{"cell_type":"code","id":"e80a3016d6e15e39","metadata":{"id":"e80a3016d6e15e39","tags":[],"ExecuteTime":{"end_time":"2025-01-23T13:24:04.584952Z","start_time":"2025-01-23T13:24:03.932011Z"}},"source":["from huggingface_hub import login\n","token = \"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\"\n","print(\"Hugging Face logging\")\n","login(token)"],"outputs":[],"execution_count":null},{"cell_type":"code","id":"96efaa14351d2553","metadata":{"id":"96efaa14351d2553","tags":[],"ExecuteTime":{"end_time":"2025-01-23T13:36:03.284132Z","start_time":"2025-01-23T13:36:03.190221Z"}},"source":["import torch\n","device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Usando dispositivo: {device}\")"],"outputs":[],"execution_count":null},{"cell_type":"code","source":["!pip install datasets evaluate"],"metadata":{"id":"oomZ5FXywF2-"},"id":"oomZ5FXywF2-","execution_count":null,"outputs":[]},{"metadata":{"id":"1d1412cd581cbfca"},"cell_type":"markdown","source":["Antes de comenzar con los modelos, vamos a familiarizarnos con los datasets para ASR. Para ello, acceda al dataset `hf-internal-testing/librispeech_asr_dummy` y estudie como está estructurado. A continuación vamos a descargarlo:"],"id":"1d1412cd581cbfca"},{"cell_type":"code","id":"egafcaMc-SFs","metadata":{"id":"egafcaMc-SFs","tags":[],"ExecuteTime":{"end_time":"2025-01-23T13:24:25.652511Z","start_time":"2025-01-23T13:24:06.630920Z"}},"source":["from datasets import load_dataset\n","\n","# Cargando dataset `hf-internal-testing/librispeech_asr_dummy`\n","ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:10]\")\n","sample = ds[0][\"audio\"]"],"outputs":[],"execution_count":null},{"cell_type":"markdown","id":"cf1f7a5ba64bfa50","metadata":{"id":"cf1f7a5ba64bfa50"},"source":["## Transcripción de audio"]},{"metadata":{"id":"8e535da1f269aae1"},"cell_type":"markdown","source":["Para realizar la transcripción de audio, usaremos un modelo muy conocido llamado whisper, en particular, su version minúscula (`openai/whisper-tiny`)"],"id":"8e535da1f269aae1"},{"cell_type":"code","id":"f26777b9da2635b5","metadata":{"id":"f26777b9da2635b5","tags":[],"ExecuteTime":{"end_time":"2025-01-23T13:25:25.972724Z","start_time":"2025-01-23T13:24:25.661005Z"}},"source":["from transformers import WhisperProcessor, WhisperForConditionalGeneration\n","\n","# Creación del modelo y processor\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n","model.config.forced_decoder_ids = None\n","\n","# Procesado del dataset\n","input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n","# Generación de la transcripción\n","predicted_ids = model.generate(input_features)\n","# Decodificación de la transcripción mostrando tokens especiales\n","transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n","print(transcription)\n","# Decodificación de la transcripción sin mostrar tokens especiales\n","transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n","print(transcription)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","id":"5c53ff3eb2619c00","metadata":{"id":"5c53ff3eb2619c00"},"source":["## Transcipción indicando idiomas"]},{"metadata":{"id":"a1b4179a09c85dd0"},"cell_type":"markdown","source":["Para afinar la eficacia de whisper, se puede indicar el idioma que se escucha en el audio. Para ello, vamos a usar un audio del dataset `mozilla-foundation/common_voice_13_0`"],"id":"a1b4179a09c85dd0"},{"metadata":{"id":"QdTPNyFH_H3E"},"cell_type":"markdown","source":["####  Tarea ASRB1\n","\n"],"id":"QdTPNyFH_H3E"},{"metadata":{"id":"964f67676627aa2d"},"cell_type":"markdown","source":["Acceder al dataset `mozilla-foundation/common_voice_13_0` y encontrar el audio que se está obteniendo en la siguiente celda (cuyo `path` es `es_test_0/common_voice_es_19698530.mp3`. Escuchar el audio para compararlo con la transcripción."],"id":"964f67676627aa2d"},{"metadata":{"id":"615jtLOk_HY1","tags":[],"ExecuteTime":{"end_time":"2025-01-23T13:25:35.561320Z","start_time":"2025-01-23T13:25:26.361114Z"}},"cell_type":"code","outputs":[],"execution_count":null,"source":["from datasets import Audio, load_dataset\n","\n","# Cargando dataset `hf-internal-testing/librispeech_asr_dummy`\n","ds = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"es\", split=\"test\", streaming=True)\n","# Casteando la frecuencia y seleccionando un audio en particular\n","ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000)) # 16_000\n","ds = ds.filter(lambda sample: \"es_test_0/common_voice_es_19698530.mp3\" in sample[\"path\"])\n","sample = next(iter(ds))[\"audio\"]\n","print(sample['path'])\n"],"id":"615jtLOk_HY1"},{"metadata":{"id":"4185a785-d628-4149-8bd3-de474cd11909"},"cell_type":"markdown","source":["#### Tarea ASRB2\n","\n","Usando el código desarrollado anteriormente para realizar las transcripciones con whisper, haga una transcripción de la variable sample. Una vez conseguida, utilice el siguiente código para indicar el idioma.\n","\n","```\n","forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n","```"],"id":"4185a785-d628-4149-8bd3-de474cd11909"},{"metadata":{"id":"Sdo4GO4W_CIM","tags":[],"ExecuteTime":{"end_time":"2025-01-23T13:34:45.912651Z","start_time":"2025-01-23T13:34:42.234915Z"}},"cell_type":"code","outputs":[],"execution_count":null,"source":["from transformers import WhisperProcessor, WhisperForConditionalGeneration\n","\n","# TODO: Creación del modelo y processor\n","\n","# DONE: En este caso indicamos el idioma del audio añadiendo el parámetro language\n","forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n","\n","# TODO: Procesado del dataset\n","\n","# TODO: Generación de la transcripción\n","\n","# TODO: Decodificación de la transcripción mostrando tokens especiales\n","\n","# TODO: Decodificación de la transcripción sin mostrar tokens especiales\n"],"id":"Sdo4GO4W_CIM"},{"cell_type":"markdown","id":"ECMkHhm7CHRf","metadata":{"id":"ECMkHhm7CHRf"},"source":["## Transcripción con traducción"]},{"cell_type":"markdown","id":"wMPcq3aACpmi","metadata":{"id":"wMPcq3aACpmi"},"source":["Además de indicar el lenguaje del audio, los modelo whisper incorporan por defecto la tarea (`task`) de traducción de múltiples idiomas al inglés. Para activarla hay que cambiar el valor del parametro `task` de `transcribe` a `translate`"]},{"metadata":{"id":"2065836c7870f629"},"cell_type":"markdown","source":["#### Tarea ASRB3\n","\n","En el siguiente fragmento, cambie el valor del parámetro `task` para que traduzca en lugar de simplemente transcribir"],"id":"2065836c7870f629"},{"cell_type":"code","id":"QCqq7ORrCG4h","metadata":{"id":"QCqq7ORrCG4h","tags":[],"ExecuteTime":{"end_time":"2025-01-23T13:35:08.701380Z","start_time":"2025-01-23T13:35:06.183601Z"}},"source":["from transformers import WhisperProcessor, WhisperForConditionalGeneration\n","\n","# Creación del modelo y processor\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n","# TODO: cambiar el valor del parámetro transcribe a translate\n","forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n","\n","\n","\n","# Procesado del dataset\n","input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n","# Generación de la transcripción\n","predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n","# Decodificación de la transcripción mostrando tokens especiales\n","transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n","print(transcription)\n","# Decodificación de la transcripción sin mostrar tokens especiales\n","transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n","print(transcription)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","id":"c08wi-6ZC6b5","metadata":{"id":"c08wi-6ZC6b5"},"source":["## Evaluación de ASR"]},{"metadata":{"id":"4e0494eafb3c8965"},"cell_type":"markdown","source":["Finalmente, podemos evaluar distintos modelos ASR, para ello, utilizamos un dataset. Para realizar la evaluación, vamos a usar las primeras 10 muestras."],"id":"4e0494eafb3c8965"},{"cell_type":"code","id":"_QgHVUv7Drct","metadata":{"id":"_QgHVUv7Drct","tags":[],"ExecuteTime":{"end_time":"2025-01-23T14:15:20.955434Z","start_time":"2025-01-23T14:15:18.909147Z"}},"source":["from datasets import load_dataset, Audio\n","\n","# Cargar el dataset\n","data = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", cache_dir=\"./data/common_voice_11_0_test\", trust_remote_code=True, streaming=True)\n","data = data.cast_column(\"audio\", Audio(sampling_rate=16000))\n","print(\"Dataset cargado correctamente\")\n","\n","# Preprocesamiento: Normalización del texto\n","def normalize_text(batch):\n","    text = batch[\"sentence\"].lower().strip()\n","    batch[\"sentence\"] = text\n","    return batch\n","\n","data = data.map(normalize_text)\n","print(\"Texto normalizado\")\n","\n","# DONE: Nos quedamos solo con 10 muestras del dataset\n","data = data.take(10)\n","\n","print(data)\n","dataset=[]\n","for sample in data:\n","  dataset.append(sample)\n","\n","print(\"Dataset final size: \",len(dataset))\n"],"outputs":[],"execution_count":null},{"metadata":{"id":"2adb25b742632751"},"cell_type":"markdown","source":["Además del dataset, empaquetamos los modelos en funciones para poder invocarlas durante la evaluación de manera sencilla."],"id":"2adb25b742632751"},{"metadata":{"ExecuteTime":{"end_time":"2025-01-23T14:14:38.163588Z","start_time":"2025-01-23T14:14:36.305093Z"},"id":"529ab9f9f05a6437"},"cell_type":"code","source":["processor_whisper = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n","model_wisper = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n","def asr_whisper(sample):\n","    global processor_whisper, model_wisper\n","    forced_decoder_ids = processor_whisper.get_decoder_prompt_ids(language=\"spanish\", task=\"translate\")\n","    input_features = processor_whisper(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n","    predicted_ids = model_wisper.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n","    return processor_whisper.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n"],"id":"529ab9f9f05a6437","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2025-01-23T13:58:31.608187Z","start_time":"2025-01-23T13:58:29.407173Z"},"id":"eb307dfe9252b196"},"cell_type":"code","source":["from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n","\n","processor_wav2vec = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-spanish\")\n","model_wav2vec = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-spanish\")\n","\n","def asr_wav(sample):\n","    global processor_wav2vec, model_wav2vec\n","    features = processor_wav2vec(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], padding=True, return_tensors=\"pt\")\n","    input_values = features.input_values\n","    attention_mask = features.attention_mask\n","    with torch.no_grad():\n","        logits = model_wav2vec(input_values, attention_mask=attention_mask).logits\n","    pred_ids = torch.argmax(logits, dim=-1)\n","    return processor_wav2vec.batch_decode(pred_ids)[0]\n","\n"],"id":"eb307dfe9252b196","outputs":[],"execution_count":null},{"cell_type":"markdown","id":"c2658eccfed555cc","metadata":{"id":"c2658eccfed555cc"},"source":["Para evaluar los modelos, vamos a utilizar la metrica WER. El **Word Error Rate (WER)** es una métrica utilizada para evaluar la precisión de los sistemas de reconocimiento automático del habla (ASR). Mide la proporción de errores cometidos en las transcripciones generadas por el modelo, comparándolas con una referencia. Estos errores se calculan como la suma de sustituciones, inserciones y eliminaciones necesarias para alinear la transcripción con la referencia, y se normalizan dividiendo por el número total de palabras en la referencia. **Un WER más bajo indica un mejor rendimiento del sistema.**\n","\n"]},{"cell_type":"code","source":["!pip install evaluate jiwer"],"metadata":{"id":"aYnebhQYxWaS"},"id":"aYnebhQYxWaS","execution_count":null,"outputs":[]},{"cell_type":"code","id":"NbdbpIHHD6W5","metadata":{"id":"NbdbpIHHD6W5","ExecuteTime":{"end_time":"2025-01-23T14:16:53.497622Z","start_time":"2025-01-23T14:15:26.418264Z"}},"source":["import torch\n","from evaluate import load\n","import matplotlib.pyplot as plt\n","\n","\n","# Métrica WER (Word Error Rate)\n","wer_metric = load(\"wer\")\n","\n","# Creamos los arrays que contendrán las predicciones de los modelos y las referencias (gold std.)\n","predictions_whisper = []\n","predictions_wav2vec = []\n","references = []\n","\n","# Recorremos el dataset y generamos las predicciones\n","for sample in data:\n","    references.append(sample['sentence'])\n","\n","    whisper_transcription = asr_whisper(sample['audio'])\n","    predictions_whisper.append(whisper_transcription)\n","\n","    wav_transcription = asr_wav(sample['audio'])\n","    predictions_wav2vec.append(wav_transcription)\n","\n","\n","# Evaluar Whisper\n","print(\"Evaluando Whisper...\")\n","whisper_wer = wer_metric.compute(predictions=predictions_whisper, references=references)\n","print(f\"WER de Whisper: {whisper_wer:.4f}\")\n","\n","# Evaluar Wav2Vec2\n","print(\"Evaluando Wav2Vec2...\")\n","wav2vec_wer = wer_metric.compute(predictions=predictions_wav2vec, references=references)\n","print(f\"WER de Wav2Vec2: {wav2vec_wer:.4f}\")\n","\n","# Crear gráfica de comparación\n","models = [\"Whisper\", \"Wav2Vec2\"]\n","wer_scores = [whisper_wer, wav2vec_wer]\n","\n","plt.figure(figsize=(8, 6))\n","plt.bar(models, wer_scores)\n","plt.title(\"Comparación de WER entre modelos ASR\")\n","plt.ylabel(\"WER (Word Error Rate)\")\n","plt.xlabel(\"Modelo\")\n","plt.ylim(0, max(wer_scores) + 0.1)\n","plt.show()\n"],"outputs":[],"execution_count":null},{"cell_type":"markdown","id":"b1ea02dc-8654-4a06-b1d5-ffeafb164390","metadata":{"id":"b1ea02dc-8654-4a06-b1d5-ffeafb164390"},"source":["#### Tarea ASRB4\n","\n","Cambiar el código anterior para evaluar los modelos con las primeras 30 muestras del dataset de test. ¿Qué ocurre con los resultados?\n"]},{"metadata":{"id":"7fabb8bdd536fabf"},"cell_type":"markdown","source":["#### Tarea ASRB5\n","\n","Añadir a la comparación de técnicas el modelo `openai/whisper-small` por `openai/whisper-tiny`. ¿Qué ocurre con los resultados?"],"id":"7fabb8bdd536fabf"},{"metadata":{"ExecuteTime":{"end_time":"2025-01-23T14:22:50.634651Z","start_time":"2025-01-23T14:21:00.775627Z"},"id":"9f414df1af4c0a30"},"cell_type":"code","source":["# TODO: Tarea ASRB5"],"id":"9f414df1af4c0a30","outputs":[],"execution_count":null},{"cell_type":"markdown","id":"349a50f561def233","metadata":{"id":"349a50f561def233"},"source":["## Transcipción de ficheros wav"]},{"cell_type":"code","id":"c2f1c972-25d5-4e94-b17b-85368c5a0840","metadata":{"id":"c2f1c972-25d5-4e94-b17b-85368c5a0840","ExecuteTime":{"end_time":"2025-01-23T14:27:46.409382Z","start_time":"2025-01-23T14:27:30.090707Z"}},"source":["from transformers import WhisperProcessor, WhisperForConditionalGeneration\n","import torchaudio\n","\n","# Cargar el modelo y el procesador\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n","model.config.forced_decoder_ids = None\n","\n","# Función para leer un archivo WAV y convertirlo a input_features\n","def process_audio_file(audio_path, processor):\n","    # Cargar el archivo WAV\n","    waveform, sampling_rate = torchaudio.load(audio_path)\n","\n","    # Resamplear a 16 kHz si es necesario\n","    target_sampling_rate = processor.feature_extractor.sampling_rate\n","    if sampling_rate != target_sampling_rate:\n","        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=target_sampling_rate)\n","        waveform = resampler(waveform)\n","\n","    # Convertir a características de entrada para Whisper\n","    input_features = processor(waveform.squeeze().numpy(), sampling_rate=target_sampling_rate, return_tensors=\"pt\").input_features\n","    return input_features\n","\n","# Transcripción de un archivo de audio\n","def transcribe_audio_file(audio_path, model, processor):\n","    # Procesar el archivo de audio\n","    input_features = process_audio_file(audio_path, processor)\n","\n","    # Generar IDs de tokens\n","    forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n","    predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n","\n","    # Decodificar los tokens generados a texto\n","    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n","    return transcription\n","\n","# Ejemplo de uso\n","audio_path = \"./provided/patria_invento.wav\"\n","transcription = transcribe_audio_file(audio_path, model, processor)\n","print(f\"Transcripción: {transcription}\")"],"outputs":[],"execution_count":null},{"cell_type":"markdown","id":"86b9e1a32b504641","metadata":{"id":"86b9e1a32b504641"},"source":["#### Tarea ASRB6"]},{"cell_type":"markdown","id":"5d2c47cd5dc1ce6f","metadata":{"id":"5d2c47cd5dc1ce6f"},"source":["Convierta a texto el audio de los ficheros `padrino.wav` y `patria_invento.wav`. Para ello, descarge los ficheros del [repositorio de github](https://github.com/cbadenes/curso-pln/tree/main/aplicaciones/automatic_speech_recognition/provided), despúes, carge los ficheros de su ordenador a una carpeta en google collab que se llame `./provided`/.\n","\n","Cuando tenga la transcripción, escuche los audios y valores cuantitativamente como de bien funciona el modelo. Para transcribir el audio utilice la función `transcribe_audio_file_chunked`."]},{"cell_type":"code","id":"91b5821fa4443738","metadata":{"id":"91b5821fa4443738","ExecuteTime":{"end_time":"2025-01-23T14:28:19.105842Z","start_time":"2025-01-23T14:28:09.055021Z"}},"source":["# TODO: transcribir el audio de \"./provided/padrino.wav\"\n"],"outputs":[],"execution_count":null},{"cell_type":"code","id":"fc4ca6ec5c2f447d","metadata":{"id":"fc4ca6ec5c2f447d","ExecuteTime":{"end_time":"2025-01-23T14:28:33.809382Z","start_time":"2025-01-23T14:28:24.008873Z"}},"source":["# TODO: transcribir el audio de \"./provided/patria_invento.wav\"\n"],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Tarea Bonus:\n","\n","\n","Repetir la experimentación con los modelos `whisper` y `wavtovec` utlizando el dataset `mozilla-foundation/common_voice_13_0`. Como el dataset es muy grande, filtrarlo a las 30 primeras entradas igual que se ha hecho en el experimento anterior. Es importante usar el metodo `take` y copiar los 30 primeros elementos a un array y tener el flag de streaming a `True`."],"metadata":{"id":"S_SXeEipuoXh"},"id":"S_SXeEipuoXh"}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0rc1"}},"nbformat":4,"nbformat_minor":5}