{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Question Answering using RAG",
   "id": "ef6da08a78fc0d60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una vez vistas las distintas técnicas de recuperación de información y como evaluar su efectividad en el notebook `rag.ipynb` en este notebook vamos a introducir un modelo de lenguaje para responder las preguntas.",
   "id": "41d220bd743c9fc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "token = \"\"\n",
    "print(\"Hugging Face logging\")\n",
    "login(token)"
   ],
   "id": "ddb614120c6443d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device_setup= \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando: \", device_setup)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "id": "1898ac31988b84d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vamos a implementar un sistema de question answering que utilice los distintos retrievers del notebook `rag.ipynb`. Para simplificar la reutilización del código, en la carpeta `./lib/rag-lib.py` hay un fichero python que implementa todos los retrievers de dicho notebook. La clase `RetrieverFactory` posee el método estático `RetrieversFactory.get_retrievers()` que devuelve un array con las instancias de todos los retrievers vistos.",
   "id": "1c078a951da61353"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from libs.rag_lib import RetrieversFactory, LLMModel, format_dataset, evaluate_qa_system, plot_evaluation_results\n",
    "\n",
    "retrievers = RetrieversFactory.get_retrievers()"
   ],
   "id": "b4813e4944d9318a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Q&A System",
   "id": "d2f5bc8717377231"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class QuestionAnswering:\n",
    "\n",
    "    def __init__(self, generator, rag):\n",
    "        self.generator = generator\n",
    "        self.rag = rag\n",
    "\n",
    "    def answer(self, query, lang=\"spanish\", do_sample=True, show_prompt=False, temperature=0.1):\n",
    "        relevant_documents = self.rag.search_documents(query, lang=lang)\n",
    "        answer = self.generator.answer(query, relevant_documents, show_prompt=show_prompt, use_context=True, do_sample=do_sample, temperature=temperature)\n",
    "        return answer"
   ],
   "id": "e0da0afaeac4bd00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pruebas manuales",
   "id": "8bcd1c7ddac5bb82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vamos a usar el conjunto de datos vistos en notebooks anteriores y darles formato para poder evaluar un sistema de pregunta respuesta.",
   "id": "c526293c2ec4b9aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ragbench = load_dataset(\"rungalileo/ragbench\", \"hotpotqa\", split=[\"test\"])\n",
    "\n",
    "# Le da formato al dataset de dic con las claves documents, queries, gold-std (respuestas)\n",
    "dataset = format_dataset(ragbench[0])"
   ],
   "id": "9bbc58cd6e69ad87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuación, vamos a usar la clase `LLModel` para crear el componente para generar las respuestas usando el contexto. Usando esa clase y un retriever vamos a construir un objeto `QueryAnswering` y comprobar si para una query del dataset responde correctamente de manera qualitativa",
   "id": "62bb3648e013fa98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from libs.rag_lib import LLMModel\n",
    "\n",
    "generator = LLMModel()\n",
    "retriever = retrievers[0]\n",
    "print(\"Usando retriever: \",type(retriever))\n",
    "retriever.build_index(dataset['documents']) # construimos el índice para los documentos"
   ],
   "id": "ef486e8c8435153a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Construcción del sistema Q&A\n",
    "qa_model = QuestionAnswering(generator, retriever)\n",
    "\n",
    "# Extraemos del dataset la query identificada como `5ae151985542990adbacf74d` y su respuesta\n",
    "questions_dict = dataset['queries']\n",
    "question = questions_dict['5ae151985542990adbacf74d']\n",
    "expected_answer = dataset['responses']['5ae151985542990adbacf74d']\n",
    "\n",
    "# Generamos una respuesta con el sistema\n",
    "answer = qa_model.answer(question, lang=\"english\", show_prompt=False)\n",
    "# TODO: ¿Como podemos saber si se está teniendo en cuenta el contexto?\n",
    "# TODO: Probar a variar la temperatura y ver las respuestas que se obtienen\n",
    "\n",
    "# Visualizamos\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer obtained: \", answer)\n",
    "print(\"Answer expected: \", expected_answer)"
   ],
   "id": "1b89872d3c5183c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea QAR1\n",
    "\n",
    "Usando el código anterior, probar el sistema de question answering con la query y respuesta cuyo identificador es `5ab3d69255429969a97a81c9`"
   ],
   "id": "308b84cad8083787"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Extraemos del dataset la query identificada como `5ab3d69255429969a97a81c9` y su respuesta\n",
    "\n",
    "\n",
    "# TODO: Generamos una respuesta con el sistema\n",
    "\n",
    "# TODO: Probar a variar la temperatura y ver las respuestas que se obtienen\n",
    "\n",
    "# TODO: Visualizamos\n"
   ],
   "id": "9e9251c1b5f40111",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea QAR2\n",
    "\n",
    "Usando el código anterior, probar el sistema de question answering con el dataset `rungalileo/ragbench`, en particular, el subconjunto `techqa` y la partición de test"
   ],
   "id": "e7fed1e58d31f67a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: load \"rungalileo/ragbench\", subset \"techqa\", partition \"test\"\n",
    "\n",
    "# TODO: use the function format_dataset to format the data\n"
   ],
   "id": "a79c2dd45bf302e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Extraemos del dataset la query identificada como `techqa_DEV_Q243` y su respuesta\n",
    "\n",
    "# TODO: Generamos una respuesta con el sistema\n",
    "\n",
    "# TODO: Visualizamos"
   ],
   "id": "e644cae07e453003",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluando el sistema RAG vs el baseline",
   "id": "2743408ee89b4803"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Como hemos podido ver en las tareas anteriores, comprar las salidas esperadas con las obtenidas no es trivial. En la siguiente celda vamos a ver un conjunto de métricas para comparar dichos pares.",
   "id": "4cc95f83109e8e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def evaluate_qa_system_adv(models, dataset):\n",
    "    results_by_model = {}\n",
    "\n",
    "    # Métricas y helpers\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    for model_name, model in models:\n",
    "        metrics = {\n",
    "            \"bleu\": [],\n",
    "            \"rouge\": {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []},\n",
    "            \"meteor\": [],\n",
    "            \"exact_match\": [],\n",
    "            \"f1\": [],\n",
    "            \"cosine_similarity\": []\n",
    "        }\n",
    "\n",
    "        for entry in dataset:\n",
    "            query = entry[\"question\"]\n",
    "            ground_truth = entry[\"answer\"]\n",
    "\n",
    "            # Respuesta del modelo\n",
    "            prediction = model.answer(query=query)\n",
    "\n",
    "            # BLEU\n",
    "            smoothing_function = SmoothingFunction()\n",
    "            bleu_score = sentence_bleu([ground_truth.split()], prediction.split(), smoothing_function=smoothing_function.method1)\n",
    "            metrics[\"bleu\"].append(bleu_score)\n",
    "\n",
    "            # ROUGE\n",
    "            rouge_scores = rouge_scorer_obj.score(ground_truth, prediction)\n",
    "            for rouge_metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "                metrics[\"rouge\"][rouge_metric].append(rouge_scores[rouge_metric].fmeasure)\n",
    "\n",
    "            # METEOR\n",
    "            meteor = meteor_score([ground_truth.split()], prediction.split())\n",
    "            metrics[\"meteor\"].append(meteor)\n",
    "\n",
    "            # Exact Match\n",
    "            exact_match = int(prediction.strip() == ground_truth.strip())\n",
    "            metrics[\"exact_match\"].append(exact_match)\n",
    "\n",
    "            # F1 Score\n",
    "            y_true = set(ground_truth.split())\n",
    "            y_pred = set(prediction.split())\n",
    "            common = y_true & y_pred\n",
    "            precision = len(common) / len(y_pred) if y_pred else 0\n",
    "            recall = len(common) / len(y_true) if y_true else 0\n",
    "            f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "            metrics[\"f1\"].append(f1)\n",
    "\n",
    "            # Cosine Similarity\n",
    "            tfidf_matrix = vectorizer.fit_transform([ground_truth, prediction])\n",
    "            cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            metrics[\"cosine_similarity\"].append(cosine_sim)\n",
    "\n",
    "        # Promediar resultados\n",
    "        avg_metrics = {\n",
    "            \"bleu\": np.mean(metrics[\"bleu\"]),\n",
    "            \"rouge\": {k: np.mean(v) for k, v in metrics[\"rouge\"].items()},\n",
    "            \"meteor\": np.mean(metrics[\"meteor\"]),\n",
    "            \"exact_match\": np.mean(metrics[\"exact_match\"]),\n",
    "            \"f1\": np.mean(metrics[\"f1\"]),\n",
    "            \"cosine_similarity\": np.mean(metrics[\"cosine_similarity\"])\n",
    "        }\n",
    "        results_by_model[model_name] = avg_metrics\n",
    "\n",
    "    return results_by_model"
   ],
   "id": "119b9e9cd8ac4721",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Usando las funciones anteriores, vamos a evaluar una pequeña parte del dataset. Para ello, primero lo filtramos",
   "id": "9c9a54a04b1517f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ragbench= load_dataset(\"rungalileo/ragbench\", \"covidqa\", split=[\"test\"])\n",
    "\n",
    "dataset = []\n",
    "index = 0\n",
    "for ds_row in ragbench[0]:\n",
    "    dataset.append({\"question\" : ds_row[\"question\"], \"documents\" : ds_row[\"documents\"], \"answer\" : ds_row[\"response\"] })\n",
    "    index += 1\n",
    "    if index == 5:\n",
    "        break"
   ],
   "id": "5e542252792effb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluación de LLM con el contexto (gold-std) y sin contexto\n",
   "id": "98c89244f77017b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Construimos un modelo LLM y lo evaluamos usando el contexto del propio dataset (el perfecto) y sin contexto. De esta manera vemos los resultados que se obtendrían sin introducir los posibles fallos de los retrievers",
   "id": "db4a8acb6371dab5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm_model = LLMModel()\n",
    "models = [\n",
    "    (\"TinyLlama-1-NC\", llm_model, False),\n",
    "    (\"TinyLlama-2-C\", llm_model, True)\n",
    "]\n",
    "# Esta fución de evaluación recupera del dataset el contexto relevante para una pregunta y, si está configurado, invoca al LLM proporcionándole dicho contexto\n",
    "results_by_model = evaluate_qa_system(models, dataset)"
   ],
   "id": "a8459c1f41578aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluación de Q&A con un retriever\n",
   "id": "3826954411df466f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A continuación, vamos a ver que impacto llegan a tener los retrievers en la efectividad. Los resultados obtenidos deberían ser mejores que los conseguidos con `TinyLlama-1-NC` y cuanto más cercanos a `TinyLlama-1-C`.\n",
    "\n",
    "Los distintos retrievers de los que disponemos son: `SparseRetriever()`, `SparseRetrieverNM()`, `DenseRetriever()`, `DenseRetriever( model='distiluse-base-multilingual-cased-v1')`, `HybridRetriever()`, `HybridRetriever(model='distiluse-base-multilingual-cased-v1')`"
   ],
   "id": "f0a0885f1e1424ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from libs.rag_lib import SparseRetriever, SparseRetrieverNM, DenseRetriever, DenseRetriever, HybridRetriever, HybridRetriever\n",
    "\n",
    "# Cargamos todos los documentos del dataset para los retrievers\n",
    "dataset_formattted = format_dataset(ragbench[0])\n",
    "documents = dataset_formattted['documents']\n",
    "# Construimos los retrievers\n",
    "retriever = SparseRetriever()\n",
    "retriever.build_index(documents)\n",
    "# TODO: probar con los distintos retrievers\n",
    "# Construimos un sistema de Q&A\n",
    "models = [ (\"Q&A-RAG-Sparse\", QuestionAnswering(llm_model, retriever)) ]\n",
    "\n",
    "# Esta función de evaluación nunca proporciona el contexto a los modelos\n",
    "results_by_model_qa = evaluate_qa_system_adv(models, dataset)"
   ],
   "id": "df2c4083236a6a4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finalmente, solo queda visualizar los distintos resultados obtenidos\n",
   "id": "63fe391e982511b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Combinamos los resultados uniéndolos\n",
    "results_by_model = results_by_model | results_by_model_qa\n",
    "\n",
    "# Mostramos los resultados\n",
    "for model_name, metrics in results_by_model.items():\n",
    "    print(\"Resultados para \",model_name)\n",
    "    for metric, value in metrics.items():\n",
    "        print(metric,\": \",value)\n",
    "\n",
    "# Representamos los resultados en gráficas\n",
    "plot_evaluation_results(results_by_model)"
   ],
   "id": "411e118eebd491b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluando con rag-mini-wiki\n",
    "\n",
    "#### Tarea QA3\n",
    "\n",
    "Utilizando el código anterior y el dataset 'rag-datasets/rag-mini-wikipedia' realizar una evaluación"
   ],
   "id": "6187ec92cf42f1e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: descargar el dataset y preparar en una variable `dataset` las preguntas, las respuestas y los documentos, limitarlo a 10/5 preguntas en principio\n",
    "\n",
    "# TODO: Preparar los documentos de contexto  para los retriever\n"
   ],
   "id": "1570d49db2764254"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO: crear y evaluar un modelo LLM que no use contexto",
   "id": "c0ee20f42287bd69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Crear una instancia de question answering\n",
    "\n",
    "# TODO: Evaluar el modelo de question answering"
   ],
   "id": "3d8e63a075341140"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  TODO: combinar los resultados con los anteriores\n",
    "\n",
    "# TODO: Mostrar resultados por pantalla e imprimir las gráficas\n"
   ],
   "id": "d4d2265787a976a6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
