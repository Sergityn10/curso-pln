{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sistemas RAG",
   "id": "74e3edd345163c62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Los sistemas RAG permiten mejorar la eficacia de los sistemas de preguntas respuestas (query answering). Por un lado reducen las alucinaciones que los modelos puedan tener y, por otro lado, permiten dar una evidencia empírica de la proveniencia de la información sobre la que se basa la respuesta. Para ello, los sistemas RAG necesitan una base documental sobre la que articular la respuesta. Cuando hablamos de documentos, no nos referimos a ficheros en sí, sino a conjuntos de información como pueden ser párrafos, frases, páginas de texto, etc. dependiendo de la granularidad que se le quiera dar a dicha unidad. El flujo del modelo RAG, dada una consulta y una base documental, sería:\n",
    "\n",
    "* Retrieval: buscar fragmentos relevantes en la base documental que tengan relación con la pregunta.\n",
    "* Augmentation: pasar los fragmentos recuperados como contexto adicional al modelo generador.\n",
    "* Generation: el modelo genera la respuesta teniendo en cuenta el contexto documental\n",
    "\n",
    "A continuación vamos a ver varias opciones para la parte de Retrieval (Recuperación), como comparar dichos sistemas, y la mejora que incorporan al módulo de generación.\n",
    "\n",
    "Para ello, vamos a reutilizar parte del código visto en el notebook de [Búsqueda Dispersa y Densa](https://github.com/cbadenes/curso-pln/blob/main/notebooks/08_Busqueda_Dispersa_y_Densa.ipynb) y en el de [RAG Avanzado](https://github.com/cbadenes/curso-pln/blob/main/notebooks/08_RAG_Avanzado.ipynb)."
   ],
   "id": "75fffec23aa77053"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "token = \"\"\n",
    "print(\"Hugging Face logging\")\n",
    "login(token)"
   ],
   "id": "2701253e0f710414",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "device_setup= \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using: \",device_setup)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "id": "5bfe622de738220c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG: Retrieval-Augmented Generation",
   "id": "dc93e1a0641bb997"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from typing import List, Tuple\n",
    "import nltk\n",
    "\n",
    "# Descarga de recursos necesarios\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "class TextPreprocessor:\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, text: str, lang='spanish') -> str:\n",
    "        text = text.lower()\n",
    "        # Tokenizar usando NLTK\n",
    "        tokens = word_tokenize(text)\n",
    "        # Eliminar stopwords usando NLTK\n",
    "        stop_words = set(stopwords.words(lang))\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "        return ' '.join(tokens)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Como vamos a trabajar con distintos tipos de retrievers, para que todos se puedan utilizar de la misma manera (métodos con el mismo nombre), vamos a crear una estructura de herencia dónde todos las clases de los retriever tendrán que implementar los métodos de la siguiente clase abstracta/interfaz.",
   "id": "8ba53edc6ec5f2fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Retriever(ABC):\n",
    "\n",
    "    def __init__(self, name='abstract_retriever'):\n",
    "        self.name = name\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    \"\"\"\n",
    "    Este método recibe un conjunto de documentos y los indexa para poder realizar búsquedas posteriores\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def build_index(self, documents: List[str], lang: str = 'spanish'):\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "        Este método búsca los documentos relevantes para una query.\n",
    "        Devuelve una lista con el la posición (index) del documento encontrado y su score de relevancia.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def search(self, query: str, top_k: int = 3, lang:str = 'spanish') -> List[Tuple[int, float]]:\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "        Este método búsca los documentos relevantes para una query.\n",
    "        Devuelve los documentos que considera relevantes.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def search_documents(self, query: str, top_k: int = 3, lang:str = 'spanish') -> List[str]:\n",
    "        pass"
   ],
   "id": "afb884c7c7a806c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SparseRetriever",
   "id": "76b9178e3605c6c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuación podemos ver un ejemplo de sparse retriever basado en TF-IDF para vectorizar los textos y el uso del modelo NearestNeighbors con la distancia coseno para recuperar los documentos relevantes. Es importante también observar como se ha indicado la herencia de la clase Retriever.",
   "id": "8394b0b0694a3049"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "'''\n",
    "    * Búsqueda eficiente: El uso de NearestNeighbors con una métrica de similitud como el coseno permite realizar búsquedas rápidas.\n",
    "    * TF-IDF como base: Las palabras más relevantes en cada documento obtienen un peso mayor, mejorando la precisión de la búsqueda.\n",
    "'''\n",
    "class SparseRetrieverNM(Retriever):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"sparse_retriever_nm\")\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.nn_model = NearestNeighbors(n_neighbors=5, metric=\"cosine\", algorithm=\"auto\")\n",
    "\n",
    "    \"\"\"\n",
    "    Construye el índice usando TF-IDF\n",
    "    \"\"\"\n",
    "    def build_index(self, documents: List[str], lang: str = 'spanish'):\n",
    "        self.documents = documents\n",
    "        # Limpiar tokens innecesarios\n",
    "        processed_docs = [TextPreprocessor.preprocess(doc, lang) for doc in self.documents]\n",
    "        # Generar embeddings dispersos TF-IDF\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(processed_docs)\n",
    "        # Construir un modelo de búsqueda eficiente\n",
    "        self.nn_model.fit(self.tfidf_matrix)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5, lang: str = 'spanish') -> List[Tuple[int, float]]:\n",
    "        # Vectorizar la consulta\n",
    "        processed_query = TextPreprocessor.preprocess(query, lang)\n",
    "        query_vector = self.vectorizer.transform([processed_query])\n",
    "\n",
    "        # Encontrar los vecinos más cercanos\n",
    "        distances, indices = self.nn_model.kneighbors(query_vector, n_neighbors=top_k)\n",
    "        # Retornar resultados como documentos y distancias inversas (para similitud)\n",
    "        return [(idx, score) for idx, score in zip(indices[0], distances[0])][::-1]\n",
    "\n",
    "    def search_documents(self, query: str, top_k: int = 3, lang: str = 'spanish') -> List[str]:\n",
    "        relevant_documents = self.search(query, top_k, lang)\n",
    "        return [self.documents[idx] for idx, score in relevant_documents]"
   ],
   "id": "87d4b9b3a1e73a3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea RAG1\n",
    "\n",
    " Utilizando de base el código de las clases pasadas, implementar un SparseRetriever basado en el vectorizador BM25. Es importante que la clase herede de `Retriever`\n"
   ],
   "id": "58e2e05dd27e6c90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## TODO: desarrollar un SparseRetriever\n",
    "class SparseRetriever(Retriever):\n",
    "    \"\"\"\n",
    "    Implementa búsqueda dispersa usando BM25.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__('sparse_retriever')\n",
    "\n",
    "\n",
    "    def build_index(self, documents: List[str], lang:str = 'spanish'):\n",
    "         pass\n",
    "\n",
    "\n",
    "    def search(self, query: str, top_k: int = 3, lang:str = 'spanish') -> List[Tuple[int, float]]:\n",
    "        pass\n",
    "\n",
    "    def search_documents(self, query: str, top_k: int = 3, lang:str = 'spanish') -> List[str]:\n",
    "        pass\n"
   ],
   "id": "9bbde3b8b4669a8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dense Retriever:\n",
   "id": "7d58ff96fdcf7bd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea RAG2\n",
    "\n",
    "De manera similar a antes, implementar un DenseRetriever que herede de la clase Retriever. La clase tiene que ser modular para poder, internamente, utilizar distintos modelos. En particular, querremos instanciar un DenseRetriever con el modelo 'sentence-transformers/all-MiniLM-L6-v2' (384 dimensional dense vector) y otro con el modelo 'distiluse-base-multilingual-cased-v1' (512 dimensional dense vector)"
   ],
   "id": "c18862e8a43b8c91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: implementar el DenseRetriever\n",
    "class DenseRetriever(Retriever):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__('dense_retriever')\n",
    "        pass\n",
    "\n",
    "\n",
    "    def build_index(self, documents: List[str], lang: str = 'spanish'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def search(self, query: str, top_k: int = 3, lang: str = 'spanish') -> List[Tuple[int, float]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def search_documents(self, query: str, top_k: int = 3, lang: str = 'spanish') -> List[str]:\n",
    "       pass\n"
   ],
   "id": "e33596aa3ad668cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Combinación de Técnicas Dispersas y Densas\n",
   "id": "da7a25b0ac4c3c79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea RAG3\n",
    "\n",
    "De manera similar a antes, implementar un HybridRetriever que herede de la clase Retriever. La clase tiene que ser combinar un *dense retriever* y un *sparse retriever*. En particular, querremos instanciar un DenseRetriever con el modelo 'sentence-transformers/all-MiniLM-L6-v2' (384 dimensional dense vector) y otro con el modelo 'distiluse-base-multilingual-cased-v1' (512 dimensional dense vector)"
   ],
   "id": "6ded98c4db0bea0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HybridRetriever(Retriever):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__('hybrid_retriever')\n",
    "        pass\n",
    "\n",
    "    def build_index(self, documents: List[str], lang: str = 'spanish'):\n",
    "        pass\n",
    "\n",
    "    def search(self, query: str, top_k: int = 3, lang: str = 'spanish') -> List[Tuple[int, float]]:\n",
    "        pass\n",
    "\n",
    "    def search_documents(self, query: str, top_k: int = 3, lang: str = 'spanish') -> List[str]:\n",
    "     pass"
   ],
   "id": "f0ff00b7f6653fcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluación de sistemas RAG",
   "id": "4df3a091500e61d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para evaluar estos sistemas, necesitamos un dataset que posea una pregunta, un conjunto de textos, y una relación entre la pregunta y cuáles de dichos textos son 'relevantes' para responder la pregunta. Para ello, podemos usar los datasets de `rungalileo/ragbench`, por ejemplo el `techqa`. Vamos a escribir el código para que por un lado se extraigan todos los posibles documentos del dataset y, por otro lado, se emparejen las consultas con los fragmentos relevantes.",
   "id": "e973b0927a251956"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "# load train/validation/test splits of individual subset\n",
    "ragbench = load_dataset(\"rungalileo/ragbench\", \"techqa\", split=[\"test\"])\n",
    "\n",
    "# Preparar documentos, consultas y relevancias manualmente\n",
    "def format_dataset(dataset: List[Dataset]):\n",
    "      # Aplanar los documentos si son listas de listas\n",
    "    documents = []\n",
    "    for doc in dataset[\"documents\"]:\n",
    "        if isinstance(doc, list):\n",
    "            documents.extend(doc)  # Añadir documentos individuales\n",
    "        else:\n",
    "            documents.append(doc)\n",
    "    queries = {dataset[\"id\"][idx] : question for idx, question in enumerate(dataset['question'])}\n",
    "    relevant_docs = {dataset[\"id\"][idx] : response for idx, response in enumerate(dataset[\"documents\"])}\n",
    "    return {'documents': documents, 'queries': queries, 'gold_std': relevant_docs}\n",
    "\n",
    "dataset = format_dataset(ragbench[0])\n",
    "for idx_doc, value in dataset['gold_std'].items():\n",
    "    print(\"document_id: \", idx_doc)\n",
    "    print(\"documents: \", len(value))\n",
    "    print(\"related query: \", dataset['queries'][idx_doc])\n",
    "    break\n"
   ],
   "id": "e447cf58714cad0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Por último vamos a escribir un método que compruebe la eficacia de un conjunto de retrievers. Nótese que, al contrario que pasaba con los query answering, para los Retrievers es posible calcular métricas no difusas (fuzzy).",
   "id": "c2f8f3c26fe562f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_retrievers(retrievers, documents, queries, gold_std, top_k=5):\n",
    "    results = {}\n",
    "    for retriever in retrievers:\n",
    "        retriever_name = retriever.get_name()\n",
    "        print(f\"Evaluando {retriever_name}...\")\n",
    "        retriever.build_index(documents)\n",
    "        model_results = []\n",
    "\n",
    "        for query_id, query in queries.items():\n",
    "            if query_id not in gold_std:\n",
    "                continue\n",
    "            ground_truth = gold_std[query_id]\n",
    "            retrieved_docs = retriever.search_documents(query, top_k=top_k)\n",
    "\n",
    "            # Calcular métricas de evaluación\n",
    "            y_true = [1 if doc_found in ground_truth else 0 for doc_found in retrieved_docs]\n",
    "            y_pred = [1] * len(retrieved_docs)\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "            model_results.append({\"precision\": precision, \"recall\": recall, \"f1\": f1})\n",
    "\n",
    "        avg_precision = np.mean([r[\"precision\"] for r in model_results])\n",
    "        avg_recall = np.mean([r[\"recall\"] for r in model_results])\n",
    "        avg_f1 = np.mean([r[\"f1\"] for r in model_results])\n",
    "\n",
    "        results[retriever_name] = {\n",
    "            \"precision\": avg_precision,\n",
    "            \"recall\": avg_recall,\n",
    "            \"f1\": avg_f1\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_comparison(results, min_y=0.7, max_y=1.0):\n",
    "    metrics = [\"precision\", \"recall\", \"f1\"]\n",
    "    models = list(results.keys())\n",
    "\n",
    "    for metric in metrics:\n",
    "        values = [results[model][metric] for model in models]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(models, values)\n",
    "        plt.title(f\"Comparación de {metric.capitalize()}\", fontsize=14)\n",
    "        plt.xlabel(\"Modelos\", fontsize=12)\n",
    "        plt.ylabel(metric.capitalize(), fontsize=12)\n",
    "        plt.ylim(min_y, max_y)\n",
    "        plt.xticks(rotation=45, fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "9ecf36859eb41c05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pruebas manuales",
   "id": "b9c778c390872c26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vamos a analizar, usando el dataset anterior, los resultados de manera manual que un retriever puede obtener.",
   "id": "2fba93d6164dd2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "retriever = SparseRetrieverNM()\n",
    "retriever.build_index(dataset['documents'])\n",
    "\n",
    "documents_found = retriever.search_documents(dataset['queries']['techqa_DEV_Q243'])\n",
    "expected_docs = dataset['gold_std']['techqa_DEV_Q243']\n",
    "print(\"Total documents to be found: \", len(expected_docs))\n",
    "for docs_found in documents_found:\n",
    "    for expected_doc in expected_docs:\n",
    "        if expected_doc in docs_found:\n",
    "            print(\"relevant document found!\")\n"
   ],
   "id": "352d2442ca797115",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pruebas de benchmark",
   "id": "d200944e3103d536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuación vamos a evaluar todos los retriever que hemos desarrollado para comprobar cual se comporta mejor en los distintos datasets del repositorio `rungalileo/ragbench`",
   "id": "edf3eaf2b13bf17f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Crear instancias de retrievers\n",
    "retrievers = [SparseRetriever(), SparseRetrieverNM(), DenseRetriever(), DenseRetriever( model='distiluse-base-multilingual-cased-v1'), HybridRetriever(), HybridRetriever(model='distiluse-base-multilingual-cased-v1')]"
   ],
   "id": "993662ca43d187d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load train/validation/test splits of individual subset\n",
    "ragbench= load_dataset(\"rungalileo/ragbench\", \"techqa\", split=[\"test\"])\n",
    "dataset = format_dataset(ragbench[0])\n",
    "\n",
    "# Evaluar los retrievers\n",
    "results = evaluate_retrievers(retrievers, dataset['documents'], dataset['queries'], dataset['gold_std'], top_k=5)\n",
    "\n",
    "# Mostrar y graficar resultados\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: Precisión={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "\n",
    "plot_comparison(results, min_y=0.0, max_y=1.0)"
   ],
   "id": "c320fcc1ade59e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load train/validation/test splits of individual subset\n",
    "ragbench= load_dataset(\"rungalileo/ragbench\", \"covidqa\", split=[\"test\"])\n",
    "dataset = format_dataset(ragbench[0])\n",
    "\n",
    "# Evaluar los retrievers\n",
    "results = evaluate_retrievers(retrievers, dataset['documents'], dataset['queries'], dataset['gold_std'], top_k=5)\n",
    "\n",
    "# Mostrar y graficar resultados\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: Precisión={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "\n",
    "plot_comparison(results, min_y=0.0, max_y=1.0)"
   ],
   "id": "d1570eaeff1da68d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ragbench= load_dataset(\"rungalileo/ragbench\", \"finqa\", split=[\"test\"])\n",
    "dataset = format_dataset(ragbench[0])\n",
    "\n",
    "# Evaluar los retrievers\n",
    "results = evaluate_retrievers(retrievers, dataset['documents'], dataset['queries'], dataset['gold_std'], top_k=5)\n",
    "\n",
    "# Mostrar y graficar resultados\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: Precisión={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "\n",
    "plot_comparison(results, min_y=0.0, max_y=1.0)"
   ],
   "id": "655663b1f8bc2915",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load train/validation/test splits of individual subset\n",
    "ragbench= load_dataset(\"rungalileo/ragbench\", \"hotpotqa\", split=[\"test\"])\n",
    "dataset = format_dataset(ragbench[0])\n",
    "\n",
    "# Evaluar los retrievers\n",
    "results = evaluate_retrievers(retrievers, dataset['documents'], dataset['queries'], dataset['gold_std'], top_k=5)\n",
    "\n",
    "# Mostrar y graficar resultados\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: Precisión={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "\n",
    "plot_comparison(results, min_y=0.0, max_y=1.0)"
   ],
   "id": "a971baada5a69d61",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
