{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Construcción aplicada de un sistema de Query Answering",
   "id": "b4db2ddd2ebc57bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "En este notebook vamos a construir un sistema de Query Answering usando la técnica RAG para que responda preguntas sobre el libro `El principito`. Para ello, primero vamos a construir un dataset a partir del libro y, posteriormente, construir el sistema usando las clases vistas en los notebooks anteriores."
   ],
   "id": "86491922a3eaba32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "token = \"\"\n",
    "print(\"Hugging Face logging\")\n",
    "login(token)"
   ],
   "id": "315702aab7d5b428",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device_setup = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using: \", device_setup)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "id": "9bce5f6f1db6fee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Tarea QAC1 - Dataset\n",
    "Lo primero que necesitamos hacer es preparar la base documental con la que alimentaremos nuestro RAG. Para ello, vamos a leer el PDF del libro y vamos a dividirlo en documentos relevantes que pueden ser frases, parrafos, etc. Para leer el fichero pdf podemos usar la libreria `pdfplumber`. Por otro lado, el libro se encuentra en la carpeta `./provided/el_principito.pdf`"
   ],
   "id": "d29b2aa72e95b7a7"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pdfplumber\n",
    "\n",
    "\n",
    "# TODO: Leer el fichero PDF y partirlo en documentos relevantes\n",
    "\n",
    "documents = []\n",
    "print(\"Tamaño del corpus documental: \",len(documents))\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preguntas y respuestas para la evaluación",
   "id": "7daa4d8d477efd1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A continuación, vamos a utilizar las siguientes preguntas y respuestas para evaluar el modelo. Las preguntas han sido escogidas aleatoriamente de distintos exámenes tipo test online sobre el libro.",
   "id": "53f72bced758d7ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "questions = {\n",
    "\t1 : \"¿El Principito vive en un asteroide?\",\n",
    "\t2 : \"¿El asteroide se llama B-612?\",\n",
    "\t3 : \"¿El Principito tiene una flor que cuida mucho?\",\n",
    "\t4 : \"¿La flor del Principito es una rosa?\",\n",
    "\t5 : \"¿El Principito se encuentra con un zorro en la Tierra?\",\n",
    "\t6 : \"¿El zorro le enseña al Principito sobre la amistad?\",\n",
    "\t7 : \"¿El aviador arregla su avión durante la historia?\",\n",
    "\t8 : \"¿El Principito considera los baobabs una amenaza para su asteroide?\",\n",
    "\t9 : \"¿El Principito piensa que los adultos siempre entienden las cosas importantes?\",\n",
    "\t10 : \"¿El Principito regresa a su asteroide al final del libro?\"\n",
    "}\n",
    "\n",
    "responses = {\n",
    "    1 : \"Si.\",\n",
    "    2 : \"Si.\",\n",
    "    3 : \"Si.\",\n",
    "    4 : \"Si.\",\n",
    "    5 : \"Si.\",\n",
    "    6 : \"Si.\",\n",
    "    7 : \"Si.\",\n",
    "    8 : \"Si.\",\n",
    "    9 : \"No.\",\n",
    "    10 : \"Implícitamente, si.\"\n",
    "}"
   ],
   "id": "c9aaa5c1c8728ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluar los modelos usando el dataset y el libro",
   "id": "6b83f05a8e97aa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from libs.qa_lib import QuestionAnsweringFactory\n",
    "from libs.qa_lib import LLMModel\n",
    "from libs.qa_lib import evaluate_qa_answers\n"
   ],
   "id": "81aaac4e47154d65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea QAC2.1\n",
    "\n",
    "Escribir el código necesario para que un modelo LLM basado en `TinyLlama/TinyLlama-1.1B-Chat-v1.0` responsa cada pregunta del dataset. Imprima por pantalla la pregunta, la respuesta esperada, y la generada"
   ],
   "id": "21fa58339e3a3c03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "answers = {}\n",
    "\n",
    "for idx, question in questions.items():\n",
    "    pass\n",
    "    # TODO: Crear un LLModel sin contexto\n",
    "    # TODO: dada una pregunta generar la respuesta\n",
    "    # TODO: imprimir la pregunta, la respuesta esperada y la obtenida\n",
    "    # TODO: guardar en la variable answers el nombre del modelo, el id de la pregunta y la respuesta obtenida"
   ],
   "id": "4fbfcc50b921001d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tarea QAC2.2\n",
    "\n",
    "Escribir el código necesario para que un modelo de Question Answering basado en `TinyLlama/TinyLlama-1.1B-Chat-v1.0` responsa cada pregunta del dataset. Imprima por pantalla la pregunta, la respuesta esperada, y la generada. Pruebe a variar la temperatura y observar las distintas respuestas."
   ],
   "id": "2889d78ea132e772"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: construir un objeto de la clase QuestionAnswering",
   "id": "fed7988f605b2724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: variar el top_k y la temperatura para mostrar como pueden mejorar las respuestas\n",
    "for idx, question in questions.items():\n",
    "    pass\n",
    "    # TODO: dada una pregunta generar la respuesta\n",
    "    # TODO: imprimir la pregunta, la respuesta esperada y la obtenida\n",
    "    # TODO: guardar en la variable answers el nombre del modelo, el id de la pregunta y la respuesta obtenida"
   ],
   "id": "8cd67c37591175ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Tarea QAC3",
   "id": "c1e73e14d260ac41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evalúe los dos modelos anteriores para ello utilice las funciones vistas en los notebooks anteriores",
   "id": "d00dde50bb519847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from libs.rag_lib import plot_evaluation_results\n",
    "# TODO: usando la funcion `evaluate_qa_answers` y `plot_evaluation_results` evaluar los resultados"
   ],
   "id": "bed951c0c43ce6a5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
