{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9/lSO3Pk4COrqiuiahHhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/curso-pln/blob/main/notebooks/03_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings de Palabras usando Sherlock Holmes\n",
        "\n",
        "\n",
        "# 1) Carga de Datos\n",
        "\n",
        "Carga un texto desde Project Gutenberg y elimina el header/footer:"
      ],
      "metadata": {
        "id": "pgX4RcCCEzfb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr89zm_uEldQ",
        "outputId": "f5c76e45-d95d-40f2-b013-32a52e2200ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando texto desde: https://www.gutenberg.org/files/1661/1661-0.txt ..\n",
            "Texto cargado: 574154 caracteres\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "def load_gutenberg_text(url):\n",
        "    print(\"Cargando texto desde:\", url, \"..\")\n",
        "    response = urllib.request.urlopen(url)\n",
        "    raw = response.read().decode('utf-8')\n",
        "\n",
        "    # Encontrar el inicio y fin del contenido real (eliminar header/footer de Gutenberg)\n",
        "    start = raw.find(\"*** START OF THE PROJECT GUTENBERG\")\n",
        "    start = raw.find(\"\\n\", start) + 1\n",
        "    end = raw.find(\"*** END OF THE PROJECT GUTENBERG\")\n",
        "\n",
        "    return raw[start:end]\n",
        "\n",
        "# URLs de los libros en Project Gutenberg\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/files/1661/1661-0.txt\",    # The Adventures of Sherlock Holmes\n",
        "    #\"https://www.gutenberg.org/files/108/108-0.txt\",      # The Return of Sherlock Holmes\n",
        "    #\"https://www.gutenberg.org/files/69700/69700-0.txt\",  # The case-book of Sherlock Holmes\n",
        "]\n",
        "\n",
        "# Carga y guarda el texto\n",
        "text = \" \"\n",
        "for url in urls:\n",
        "  book_text = load_gutenberg_text(url)\n",
        "  text += book_text\n",
        "\n",
        "\n",
        "print(\"Texto cargado:\", len(text), \"caracteres\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Preprocesamiento\n",
        "\n",
        "Tokeniza y limpia el texto:\n",
        "* Convierte a minúsculas\n",
        "* Elimina tokens que no son palabras"
      ],
      "metadata": {
        "id": "aEFtcjHpFBD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Inicializar spaCy (solo tokenizador para velocidad)\n",
        "nlp = English(disable=['tagger','parser','ner'])\n",
        "\n",
        "def tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text.lower() for token in doc\n",
        "            if token.text.strip() and not token.is_punct]\n",
        "\n",
        "# Dividir en oraciones y tokenizar\n",
        "corpus_sentences = []\n",
        "for line in text.split('\\n'):\n",
        "    if line.strip():  # ignorar líneas vacías\n",
        "        tokens = tokenize(line)\n",
        "        if tokens:  # ignorar líneas sin tokens válidos\n",
        "            corpus_sentences.append(tokens)\n",
        "\n",
        "print(\"Total de oraciones procesadas:\", len(corpus_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOY6GToqFB8G",
        "outputId": "fea49ad6-f8d0-49af-eaa0-7127df335053"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de oraciones procesadas: 9313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Entrenamiento de Modelos"
      ],
      "metadata": {
        "id": "NAXkjJtYFF0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, FastText"
      ],
      "metadata": {
        "id": "WLssPaVaFIOH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar Word2Vec"
      ],
      "metadata": {
        "id": "qz6fz_DXFX0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences=corpus_sentences,\n",
        "                    vector_size=100,\n",
        "                    window=5,\n",
        "                    min_count=2,\n",
        "                    workers=4)"
      ],
      "metadata": {
        "id": "-E69AoT3FLt9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar FastText"
      ],
      "metadata": {
        "id": "OJlQfyhbFazu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = FastText(sentences=corpus_sentences,\n",
        "                   vector_size=100,\n",
        "                   window=5,\n",
        "                   min_count=2,\n",
        "                   workers=4)"
      ],
      "metadata": {
        "id": "qHLPLzC6FNtk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar modelos"
      ],
      "metadata": {
        "id": "lUuvM3a_FdVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.save(\"sherlock_w2v.model\")\n",
        "ft_model.save(\"sherlock_ft.model\")"
      ],
      "metadata": {
        "id": "CcwtMCPHFSmx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Análisis de Similitudes"
      ],
      "metadata": {
        "id": "Alaz2yIXFV2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar vectores"
      ],
      "metadata": {
        "id": "-Af8SyvvFg24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_vectors = w2v_model.wv\n",
        "ft_vectors = ft_model.wv\n",
        "\n",
        "print(\"\\nEstadísticas de los modelos:\")\n",
        "print(\"Dimensión de los vectores:\", w2v_vectors.vector_size)\n",
        "print(\"Número de palabras (Word2Vec):\", len(w2v_vectors.index_to_key))\n",
        "print(\"Número de palabras (FastText):\", len(ft_vectors.index_to_key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yLfuYjtFWWr",
        "outputId": "16a8397e-2025-4f64-a0f1-4564db2088f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Estadísticas de los modelos:\n",
            "Dimensión de los vectores: 100\n",
            "Número de palabras (Word2Vec): 4390\n",
            "Número de palabras (FastText): 4390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPalabras más similares a 'holmes' (Word2Vec):\")\n",
        "print(w2v_vectors.most_similar('holmes'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyZz7D9DFk5l",
        "outputId": "fbabac73-566b-4011-af4c-acd055a7c45d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palabras más similares a 'holmes' (Word2Vec):\n",
            "[('wilson', 0.9974080920219421), ('lestrade', 0.9969411492347717), ('dear', 0.996678352355957), ('he', 0.996609091758728), ('mr.', 0.9964424967765808), ('she', 0.9964025616645813), ('pray', 0.996375322341919), ('let', 0.9963335394859314), ('help', 0.9962263703346252), ('far', 0.9962010979652405)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPalabras más similares a 'crime' (Word2Vec):\")\n",
        "print(w2v_vectors.most_similar('crime'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLq7S5YvFqXC",
        "outputId": "b2786611-d517-4c2d-ef0c-1eed6df429ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palabras más similares a 'crime' (Word2Vec):\n",
            "[('most', 0.9991777539253235), ('years', 0.9991291165351868), ('or', 0.9991259574890137), ('time', 0.9991074800491333), ('some', 0.9990922808647156), ('five', 0.9990889430046082), ('first', 0.9990875720977783), ('long', 0.9990733861923218), ('like', 0.9990726709365845), ('every', 0.9990718960762024)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSimilitud entre 'holmes' y 'detective':\")\n",
        "print(\"Word2Vec:\", w2v_vectors.similarity('holmes', 'detective'))\n",
        "print(\"FastText:\", ft_vectors.similarity('holmes', 'detective'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_vPFl2EFsUo",
        "outputId": "fafe3172-accb-4cee-ea38-f82f9caee0bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Similitud entre 'holmes' y 'detective':\n",
            "Word2Vec: 0.9788761\n",
            "FastText: 0.9993339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Experimentos con palabras fuera de vocabulario"
      ],
      "metadata": {
        "id": "SEwj5kfhFu3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPrueba con palabra fuera de vocabulario:\")\n",
        "try:\n",
        "    print(\"Word2Vec - Similares a 'investigador':\")\n",
        "    print(w2v_vectors.most_similar('investigador'))\n",
        "except KeyError:\n",
        "    print(\"Word2Vec no puede manejar palabras fuera de vocabulario\")\n",
        "\n",
        "print(\"\\nFastText - Similares a 'investigador':\")\n",
        "print(ft_vectors.most_similar('investigador'))  # FastText puede generar vectores para palabras nuevas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMxXTQUQFwvo",
        "outputId": "b497f052-802f-4908-907a-30cf92ae94ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prueba con palabra fuera de vocabulario:\n",
            "Word2Vec - Similares a 'investigador':\n",
            "Word2Vec no puede manejar palabras fuera de vocabulario\n",
            "\n",
            "FastText - Similares a 'investigador':\n",
            "[('investigation', 0.9999861717224121), ('investigations', 0.9999841451644897), ('investments', 0.9999807476997375), ('invent', 0.9999798536300659), ('information', 0.9999767541885376), ('instrument', 0.9999763369560242), ('portion', 0.9999760389328003), ('plantation', 0.9999752640724182), ('inspection', 0.9999748468399048), ('instant', 0.9999748468399048)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Analogías"
      ],
      "metadata": {
        "id": "G-wbbEIyF5qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAnalogías (Word2Vec):\")\n",
        "result = w2v_vectors.most_similar(positive=['holmes', 'crime'],\n",
        "                                negative=['police'])\n",
        "print(\"holmes:crime como police:?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmtgbjYoF3La",
        "outputId": "b92af6b0-1f47-4e59-d562-526ed129a427"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analogías (Word2Vec):\n",
            "holmes:crime como police:?\n",
            "[('he', 0.9967093467712402), ('rucastle', 0.9966807961463928), ('wilson', 0.9961453080177307), ('lestrade', 0.9957737326622009), ('also', 0.9957162141799927), ('st.', 0.9956408143043518), ('merryweather', 0.9955851435661316), ('john', 0.9955640435218811), ('always', 0.995510458946228), ('neville', 0.9954965114593506)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Mejora del Preprocesamiento\n",
        "\n"
      ],
      "metadata": {
        "id": "4AYgvSSBGRh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función mejorada de limpieza de texto:\n",
        "* Elimina números\n",
        "* Elimina puntuación\n",
        "* Elimina texto entre paréntesis (acotaciones)\n",
        "* Elimina múltiples espacios\n",
        "* Normaliza apóstrofes y comillas"
      ],
      "metadata": {
        "id": "GK2hvP4_tzmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    try:\n",
        "        # Normalizar apóstrofes y comillas\n",
        "        text = re.sub(r\"[''']\", \"'\", text)\n",
        "        text = re.sub(r'[\"\"\"]', '\"', text)\n",
        "\n",
        "        # Eliminar texto entre paréntesis\n",
        "        text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "\n",
        "        # Eliminar números\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Eliminar puntuación preservando apóstrofes\n",
        "        text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
        "\n",
        "        # Normalizar espacios\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la limpieza del texto: {e}\")\n",
        "        return text"
      ],
      "metadata": {
        "id": "RAfYWPtVGTN-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesa un token individual:\n",
        "* Convierte a minúsculas\n",
        "* Maneja contracciones comunes\n",
        "* Elimina tokens muy cortos"
      ],
      "metadata": {
        "id": "NGDqBSQpt4gQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_token(token):\n",
        "    # Convertir a minúsculas\n",
        "    token = token.lower()\n",
        "\n",
        "    # Diccionario de contracciones\n",
        "    contractions = {\n",
        "        \"i'm\": \"i am\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"when's\": \"when is\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"why's\": \"why is\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"mustn't\": \"must not\"\n",
        "    }\n",
        "\n",
        "    # Expandir contracciones\n",
        "    if token in contractions:\n",
        "        return contractions[token].split()\n",
        "\n",
        "    # Eliminar tokens muy cortos (menos de 2 caracteres)\n",
        "    if len(token) < 2:\n",
        "        return []\n",
        "\n",
        "    return [token]"
      ],
      "metadata": {
        "id": "tzFWD97Vt9W4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Versión mejorada de tokenización:\n",
        "* Usa la limpieza mejorada\n",
        "* Maneja contracciones\n",
        "* Opcionalmente elimina stopwords\n",
        "* Elimina tokens no válidos"
      ],
      "metadata": {
        "id": "l7Ux-UzJuA3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def tokenize_improved(text, remove_stopwords=True):\n",
        "    # Obtener stopwords\n",
        "    stop_words = set(stopwords.words('english')) if remove_stopwords else set()\n",
        "\n",
        "    # Limpiar el texto\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # Tokenizar\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Procesar cada token\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        processed_tokens = preprocess_token(token.text)\n",
        "        for t in processed_tokens:\n",
        "            if remove_stopwords and t in stop_words:\n",
        "                continue\n",
        "            if t.strip() and not t.isnumeric():\n",
        "                tokens.append(t)\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSEnWfCVuKgc",
        "outputId": "5c8f00b8-6284-489a-861c-09f5f1d8536e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesar el texto con la nueva función"
      ],
      "metadata": {
        "id": "tWqTrGlVGct8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "improved_sentences = []\n",
        "for line in text.split('\\n'):\n",
        "    if line.strip():\n",
        "        tokens = tokenize_improved(line)\n",
        "        if tokens:\n",
        "            improved_sentences.append(tokens)\n",
        "\n",
        "print(\"Total de oraciones procesadas (mejorado):\", len(improved_sentences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4fH2gNoGaG7",
        "outputId": "36e78c8f-e7fc-4882-cd3b-76277c5c569a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de oraciones procesadas (mejorado): 9220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Entrenamiento de Modelos"
      ],
      "metadata": {
        "id": "5meEszkk3bri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences=improved_sentences,\n",
        "                    vector_size=100,\n",
        "                    window=5,\n",
        "                    min_count=2,\n",
        "                    workers=4)"
      ],
      "metadata": {
        "id": "Kg5ylTKF3d7U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) Análisis de Similitudes"
      ],
      "metadata": {
        "id": "PVz5Pz1fClXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPalabras más similares a 'holmes' (Word2Vec):\")\n",
        "print(w2v_vectors.most_similar('holmes'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a1w7MU4Cn2D",
        "outputId": "19cec753-f958-4c2b-c5c5-e969dcd2c623"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palabras más similares a 'holmes' (Word2Vec):\n",
            "[('wilson', 0.9974080920219421), ('lestrade', 0.9969411492347717), ('dear', 0.996678352355957), ('he', 0.996609091758728), ('mr.', 0.9964424967765808), ('she', 0.9964025616645813), ('pray', 0.996375322341919), ('let', 0.9963335394859314), ('help', 0.9962263703346252), ('far', 0.9962010979652405)]\n"
          ]
        }
      ]
    }
  ]
}