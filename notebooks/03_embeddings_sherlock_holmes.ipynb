{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyML+BFyUXjg96XMrvp8Fzv0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/curso-pln/blob/main/notebooks/03_embeddings_sherlock_holmes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings de Palabras usando Sherlock Holmes\n",
        "\n",
        "\n",
        "# 1) Carga de Datos\n",
        "\n",
        "Carga un texto desde Project Gutenberg y elimina el header/footer:"
      ],
      "metadata": {
        "id": "pgX4RcCCEzfb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr89zm_uEldQ",
        "outputId": "cddd42c1-5472-46bd-e666-cfe19a15e95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando texto desde: https://www.gutenberg.org/files/1661/1661-0.txt ..\n",
            "Cargando texto desde: https://www.gutenberg.org/files/108/108-0.txt ..\n",
            "Cargando texto desde: https://www.gutenberg.org/files/2097/2097-0.txt ..\n",
            "Cargando texto desde: https://www.gutenberg.org/files/244/244-0.txt ..\n",
            "Texto cargado: 1742150 caracteres\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "def load_gutenberg_text(url):\n",
        "    print(\"Cargando texto desde:\", url, \"..\")\n",
        "    response = urllib.request.urlopen(url)\n",
        "    raw = response.read().decode('utf-8')\n",
        "\n",
        "    # Encontrar el inicio y fin del contenido real (eliminar header/footer de Gutenberg)\n",
        "    start = raw.find(\"*** START OF THE PROJECT GUTENBERG\")\n",
        "    start = raw.find(\"\\n\", start) + 1\n",
        "    end = raw.find(\"*** END OF THE PROJECT GUTENBERG\")\n",
        "\n",
        "    return raw[start:end]\n",
        "\n",
        "# URLs de los libros en Project Gutenberg\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/files/1661/1661-0.txt\",    # The Adventures of Sherlock Holmes\n",
        "    \"https://www.gutenberg.org/files/108/108-0.txt\",      # The Return of Sherlock Holmes\n",
        "    \"https://www.gutenberg.org/files/2097/2097-0.txt\",    # The Hound of the Baskervilles\n",
        "    \"https://www.gutenberg.org/files/244/244-0.txt\"       # A Study in Scarlet\n",
        "]\n",
        "\n",
        "# Carga y guarda el texto\n",
        "text = \" \"\n",
        "for url in urls:\n",
        "  book_text = load_gutenberg_text(url)\n",
        "  text += book_text\n",
        "\n",
        "\n",
        "print(\"Texto cargado:\", len(text), \"caracteres\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Preprocesamiento\n",
        "\n",
        "Tokeniza y limpia el texto:\n",
        "* Convierte a minúsculas\n",
        "* Elimina tokens que no son palabras"
      ],
      "metadata": {
        "id": "aEFtcjHpFBD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Inicializar spaCy (solo tokenizador para velocidad)\n",
        "nlp = English(disable=['tagger','parser','ner'])\n",
        "\n",
        "def tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text.lower() for token in doc\n",
        "            if token.text.strip() and not token.is_punct]\n",
        "\n",
        "# Dividir en oraciones y tokenizar\n",
        "corpus_sentences = []\n",
        "for line in text.split('\\n'):\n",
        "    if line.strip():  # ignorar líneas vacías\n",
        "        tokens = tokenize(line)\n",
        "        if tokens:  # ignorar líneas sin tokens válidos\n",
        "            corpus_sentences.append(tokens)\n",
        "\n",
        "print(\"Total de oraciones procesadas:\", len(corpus_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOY6GToqFB8G",
        "outputId": "8607e852-a11b-43b2-edc2-596ef5883cfc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de oraciones procesadas: 27683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Entrenamiento de Modelos"
      ],
      "metadata": {
        "id": "NAXkjJtYFF0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, FastText"
      ],
      "metadata": {
        "id": "WLssPaVaFIOH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar Word2Vec"
      ],
      "metadata": {
        "id": "qz6fz_DXFX0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences=corpus_sentences,\n",
        "                    vector_size=300,      # Aumentar dimensionalidad (300)\n",
        "                    window=8,             # Ventana más amplia para capturar más contexto (8)\n",
        "                    min_count=2,          # Filtrar palabras poco frecuentes (5)\n",
        "                    workers=4,\n",
        "                    sg=1,                 # Usar Skip-gram\n",
        "                    epochs= 30,          # Más épocas de entrenamiento\n",
        "                    negative= 15,        # Negative sampling\n",
        "                    alpha= 0.025,        # Learning rate inicial\n",
        "                    min_alpha= 0.0001    # Learning rate final\n",
        "                  )"
      ],
      "metadata": {
        "id": "-E69AoT3FLt9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar FastText"
      ],
      "metadata": {
        "id": "OJlQfyhbFazu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = FastText(sentences=corpus_sentences,\n",
        "                   vector_size=300,    # Aumentar dimensionalidad (300)\n",
        "                   window=8,           # Ventana más amplia para capturar más contexto (8)\n",
        "                   min_count=2,        # Mantener min_count bajo para capturar más variantes\n",
        "                   workers=4,\n",
        "                   sg=1,               # Skip-gram para mejor calidad\n",
        "                   min_n=2,            # Tamaño mínimo de n-gramas\n",
        "                   max_n=6,            # Tamaño máximo de n-gramas (aumentado para capturar más patrones)\n",
        "                   epochs=30,          # Más épocas de entrenamiento\n",
        "                   word_ngrams=1,      # Habilitar n-gramas de palabras\n",
        "                   negative=15,        # Más muestras negativas\n",
        "                   alpha=0.025,        # Learning rate inicial\n",
        "                   min_alpha=0.0001    # Learning rate final\n",
        "              )"
      ],
      "metadata": {
        "id": "qHLPLzC6FNtk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar modelos"
      ],
      "metadata": {
        "id": "lUuvM3a_FdVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.save(\"sherlock_w2v.model\")\n",
        "ft_model.save(\"sherlock_ft.model\")"
      ],
      "metadata": {
        "id": "CcwtMCPHFSmx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Análisis de Similitudes"
      ],
      "metadata": {
        "id": "Alaz2yIXFV2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar vectores"
      ],
      "metadata": {
        "id": "-Af8SyvvFg24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_vectors = w2v_model.wv\n",
        "ft_vectors = ft_model.wv\n",
        "\n",
        "print(\"\\nEstadísticas de los modelos:\")\n",
        "print(\"Dimensión de los vectores:\", w2v_vectors.vector_size)\n",
        "print(\"Número de palabras (Word2Vec):\", len(w2v_vectors.index_to_key))\n",
        "print(\"Número de palabras (FastText):\", len(ft_vectors.index_to_key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yLfuYjtFWWr",
        "outputId": "595ac56a-cae9-4706-d5df-e87fd87a855a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Estadísticas de los modelos:\n",
            "Dimensión de los vectores: 300\n",
            "Número de palabras (Word2Vec): 8416\n",
            "Número de palabras (FastText): 8416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPalabras más similares a 'holmes' (Word2Vec):\")\n",
        "print(w2v_vectors.most_similar('holmes'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyZz7D9DFk5l",
        "outputId": "1633be31-0ff5-40e8-d24f-856b95cc1300"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palabras más similares a 'holmes' (Word2Vec):\n",
            "[('sherlock', 0.5627038478851318), ('demurely', 0.4705038368701935), ('gleefully', 0.4312404692173004), ('involuntarily', 0.4271424114704132), ('cheerily', 0.4240522086620331), ('sardonically', 0.4198615252971649), ('misjudged', 0.41461366415023804), ('compel', 0.41146203875541687), ('aback', 0.4071274995803833), ('triumphantly', 0.4049912095069885)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPalabras más similares a 'crime' (Word2Vec):\")\n",
        "print(w2v_vectors.most_similar('crime'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLq7S5YvFqXC",
        "outputId": "fed2ea38-0c2a-4cdd-8d7c-f76835e2fc25"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palabras más similares a 'crime' (Word2Vec):\n",
            "[('committed', 0.5095756649971008), ('literature', 0.47296348214149475), ('featureless', 0.4724273979663849), ('deliberate', 0.45530077815055847), ('records', 0.45302456617355347), ('logic', 0.4432651400566101), ('detect', 0.43559083342552185), ('talent', 0.4308238923549652), ('perpetrator', 0.4281263053417206), ('sots', 0.4210221469402313)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSimilitud entre 'crime' y 'art':\")\n",
        "print(\"Word2Vec:\", w2v_vectors.similarity('crime', 'art'))\n",
        "print(\"FastText:\", ft_vectors.similarity('crime', 'art'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_vPFl2EFsUo",
        "outputId": "c75527c8-f901-4e0a-b959-64b9ba8a76a4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Similitud entre 'crime' y 'art':\n",
            "Word2Vec: 0.22276348\n",
            "FastText: 0.073123716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Experimentos con palabras fuera de vocabulario"
      ],
      "metadata": {
        "id": "SEwj5kfhFu3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPrueba con palabra fuera de vocabulario:\")\n",
        "try:\n",
        "    print(\"Word2Vec - Similares a 'investigador':\")\n",
        "    print(w2v_vectors.most_similar('investigador'))\n",
        "except KeyError:\n",
        "    print(\"Word2Vec no puede manejar palabras fuera de vocabulario\")\n",
        "\n",
        "print(\"\\nFastText - Similares a 'investigador':\")\n",
        "print(ft_vectors.most_similar('investigador'))  # FastText puede generar vectores para palabras nuevas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMxXTQUQFwvo",
        "outputId": "0c890628-4ce0-4bc2-8f33-d431b80cc5d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prueba con palabra fuera de vocabulario:\n",
            "Word2Vec - Similares a 'investigador':\n",
            "Word2Vec no puede manejar palabras fuera de vocabulario\n",
            "\n",
            "FastText - Similares a 'investigador':\n",
            "[('investigate', 0.9296735525131226), ('investigated', 0.915936291217804), ('investigating', 0.9112592935562134), ('investigations', 0.8825503587722778), ('investigation', 0.8762537240982056), ('invest', 0.8713880777359009), ('investments', 0.7584698796272278), ('testimonial', 0.5811780691146851), ('domestic', 0.5748422145843506), ('obligations', 0.5539227724075317)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Analogías"
      ],
      "metadata": {
        "id": "G-wbbEIyF5qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAnalogías (Word2Vec):\")\n",
        "result = w2v_vectors.most_similar(positive=['holmes', 'police'],\n",
        "                                negative=['evidence'])\n",
        "print(\"holmes:police como crime:?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmtgbjYoF3La",
        "outputId": "43e231bc-f4eb-4af6-806e-fe6881b580a9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analogías (Word2Vec):\n",
            "holmes:police como crime:?\n",
            "[('sherlock', 0.3552018105983734), ('wink', 0.337390661239624), ('jove', 0.3190644085407257), ('duncan', 0.309001088142395), ('soda', 0.30734172463417053), ('triumph', 0.30677905678749084), ('apologize', 0.3051346242427826), ('grinning', 0.3048025369644165), ('curtly', 0.3041435480117798), ('basket', 0.2993147075176239)]\n"
          ]
        }
      ]
    }
  ]
}