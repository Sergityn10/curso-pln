{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7AdHO5RxxJkumt8C7Y8eO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/curso-pln/blob/main/notebooks/01_Preprocesamiento_Datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Técnicas de Preprocesamiento de Texto en PLN\n",
        "\n",
        "\n",
        " Este notebook demuestra las principales técnicas de preprocesamiento de texto\n",
        " utilizadas en Procesamiento de Lenguaje Natural (PLN).\n"
      ],
      "metadata": {
        "id": "0A_20cyC7anK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Configuración del Entorno"
      ],
      "metadata": {
        "id": "_eb7Sjnc7mle"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LGx1Glq7QRi",
        "outputId": "75861722-da09-45c8-e598-26f35dad2ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==3.7.6 in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.6) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.6) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.6) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.6) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.6) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.6) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.6) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.7.6) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.6) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.7.6) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6) (0.1.2)\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Primero instalamos las bibliotecas necesarias\n",
        "!pip install -U spacy==3.7.6 nltk\n",
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Datos de Ejemplo"
      ],
      "metadata": {
        "id": "1EvK68iH7rre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_ejemplo = \"\"\"El procesamiento de lenguaje natural es fascinante.\n",
        "Los investigadores están desarrollando nuevas técnicas cada día.\n",
        "El aprendizaje automático ha revolucionado este campo de estudio.\"\"\"\n",
        "\n",
        "print(\"Texto de ejemplo:\")\n",
        "print(texto_ejemplo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV99mwiD7wFi",
        "outputId": "d9dede30-a6c0-4984-8bbe-7db04cb19d20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto de ejemplo:\n",
            "El procesamiento de lenguaje natural es fascinante. \n",
            "Los investigadores están desarrollando nuevas técnicas cada día.\n",
            "El aprendizaje automático ha revolucionado este campo de estudio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Tokenización"
      ],
      "metadata": {
        "id": "P7mLx0lf8Gcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Utilizando NLTK"
      ],
      "metadata": {
        "id": "Cf0MZaKu8KF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización por palabras usando NLTK\n",
        "tokens_palabras = word_tokenize(texto_ejemplo, language='spanish')\n",
        "print(\"\\nTokenización por palabras (NLTK):\")\n",
        "print(tokens_palabras)\n",
        "\n",
        "# Tokenización por oraciones usando NLTK\n",
        "oraciones = sent_tokenize(texto_ejemplo, language='spanish')\n",
        "print(\"\\nTokenización por oraciones (NLTK):\")\n",
        "for i, oracion in enumerate(oraciones, 1):\n",
        "    print(f\"Oración {i}: {oracion}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taRzK7S-8M_t",
        "outputId": "b337cfa5-c724-4a4e-fd3c-0e0aa042fdf9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenización por palabras (NLTK):\n",
            "['El', 'procesamiento', 'de', 'lenguaje', 'natural', 'es', 'fascinante', '.', 'Los', 'investigadores', 'están', 'desarrollando', 'nuevas', 'técnicas', 'cada', 'día', '.', 'El', 'aprendizaje', 'automático', 'ha', 'revolucionado', 'este', 'campo', 'de', 'estudio', '.']\n",
            "\n",
            "Tokenización por oraciones (NLTK):\n",
            "Oración 1: El procesamiento de lenguaje natural es fascinante.\n",
            "Oración 2: Los investigadores están desarrollando nuevas técnicas cada día.\n",
            "Oración 3: El aprendizaje automático ha revolucionado este campo de estudio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Utilizando spaCy"
      ],
      "metadata": {
        "id": "UoOAOYGc9cU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('es_core_news_sm')\n",
        "doc = nlp(texto_ejemplo)\n",
        "tokens_spacy = [token.text for token in doc]\n",
        "\n",
        "print(\"\\nTokenización con spaCy:\")\n",
        "print(tokens_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbRNeMlt9fl3",
        "outputId": "99b3bea9-2e70-4f4b-9932-9a1194b903e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenización con spaCy:\n",
            "['El', 'procesamiento', 'de', 'lenguaje', 'natural', 'es', 'fascinante', '.', '\\n', 'Los', 'investigadores', 'están', 'desarrollando', 'nuevas', 'técnicas', 'cada', 'día', '.', '\\n', 'El', 'aprendizaje', 'automático', 'ha', 'revolucionado', 'este', 'campo', 'de', 'estudio', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Stemming"
      ],
      "metadata": {
        "id": "d_bDdFov9wNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando Porter Stemmer de NLTK\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(token) for token in tokens_palabras]\n",
        "\n",
        "print(\"\\nPalabras originales vs stems:\")\n",
        "for original, stem in list(zip(tokens_palabras, stems))[:10]:\n",
        "    print(f\"{original:15} -> {stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGHYKjpL9yTJ",
        "outputId": "7cc101a4-60c9-4462-b08a-9dae00004c9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palabras originales vs stems:\n",
            "El              -> el\n",
            "procesamiento   -> procesamiento\n",
            "de              -> de\n",
            "lenguaje        -> lenguaj\n",
            "natural         -> natur\n",
            "es              -> es\n",
            "fascinante      -> fascinant\n",
            ".               -> .\n",
            "Los             -> lo\n",
            "investigadores  -> investigador\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Lematización"
      ],
      "metadata": {
        "id": "kXSQaTkc91HK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas_spacy = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"\\nPalabras originales vs lemas (spaCy):\")\n",
        "for original, lemma in list(zip(tokens_spacy, lemmas_spacy)):\n",
        "    print(f\"{original:15} -> {lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ickv01GK93ys",
        "outputId": "72e2b3dd-5502-477f-eefa-7455743f0cdb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palabras originales vs lemas (spaCy):\n",
            "El              -> el\n",
            "procesamiento   -> procesamiento\n",
            "de              -> de\n",
            "lenguaje        -> lenguaje\n",
            "natural         -> natural\n",
            "es              -> ser\n",
            "fascinante      -> fascinante\n",
            ".               -> .\n",
            "\n",
            "               -> \n",
            "\n",
            "Los             -> el\n",
            "investigadores  -> investigador\n",
            "están           -> estar\n",
            "desarrollando   -> desarrollar\n",
            "nuevas          -> nuevo\n",
            "técnicas        -> técnica\n",
            "cada            -> cada\n",
            "día             -> día\n",
            ".               -> .\n",
            "\n",
            "               -> \n",
            "\n",
            "El              -> el\n",
            "aprendizaje     -> aprendizaje\n",
            "automático      -> automático\n",
            "ha              -> haber\n",
            "revolucionado   -> revolucionar\n",
            "este            -> este\n",
            "campo           -> campo\n",
            "de              -> de\n",
            "estudio         -> estudio\n",
            ".               -> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Etiquetado POS (Part-of-Speech)"
      ],
      "metadata": {
        "id": "j8cKDtf5-BO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEtiquetado POS con spaCy:\")\n",
        "for token in list(doc)[:10]:\n",
        "    print(f\"{token.text:15} -> {token.pos_:10} -> {token.dep_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pvPXgTS-FZG",
        "outputId": "c7ed49d1-1df0-4b7c-89ab-97f42924e0bf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Etiquetado POS con spaCy:\n",
            "El              -> DET        -> det\n",
            "procesamiento   -> NOUN       -> nsubj\n",
            "de              -> ADP        -> case\n",
            "lenguaje        -> NOUN       -> nmod\n",
            "natural         -> ADJ        -> amod\n",
            "es              -> AUX        -> cop\n",
            "fascinante      -> ADJ        -> ROOT\n",
            ".               -> PUNCT      -> punct\n",
            "\n",
            "               -> SPACE      -> dep\n",
            "Los             -> DET        -> det\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Ejemplo Práctico"
      ],
      "metadata": {
        "id": "RxqZFP65-Jlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_analisis = \"\"\"El pequeño gato negro saltaba alegremente por el jardín.\n",
        "Los pájaros volaban asustados mientras el gato jugaba.\"\"\"\n",
        "\n",
        "doc_analisis = nlp(texto_analisis)\n",
        "\n",
        "print(\"\\nAnálisis completo del texto:\")\n",
        "print(\"Token\\t\\tLema\\t\\tPOS\\t\\tDependencia\")\n",
        "print(\"-\" * 60)\n",
        "for token in doc_analisis:\n",
        "    print(f\"{token.text:12} {token.lemma_:12} {token.pos_:12} {token.dep_:12}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHDmSkY9-MHH",
        "outputId": "ac04bf3c-946b-4736-a87a-8fc9bea41255"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Análisis completo del texto:\n",
            "Token\t\tLema\t\tPOS\t\tDependencia\n",
            "------------------------------------------------------------\n",
            "El           el           DET          det         \n",
            "pequeño      pequeño      ADJ          amod        \n",
            "gato         gato         NOUN         nsubj       \n",
            "negro        negro        ADJ          amod        \n",
            "saltaba      saltar       VERB         ROOT        \n",
            "alegremente  alegremente  ADV          advmod      \n",
            "por          por          ADP          case        \n",
            "el           el           DET          det         \n",
            "jardín       jardín       NOUN         obl         \n",
            ".            .            PUNCT        punct       \n",
            "\n",
            "            \n",
            "            SPACE        dep         \n",
            "Los          el           DET          det         \n",
            "pájaros      pájaro       NOUN         nsubj       \n",
            "volaban      volar        VERB         ROOT        \n",
            "asustados    asustado     ADJ          obj         \n",
            "mientras     mientras     SCONJ        mark        \n",
            "el           el           DET          det         \n",
            "gato         gato         NOUN         nsubj       \n",
            "jugaba       jugar        VERB         advcl       \n",
            ".            .            PUNCT        punct       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Funciones de Utilidad"
      ],
      "metadata": {
        "id": "xFO7iqDl-SNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analizar_texto(texto, nlp):\n",
        "    \"\"\"\n",
        "    Realiza un análisis completo de un texto usando spaCy.\n",
        "\n",
        "    Args:\n",
        "        texto (str): Texto a analizar\n",
        "        nlp: Modelo de spaCy cargado\n",
        "\n",
        "    Returns:\n",
        "        dict: Diccionario con los resultados del análisis\n",
        "    \"\"\"\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    return {\n",
        "        'tokens': [token.text for token in doc],\n",
        "        'lemas': [token.lemma_ for token in doc],\n",
        "        'pos': [token.pos_ for token in doc],\n",
        "        'dependencias': [token.dep_ for token in doc],\n",
        "        'oraciones': [str(sent) for sent in doc.sents],\n",
        "        'entidades': [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    }\n",
        "\n",
        "# Ejemplo de uso de la función de utilidad\n",
        "texto_prueba = \"María estudia ingeniería en la Universidad Politécnica de Madrid.\"\n",
        "resultados = analizar_texto(texto_prueba, nlp)\n",
        "\n",
        "print(\"\\nResultados del análisis automático:\")\n",
        "for key, value in resultados.items():\n",
        "    print(f\"\\n{key.capitalize()}:\")\n",
        "    print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-sTdLH9-UX5",
        "outputId": "200273a5-cec4-4898-bfa8-ea7ed10749c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resultados del análisis automático:\n",
            "\n",
            "Tokens:\n",
            "['María', 'estudia', 'ingeniería', 'en', 'la', 'Universidad', 'Politécnica', 'de', 'Madrid', '.']\n",
            "\n",
            "Lemas:\n",
            "['María', 'estudiar', 'ingeniería', 'en', 'el', 'Universidad', 'Politécnica', 'de', 'Madrid', '.']\n",
            "\n",
            "Pos:\n",
            "['PROPN', 'VERB', 'NOUN', 'ADP', 'DET', 'PROPN', 'PROPN', 'ADP', 'PROPN', 'PUNCT']\n",
            "\n",
            "Dependencias:\n",
            "['nsubj', 'ROOT', 'obj', 'case', 'det', 'obl', 'flat', 'case', 'flat', 'punct']\n",
            "\n",
            "Oraciones:\n",
            "['María estudia ingeniería en la Universidad Politécnica de Madrid.']\n",
            "\n",
            "Entidades:\n",
            "[('Universidad Politécnica de Madrid', 'ORG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Ejercicios para practicar:\n",
        "\n",
        "1. Tokenización:\n",
        "   - Prueba diferentes textos y compara los resultados de NLTK vs spaCy\n",
        "   - Implementa un tokenizador personalizado para un caso específico\n",
        "\n",
        "2. Stemming y Lematización:\n",
        "   - Compara los resultados de stemming vs lematización\n",
        "   - Analiza casos donde cada técnica podría ser más apropiada\n",
        "\n",
        "3. Análisis POS:\n",
        "   - Identifica patrones gramaticales comunes\n",
        "   - Extrae frases nominales o verbales\n",
        "   - Crea estadísticas sobre el uso de diferentes partes del discurso\n",
        "\n",
        "4. Proyecto Integrado:\n",
        "   - Desarrolla un analizador de textos que combine todas las técnicas\n",
        "   - Aplícalo a un conjunto de documentos reales\n",
        "   - Genera visualizaciones de los resultados"
      ],
      "metadata": {
        "id": "o9y5yap4-axo"
      }
    }
  ]
}